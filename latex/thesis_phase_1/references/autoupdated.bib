
@article{jurafsky_speech_nodate,
	title = {Speech and Language Processing},
	pages = {561},
	author = {Jurafsky, Daniel and Martin, James H and Norvig, Peter and Russell, Stuart},
	langid = {english},
	note = {{ZSCC}: {NoCitationData}[s1]},
	file = {Jurafsky et al. - Speech and Language Processing.pdf:C\:\\Users\\ohund\\Zotero\\storage\\QJE89FD3\\Jurafsky et al. - Speech and Language Processing.pdf:application/pdf}
}

@article{danilevsky_survey_2020,
	title = {A Survey of the State of Explainable {AI} for Natural Language Processing},
	url = {http://arxiv.org/abs/2010.00711},
	abstract = {Recent years have seen important advances in the quality of state-of-the-art models, but this has come at the expense of models becoming less interpretable. This survey presents an overview of the current state of Explainable {AI} ({XAI}), considered within the domain of Natural Language Processing ({NLP}). We discuss the main categorization of explanations, as well as the various ways explanations can be arrived at and visualized. We detail the operations and explainability techniques currently available for generating explanations for {NLP} model predictions, to serve as a resource for model developers in the community. Finally, we point out the current gaps and encourage directions for future work in this important research area.},
	journaltitle = {{arXiv}:2010.00711 [cs]},
	author = {Danilevsky, Marina and Qian, Kun and Aharonov, Ranit and Katsis, Yannis and Kawas, Ban and Sen, Prithviraj},
	urldate = {2021-07-09},
	date = {2020-10-01},
	eprinttype = {arxiv},
	eprint = {2010.00711},
	note = {{ZSCC}: 0000023 },
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, done, Computer Science - Machine Learning, I.2.7, {XAI}},
	file = {arXiv.org Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\263WG3LE\\2010.html:text/html;Danilevsky et al_2020_A Survey of the State of Explainable AI for Natural Language Processing.pdf:C\:\\Users\\ohund\\Zotero\\storage\\947IF9L2\\Danilevsky et al_2020_A Survey of the State of Explainable AI for Natural Language Processing.pdf:application/pdf}
}

@inproceedings{qian_xnlp_2021,
	location = {New York, {NY}, {USA}},
	title = {{XNLP}: A Living Survey for {XAI} Research in Natural Language Processing},
	isbn = {978-1-4503-8018-8},
	url = {https://doi.org/10.1145/3397482.3450728},
	doi = {10.1145/3397482.3450728},
	series = {{IUI} '21},
	shorttitle = {{XNLP}},
	abstract = {We present {XNLP}: an interactive browser-based system embodying a living survey of recent state-of-the-art research in the field of Explainable {AI} ({XAI}) within the domain of Natural Language Processing ({NLP}). The system visually organizes and illustrates {XAI}-{NLP} publications and distills their content to allow users to gain insights, generate ideas, and explore the field. We hope that {XNLP} can become a leading demonstrative example of a living survey, balancing the depth and quality of a traditional well-constructed survey paper with the collaborative dynamism of a widely available interactive tool. {XNLP} can be accessed at: https://xainlp2020.github.io/xainlp.},
	pages = {78--80},
	booktitle = {26th International Conference on Intelligent User Interfaces},
	publisher = {Association for Computing Machinery},
	author = {Qian, Kun and Danilevsky, Marina and Katsis, Yannis and Kawas, Ban and Oduor, Erick and Popa, Lucian and Li, Yunyao},
	urldate = {2021-07-09},
	date = {2021-04-14},
	note = {{ZSCC}: 0000001},
	keywords = {{XAI}, Explainable {AI}, interactive survey, natural language processing},
	file = {Qian et al_2021_XNLP.pdf:C\:\\Users\\ohund\\Zotero\\storage\\B2UGPC2Z\\Qian et al_2021_XNLP.pdf:application/pdf}
}

@online{shreyas_deep_2019,
	title = {Deep Embedding’s for Categorical variables (Cat2Vec)},
	url = {https://towardsdatascience.com/deep-embeddings-for-categorical-variables-cat2vec-b05c8ab63ac0},
	abstract = {In this blog I am going to take you through the steps involved in creating a embedding for categorical variables using a deep learning…},
	titleaddon = {Medium},
	author = {Shreyas, Prajwal},
	urldate = {2021-07-09},
	date = {2019-09-22},
	langid = {english},
	note = {{ZSCC}: {NoCitationData}[s0]},
	file = {Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\PYNDCK4U\\deep-embeddings-for-categorical-variables-cat2vec-b05c8ab63ac0.html:text/html}
}

@inproceedings{de_koninck_act2vec_2018,
	location = {Cham},
	title = {act2vec, trace2vec, log2vec, and model2vec: Representation Learning for Business Processes},
	isbn = {978-3-319-98648-7},
	doi = {10.1007/978-3-319-98648-7_18},
	series = {Lecture Notes in Computer Science},
	shorttitle = {act2vec, trace2vec, log2vec, and model2vec},
	abstract = {In process mining, the challenge is typically to turn raw event data into meaningful models, insights, or actions. One of the key problems of a data-driven analysis of processes, is the high dimensionality of the data. In this paper, we address this problem by developing representation learning techniques for business processes. More specifically, the representation learning paradigm is applied to activities, traces, logs, and models in order to learn highly informative but low-dimensional vectors, often referred to as embeddings, based on a neural network architecture. Subsequently, these vectors can be used for automated inference tasks such as trace clustering, process comparison, predictive process monitoring, anomaly detection, etc. Accordingly, the main contribution of this paper is the proposal of representation learning architectures at the level of activities, traces, logs, and models that can produce a distributed representation of these objects and a thorough analysis of potential applications. In an experimental evaluation, we show the power of such derived representations in the context of trace clustering and process model comparison.},
	pages = {305--321},
	booktitle = {Business Process Management},
	publisher = {Springer International Publishing},
	author = {De Koninck, Pieter and vanden Broucke, Seppe and De Weerdt, Jochen},
	editor = {Weske, Mathias and Montali, Marco and Weber, Ingo and vom Brocke, Jan},
	date = {2018},
	langid = {english},
	note = {{ZSCC}: {NoCitationData}[s0]
tex.ids= dekoninck\_Act2vecTrace2vecLog2vec\_2018a},
	keywords = {done, Process mining, Representation learning, Word embedding, Embeddings},
	file = {De Koninck et al_2018_act2vec, trace2vec, log2vec, and model2vec.pdf:C\:\\Users\\ohund\\Zotero\\storage\\645Y5CWQ\\De Koninck et al_2018_act2vec, trace2vec, log2vec, and model2vec.pdf:application/pdf}
}

@report{gartnerpm2019-interview,
	title = {Interview in the 2019 gartner market guide for process mining, research note G00387812 by m. Kerremans},
	author = {Aalst, W.M.P. van der},
	date = {2019},
	note = {{ZSCC}: {NoCitationData}[s0] 
tex.added-at: 2020-01-07T14:01:33.000+0100
tex.biburl: https://www.bibsonomy.org/bibtex/210c445ad11af96f07ece965cc1b66567/wvdaalst
tex.interhash: e1a6d063d88e9a533df371ad2e2e90f6
tex.intrahash: 10c445ad11af96f07ece965cc1b66567
tex.timestamp: 2020-01-07T14:02:12.000+0100},
	keywords = {imported}
}

@article{kerremans_market_2019,
	title = {Market guide for process mining},
	abstract = {New forms of automation (e.g., robotic process automation) and knowledge of the underlying
processes/interactions are key to digital transformation. Process mining helps enterprise
architecture and technology innovation leaders assess operations and performance,
increasing these initiatives’ value.},
	journaltitle = {Gartner Inc},
	shortjournal = {Gartner Inc},
	author = {Kerremans, Marc},
	date = {2019},
	note = {{ZSCC}: 0000017},
	file = {market_guide_for_pm_gartner.pdf:C\:\\Users\\ohund\\Zotero\\storage\\RGB8X77N\\market_guide_for_pm_gartner.pdf:application/pdf}
}

@article{viner_process_2021,
	title = {A Process Mining Software Comparison},
	url = {http://arxiv.org/abs/2007.14038},
	abstract = {www.processmining-software.com is a dedicated website for process mining software comparison and was developed to give practitioners and researchers an overview of commercial tools available on the market. Based on literature review and experimental tool testing, a set of criteria was developed in order to assess the tools' functional capabilities in an objective manner. With our publicly accessible website, we intend to increase the transparency of tool functionality. Being an academic endeavour, the non-commercial nature of the study ensures a less biased assessment as compared with reports from analyst firms.},
	journaltitle = {{arXiv}:2007.14038 [cs]},
	author = {Viner, Daniel and Stierle, Matthias and Matzner, Martin},
	urldate = {2021-07-13},
	date = {2021-02-04},
	eprinttype = {arxiv},
	eprint = {2007.14038},
	note = {{ZSCC}: {NoCitationData}[s0] },
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Software Engineering},
	file = {arXiv.org Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\6QHQVJZF\\2007.html:text/html;Viner et al_2021_A Process Mining Software Comparison.pdf:C\:\\Users\\ohund\\Zotero\\storage\\JXYZZDFN\\Viner et al_2021_A Process Mining Software Comparison.pdf:application/pdf}
}

@video{aip_-_pursuing_sota_ai_for_everyone_neurips_nodate,
	title = {{NeurIPS} 2020 {\textbar} An Explanation to What is Counterfactuals in Interpretable {AI}? (Tutorial)},
	url = {https://www.youtube.com/watch?v=adTazXyLn38},
	shorttitle = {{NeurIPS} 2020 {\textbar} An Explanation to What is Counterfactuals in Interpretable {AI}?},
	abstract = {Join the channel membership:
https://www.youtube.com/c/{AIPursuit}/join

Subscribe to the channel:
https://www.youtube.com/c/{AIPursuit}?s...

Support and Donation:
Paypal ⇢ https://paypal.me/tayhengee
Patreon ⇢ https://www.patreon.com/hengee
{BTC} ⇢ bc1q2r7eymlf20576alvcmryn28tgrvxqw5r30cmpu
{ETH} ⇢ 0x58c4bD4244686F3b4e636EfeBD159258A5513744
Doge ⇢ {DSGNbzuS}1s6x81ZSbSHHV5uGDxJXePeyKy

Wanted to own {BTC}, {ETH}, or even Dogecoin? Kickstart your crypto portfolio with the largest crypto market Binance with my affiliate link:
https://accounts.binance.com/en/regis... 
The video is reposted for educational purposes and encourages involvement in the field of research.},
	author = {{AIP - Pursuing SoTA AI for everyone}},
	urldate = {2021-07-23},
	note = {{ZSCC}: {NoCitationData}[s0]}
}

@inproceedings{michel_evaluation_2019,
	location = {Minneapolis, Minnesota},
	title = {On Evaluation of Adversarial Perturbations for Sequence-to-Sequence Models},
	url = {https://aclanthology.org/N19-1314},
	doi = {10.18653/v1/N19-1314},
	abstract = {Adversarial examples — perturbations to the input of a model that elicit large changes in the output — have been shown to be an effective way of assessing the robustness of sequence-to-sequence (seq2seq) models. However, these perturbations only indicate weaknesses in the model if they do not change the input so significantly that it legitimately results in changes in the expected output. This fact has largely been ignored in the evaluations of the growing body of related literature. Using the example of untargeted attacks on machine translation ({MT}), we propose a new evaluation framework for adversarial attacks on seq2seq models that takes the semantic equivalence of the pre- and post-perturbation input into account. Using this framework, we demonstrate that existing methods may not preserve meaning in general, breaking the aforementioned assumption that source side perturbations should not result in changes in the expected output. We further use this framework to demonstrate that adding additional constraints on attacks allows for adversarial perturbations that are more meaning-preserving, but nonetheless largely change the output sequence. Finally, we show that performing untargeted adversarial training with meaning-preserving attacks is beneficial to the model in terms of adversarial robustness, without hurting test performance. A toolkit implementing our evaluation framework is released at https://github.com/pmichel31415/teapot-nlp.},
	eventtitle = {{NAACL}-{HLT} 2019},
	pages = {3103--3114},
	booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Michel, Paul and Li, Xian and Neubig, Graham and Pino, Juan},
	urldate = {2021-09-03},
	date = {2019-06},
	note = {{ZSCC}: 0000061},
	keywords = {irrelevant, Evaluation},
	file = {Michel et al_2019_On Evaluation of Adversarial Perturbations for Sequence-to-Sequence Models.pdf:C\:\\Users\\ohund\\Zotero\\storage\\E9E9BQRK\\Michel et al_2019_On Evaluation of Adversarial Perturbations for Sequence-to-Sequence Models.pdf:application/pdf}
}

@article{gardner_evaluating_2020,
	title = {Evaluating Models' Local Decision Boundaries via Contrast Sets},
	url = {http://arxiv.org/abs/2004.02709},
	abstract = {Standard test sets for supervised learning evaluate in-distribution generalization. Unfortunately, when a dataset has systematic gaps (e.g., annotation artifacts), these evaluations are misleading: a model can learn simple decision rules that perform well on the test set but do not capture a dataset's intended capabilities. We propose a new annotation paradigm for {NLP} that helps to close systematic gaps in the test data. In particular, after a dataset is constructed, we recommend that the dataset authors manually perturb the test instances in small but meaningful ways that (typically) change the gold label, creating contrast sets. Contrast sets provide a local view of a model's decision boundary, which can be used to more accurately evaluate a model's true linguistic capabilities. We demonstrate the efficacy of contrast sets by creating them for 10 diverse {NLP} datasets (e.g., {DROP} reading comprehension, {UD} parsing, {IMDb} sentiment analysis). Although our contrast sets are not explicitly adversarial, model performance is significantly lower on them than on the original test sets---up to 25{\textbackslash}\% in some cases. We release our contrast sets as new evaluation benchmarks and encourage future dataset construction efforts to follow similar annotation processes.},
	journaltitle = {{arXiv}:2004.02709 [cs]},
	author = {Gardner, Matt and Artzi, Yoav and Basmova, Victoria and Berant, Jonathan and Bogin, Ben and Chen, Sihao and Dasigi, Pradeep and Dua, Dheeru and Elazar, Yanai and Gottumukkala, Ananth and Gupta, Nitish and Hajishirzi, Hanna and Ilharco, Gabriel and Khashabi, Daniel and Lin, Kevin and Liu, Jiangming and Liu, Nelson F. and Mulcaire, Phoebe and Ning, Qiang and Singh, Sameer and Smith, Noah A. and Subramanian, Sanjay and Tsarfaty, Reut and Wallace, Eric and Zhang, Ally and Zhou, Ben},
	urldate = {2021-09-03},
	date = {2020-10-01},
	eprinttype = {arxiv},
	eprint = {2004.02709},
	note = {{ZSCC}: {NoCitationData}[s0] },
	keywords = {Computer Science - Computation and Language, irrelevant, Evaluation},
	file = {arXiv.org Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\9BVDYYVT\\2004.html:text/html;Gardner et al_2020_Evaluating Models' Local Decision Boundaries via Contrast Sets.pdf:C\:\\Users\\ohund\\Zotero\\storage\\2PHLBZLN\\Gardner et al_2020_Evaluating Models' Local Decision Boundaries via Contrast Sets.pdf:application/pdf}
}

@inproceedings{wang_closer_2021,
	location = {Online},
	title = {A Closer Look into the Robustness of Neural Dependency Parsers Using Better Adversarial Examples},
	url = {https://aclanthology.org/2021.findings-acl.207},
	doi = {10.18653/v1/2021.findings-acl.207},
	eventtitle = {Findings 2021},
	pages = {2344--2354},
	booktitle = {Findings of the Association for Computational Linguistics: {ACL}-{IJCNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Yuxuan and Che, Wanxiang and Titov, Ivan and Cohen, Shay B. and Lei, Zhilin and Liu, Ting},
	urldate = {2021-09-03},
	date = {2021-08},
	note = {{ZSCC}: {NoCitationData}[s0]},
	keywords = {Pertubations},
	file = {Wang et al_2021_A Closer Look into the Robustness of Neural Dependency Parsers Using Better.pdf:C\:\\Users\\ohund\\Zotero\\storage\\X6S6ZXHY\\Wang et al_2021_A Closer Look into the Robustness of Neural Dependency Parsers Using Better.pdf:application/pdf}
}

@inproceedings{malik_adv-olm_2021,
	location = {Online},
	title = {Adv-{OLM}: Generating Textual Adversaries via {OLM}},
	url = {https://aclanthology.org/2021.eacl-main.71},
	shorttitle = {Adv-{OLM}},
	abstract = {Deep learning models are susceptible to adversarial examples that have imperceptible perturbations in the original input, resulting in adversarial attacks against these models. Analysis of these attacks on the state of the art transformers in {NLP} can help improve the robustness of these models against such adversarial inputs. In this paper, we present Adv-{OLM}, a black-box attack method that adapts the idea of Occlusion and Language Models ({OLM}) to the current state of the art attack methods. {OLM} is used to rank words of a sentence, which are later substituted using word replacement strategies. We experimentally show that our approach outperforms other attack methods for several text classification tasks.},
	eventtitle = {{EACL} 2021},
	pages = {841--849},
	booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
	publisher = {Association for Computational Linguistics},
	author = {Malik, Vijit and Bhat, Ashwani and Modi, Ashutosh},
	urldate = {2021-09-03},
	date = {2021-04},
	note = {{ZSCC}: 0000002},
	keywords = {Pertubations, Supervised},
	file = {Malik et al_2021_Adv-OLM.pdf:C\:\\Users\\ohund\\Zotero\\storage\\IJMRNIDE\\Malik et al_2021_Adv-OLM.pdf:application/pdf}
}

@book{molnar_61_nodate,
	title = {6.1 Counterfactual Explanations {\textbar} Interpretable Machine Learning},
	url = {https://christophm.github.io/interpretable-ml-book/counterfactual.html},
	abstract = {Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable.},
	author = {Molnar, Christoph},
	urldate = {2021-09-08},
	note = {{ZSCC}: {NoCitationData}[s0]},
	keywords = {{XAI}},
	file = {Molnar_6.pdf:C\:\\Users\\ohund\\Zotero\\storage\\3858VE5Q\\Molnar_6.pdf:application/pdf;Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\3GKZ4WVW\\counterfactual.html:text/html}
}

@book{molnar_62_nodate,
	title = {6.2 Adversarial Examples {\textbar} Interpretable Machine Learning},
	url = {https://christophm.github.io/interpretable-ml-book/adversarial.html},
	abstract = {Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable.},
	author = {Molnar, Christoph},
	urldate = {2021-09-09},
	note = {{ZSCC}: {NoCitationData}[s0]},
	file = {Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\H8UE4MDH\\adversarial.html:text/html}
}

@book{molnar_64_nodate,
	title = {6.4 Influential Instances {\textbar} Interpretable Machine Learning},
	url = {https://christophm.github.io/interpretable-ml-book/influential.html},
	abstract = {Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable.},
	author = {Molnar, Christoph},
	urldate = {2021-09-09},
	note = {{ZSCC}: {NoCitationData}[s0]},
	file = {Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\8D78KEI3\\influential.html:text/html}
}

@book{molnar_chapter_nodate,
	title = {Chapter 6 Example-Based Explanations {\textbar} Interpretable Machine Learning},
	url = {https://christophm.github.io/interpretable-ml-book/example-based.html},
	abstract = {Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable.},
	author = {Molnar, Christoph},
	urldate = {2021-09-09},
	note = {{ZSCC}: {NoCitationData}[s0]},
	file = {Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\ZJIVIZPL\\example-based.html:text/html}
}

@book{molnar_63_nodate,
	title = {6.3 Prototypes and Criticisms {\textbar} Interpretable Machine Learning},
	url = {https://christophm.github.io/interpretable-ml-book/proto.html},
	abstract = {Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable.},
	author = {Molnar, Christoph},
	urldate = {2021-09-09},
	note = {{ZSCC}: {NoCitationData}[s0]},
	file = {Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\G6ZRC6EU\\proto.html:text/html}
}

@software{noauthor_must-read_2021,
	title = {Must-read Papers on Textual Adversarial Attack and Defense ({TAAD})},
	url = {https://github.com/thunlp/TAADpapers},
	abstract = {Must-read Papers on Textual Adversarial Attack and Defense},
	publisher = {{THUNLP}},
	urldate = {2021-09-09},
	date = {2021-09-07},
	note = {original-date: 2019-06-09T09:46:05Z},
	keywords = {adversarial-attacks, adversarial-defense, adversarial-learning, natural-language-processing, nlp, paper-list}
}

@inproceedings{wang_counterfactual_2021,
	location = {New York, {NY}, {USA}},
	title = {Counterfactual Data-Augmented Sequential Recommendation},
	isbn = {978-1-4503-8037-9},
	url = {https://doi.org/10.1145/3404835.3462855},
	doi = {10.1145/3404835.3462855},
	series = {{SIGIR} '21},
	abstract = {Sequential recommendation aims at predicting users' preferences based on their historical behaviors. However, this recommendation strategy may not perform well in practice due to the sparsity of the real-world data. In this paper, we propose a novel counterfactual data augmentation framework to mitigate the impact of the imperfect training data and empower sequential recommendation models. Our framework is composed of a sampler model and an anchor model. The sampler model aims to generate new user behavior sequences based on the observed ones, while the anchor model is leveraged to provide the final recommendation list, which is trained based on both observed and generated sequences. We design the sampler model to answer the key counterfactual question: "what would a user like to buy if her previously purchased items had been different?". Beyond heuristic intervention methods, we leverage two learning-based methods to implement the sampler model, and thus, improve the quality of the generated sequences when training the anchor model. Additionally, we analyze the influence of the generated sequences on the anchor model in theory and achieve a trade-off between the information and the noise introduced by the generated sequences. Experiments on nine real-world datasets demonstrate our framework's effectiveness and generality.},
	pages = {347--356},
	booktitle = {Proceedings of the 44th International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval},
	publisher = {Association for Computing Machinery},
	author = {Wang, Zhenlei and Zhang, Jingsen and Xu, Hongteng and Chen, Xu and Zhang, Yongfeng and Zhao, Wayne Xin and Wen, Ji-Rong},
	urldate = {2021-09-09},
	date = {2021-07-11},
	note = {{ZSCC}: 0000001},
	keywords = {counterfactual data augmentation, recommendation system},
	file = {Wang et al_2021_Counterfactual Data-Augmented Sequential Recommendation.pdf:C\:\\Users\\ohund\\Zotero\\storage\\NRZL6QT9\\Wang et al_2021_Counterfactual Data-Augmented Sequential Recommendation.pdf:application/pdf}
}

@inproceedings{karimi_model-agnostic_2020,
	title = {Model-Agnostic Counterfactual Explanations for Consequential Decisions},
	url = {https://proceedings.mlr.press/v108/karimi20a.html},
	eventtitle = {International Conference on Artificial Intelligence and Statistics},
	pages = {895--905},
	booktitle = {International Conference on Artificial Intelligence and Statistics},
	publisher = {{PMLR}},
	author = {Karimi, Amir-Hossein and Barthe, Gilles and Balle, Borja and Valera, Isabel},
	urldate = {2021-09-09},
	date = {2020-06-03},
	langid = {english},
	note = {{ZSCC}: 0000071 
{ISSN}: 2640-3498},
	keywords = {irrelevant},
	file = {Karimi et al_2020_Model-Agnostic Counterfactual Explanations for Consequential Decisions.pdf:C\:\\Users\\ohund\\Zotero\\storage\\3JJDW3JU\\Karimi et al_2020_Model-Agnostic Counterfactual Explanations for Consequential Decisions.pdf:application/pdf;Supplementary PDF:C\:\\Users\\ohund\\Zotero\\storage\\6SQ5DBI9\\Karimi et al. - 2020 - Model-Agnostic Counterfactual Explanations for Con.pdf:application/pdf}
}

@article{tsirtsis_counterfactual_2021,
	title = {Counterfactual Explanations in Sequential Decision Making Under Uncertainty},
	url = {http://arxiv.org/abs/2107.02776},
	abstract = {Methods to find counterfactual explanations have predominantly focused on one step decision making processes. In this work, we initiate the development of methods to find counterfactual explanations for decision making processes in which multiple, dependent actions are taken sequentially over time. We start by formally characterizing a sequence of actions and states using finite horizon Markov decision processes and the Gumbel-Max structural causal model. Building upon this characterization, we formally state the problem of finding counterfactual explanations for sequential decision making processes. In our problem formulation, the counterfactual explanation specifies an alternative sequence of actions differing in at most k actions from the observed sequence that could have led the observed process realization to a better outcome. Then, we introduce a polynomial time algorithm based on dynamic programming to build a counterfactual policy that is guaranteed to always provide the optimal counterfactual explanation on every possible realization of the counterfactual environment dynamics. We validate our algorithm using both synthetic and real data from cognitive behavioral therapy and show that the counterfactual explanations our algorithm finds can provide valuable insights to enhance sequential decision making under uncertainty.},
	journaltitle = {{arXiv}:2107.02776 [cs, stat]},
	author = {Tsirtsis, Stratis and De, Abir and Gomez-Rodriguez, Manuel},
	urldate = {2021-09-09},
	date = {2021-07-06},
	eprinttype = {arxiv},
	eprint = {2107.02776},
	note = {{ZSCC}: 0000000 },
	keywords = {done, hot, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computers and Society},
	file = {arXiv.org Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\NHKSHY45\\2107.html:text/html;Tsirtsis et al_2021_Counterfactual Explanations in Sequential Decision Making Under Uncertainty.pdf:C\:\\Users\\ohund\\Zotero\\storage\\UVJHCHIC\\Tsirtsis et al_2021_Counterfactual Explanations in Sequential Decision Making Under Uncertainty.pdf:application/pdf}
}

@article{oberst_counterfactual_2019,
	title = {Counterfactual Off-Policy Evaluation with Gumbel-Max Structural Causal Models},
	url = {http://arxiv.org/abs/1905.05824},
	abstract = {We introduce an off-policy evaluation procedure for highlighting episodes where applying a reinforcement learned ({RL}) policy is likely to have produced a substantially different outcome than the observed policy. In particular, we introduce a class of structural causal models ({SCMs}) for generating counterfactual trajectories in finite partially observable Markov Decision Processes ({POMDPs}). We see this as a useful procedure for off-policy "debugging" in high-risk settings (e.g., healthcare); by decomposing the expected difference in reward between the {RL} and observed policy into specific episodes, we can identify episodes where the counterfactual difference in reward is most dramatic. This in turn can be used to facilitate review of specific episodes by domain experts. We demonstrate the utility of this procedure with a synthetic environment of sepsis management.},
	journaltitle = {{arXiv}:1905.05824 [cs, stat]},
	author = {Oberst, Michael and Sontag, David},
	urldate = {2021-09-22},
	date = {2019-06-06},
	eprinttype = {arxiv},
	eprint = {1905.05824},
	note = {{ZSCC}: 0000061 },
	keywords = {hot, skimmed, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\ohund\\Zotero\\storage\\PTY3DHVM\\Oberst and Sontag - 2019 - Counterfactual Off-Policy Evaluation with Gumbel-M.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\XTWV66VS\\1905.html:text/html}
}

@online{kostadinov_understanding_2019,
	title = {Understanding Encoder-Decoder Sequence to Sequence Model},
	url = {https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346},
	abstract = {In this article, I will try to give a short and concise explanation of the sequence to sequence model which have recently achieved…},
	titleaddon = {Medium},
	author = {Kostadinov, Simeon},
	urldate = {2021-09-23},
	date = {2019-11-10},
	langid = {english},
	note = {{ZSCC}: 0000012},
	keywords = {blog},
	file = {Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\XCC9SQEA\\understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346.html:text/html}
}

@article{strobelt_seq2seq-vis_2018,
	title = {Seq2Seq-Vis: A Visual Debugging Tool for Sequence-to-Sequence Models},
	url = {http://arxiv.org/abs/1804.09299},
	shorttitle = {Seq2Seq-Vis},
	abstract = {Neural Sequence-to-Sequence models have proven to be accurate and robust for many sequence prediction tasks, and have become the standard approach for automatic translation of text. The models work in a five stage blackbox process that involves encoding a source sequence to a vector space and then decoding out to a new target sequence. This process is now standard, but like many deep learning methods remains quite difficult to understand or debug. In this work, we present a visual analysis tool that allows interaction with a trained sequence-to-sequence model through each stage of the translation process. The aim is to identify which patterns have been learned and to detect model errors. We demonstrate the utility of our tool through several real-world large-scale sequence-to-sequence use cases.},
	journaltitle = {{arXiv}:1804.09299 [cs]},
	author = {Strobelt, Hendrik and Gehrmann, Sebastian and Behrisch, Michael and Perer, Adam and Pfister, Hanspeter and Rush, Alexander M.},
	urldate = {2021-09-23},
	date = {2018-10-16},
	eprinttype = {arxiv},
	eprint = {1804.09299},
	note = {{ZSCC}: {NoCitationData}[s0] },
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv.org Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\ZHN7NZ7R\\1804.html:text/html;Strobelt et al_2018_Seq2Seq-Vis.pdf:C\:\\Users\\ohund\\Zotero\\storage\\ZF3Y94CV\\Strobelt et al_2018_Seq2Seq-Vis.pdf:application/pdf}
}

@article{jain_attention_2019,
	title = {Attention is not Explanation},
	url = {http://arxiv.org/abs/1902.10186},
	abstract = {Attention mechanisms have seen wide adoption in neural {NLP} models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work, we perform extensive experiments across a variety of {NLP} tasks that aim to assess the degree to which attention weights provide meaningful `explanations' for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do. Code for all experiments is available at https://github.com/successar/{AttentionExplanation}.},
	journaltitle = {{arXiv}:1902.10186 [cs]},
	author = {Jain, Sarthak and Wallace, Byron C.},
	urldate = {2021-09-23},
	date = {2019-05-08},
	eprinttype = {arxiv},
	eprint = {1902.10186},
	note = {{ZSCC}: 0000467 },
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\N3TUYE9A\\1902.html:text/html;Jain_Wallace_2019_Attention is not Explanation.pdf:C\:\\Users\\ohund\\Zotero\\storage\\MN9JMMLD\\Jain_Wallace_2019_Attention is not Explanation.pdf:application/pdf}
}

@article{verma_counterfactual_2020,
	title = {Counterfactual Explanations for Machine Learning: A Review},
	url = {http://arxiv.org/abs/2010.10596},
	shorttitle = {Counterfactual Explanations for Machine Learning},
	abstract = {Machine learning plays a role in many deployed decision systems, often in ways that are difficult or impossible to understand by human stakeholders. Explaining, in a human-understandable way, the relationship between the input and output of machine learning models is essential to the development of trustworthy machine-learning-based systems. A burgeoning body of research seeks to define the goals and methods of explainability in machine learning. In this paper, we seek to review and categorize research on counterfactual explanations, a specific class of explanation that provides a link between what could have happened had input to a model been changed in a particular way. Modern approaches to counterfactual explainability in machine learning draw connections to the established legal doctrine in many countries, making them appealing to fielded systems in high-impact areas such as finance and healthcare. Thus, we design a rubric with desirable properties of counterfactual explanation algorithms and comprehensively evaluate all currently-proposed algorithms against that rubric. Our rubric provides easy comparison and comprehension of the advantages and disadvantages of different approaches and serves as an introduction to major research themes in this field. We also identify gaps and discuss promising research directions in the space of counterfactual explainability.},
	journaltitle = {{arXiv}:2010.10596 [cs, stat]},
	author = {Verma, Sahil and Dickerson, John and Hines, Keegan},
	urldate = {2021-09-23},
	date = {2020-10-20},
	eprinttype = {arxiv},
	eprint = {2010.10596},
	note = {{ZSCC}: 0000048 },
	keywords = {Computer Science - Artificial Intelligence, done, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\ARJRNWD2\\2010.html:text/html;Verma et al_2020_Counterfactual Explanations for Machine Learning.pdf:C\:\\Users\\ohund\\Zotero\\storage\\YYRRP69V\\Verma et al_2020_Counterfactual Explanations for Machine Learning.pdf:application/pdf}
}

@article{verboven_combining_nodate,
	title = {Combining the Clinical and Operational Perspective in Heterogeneous Treatment Eﬀect Inference in Healthcare Processes},
	abstract = {Recent developments in causal machine learning open perspectives for new approaches that support decision-making in healthcare processes using causal models. In particular, Heterogeneous Treatment Eﬀect ({HTE}) inference enables the estimation of causal treatment eﬀects for individual cases, oﬀering great potential in a process mining context. At the same time, {HTE} literature typically focuses on clinical outcome measures, disregarding process eﬃciency. This paper shows the potential of jointly considering the clinical and operational eﬀects of treatments in the context of healthcare processes. Moreover, we present a simple pipeline that makes existing {HTE} machine learning techniques directly applicable to event logs. Besides these conceptual contributions, a proofof-concept application starting from the publicly available sepsis event log is outlined, forming the basis for a critical reﬂection regarding {HTE} estimation in a process mining context.},
	pages = {12},
	author = {Verboven, Sam and Martin, Niels},
	langid = {english},
	note = {{ZSCC}: {NoCitationData}[s0]},
	keywords = {hot, skimmed},
	file = {Verboven und Martin - Combining the Clinical and Operational Perspective.pdf:C\:\\Users\\ohund\\Zotero\\storage\\W936WH4B\\Verboven und Martin - Combining the Clinical and Operational Perspective.pdf:application/pdf}
}

@incollection{lee_counter-factual_2013,
	location = {Berlin, Heidelberg},
	title = {Counter-Factual Reinforcement Learning: How to Model Decision-Makers That Anticipate the Future},
	isbn = {978-3-642-36406-8},
	url = {https://doi.org/10.1007/978-3-642-36406-8_4},
	series = {Studies in Computational Intelligence},
	shorttitle = {Counter-Factual Reinforcement Learning},
	abstract = {This chapter introduces a novel framework for modeling interacting humans in a multi-stage game. This ”iterated semi network-form game” framework has the following desirable characteristics: (1) Bounded rational players, (2) strategic players (i.e., players account for one another’s reward functions when predicting one another’s behavior), and (3) computational tractability even on real-world systems. We achieve these benefits by combining concepts from game theory and reinforcement learning. To be precise, we extend the bounded rational ”level-K reasoning” model to apply to games over multiple stages. Our extension allows the decomposition of the overall modeling problem into a series of smaller ones, each of which can be solved by standard reinforcement learning algorithms. We call this hybrid approach ”level-K reinforcement learning”. We investigate these ideas in a cyber battle scenario over a smart power grid and discuss the relationship between the behavior predicted by our model and what one might expect of real human defenders and attackers.},
	pages = {101--128},
	booktitle = {Decision Making and Imperfection},
	publisher = {Springer},
	author = {Lee, Ritchie and Wolpert, David H. and Bono, James and Backhaus, Scott and Bent, Russell and Tracey, Brendan},
	editor = {Guy, Tatiana V. and Karny, Miroslav and Wolpert, David},
	urldate = {2021-09-28},
	date = {2013},
	langid = {english},
	doi = {10.1007/978-3-642-36406-8_4},
	note = {{ZSCC}: {NoCitationData}[s0] },
	keywords = {irrelevant, Reinforcement Learning, Power Grid, Reactive Power, Reward Function, Solution Concept},
	file = {Lee et al_2013_Counter-Factual Reinforcement Learning.pdf:C\:\\Users\\ohund\\Zotero\\storage\\568AVM36\\Lee et al_2013_Counter-Factual Reinforcement Learning.pdf:application/pdf}
}

@article{samoilescu_model-agnostic_2021,
	title = {Model-agnostic and Scalable Counterfactual Explanations via Reinforcement Learning},
	url = {http://arxiv.org/abs/2106.02597},
	abstract = {Counterfactual instances are a powerful tool to obtain valuable insights into automated decision processes, describing the necessary minimal changes in the input space to alter the prediction towards a desired target. Most previous approaches require a separate, computationally expensive optimization procedure per instance, making them impractical for both large amounts of data and high-dimensional data. Moreover, these methods are often restricted to certain subclasses of machine learning models (e.g. differentiable or tree-based models). In this work, we propose a deep reinforcement learning approach that transforms the optimization procedure into an end-to-end learnable process, allowing us to generate batches of counterfactual instances in a single forward pass. Our experiments on real-world data show that our method i) is model-agnostic (does not assume differentiability), relying only on feedback from model predictions; ii) allows for generating target-conditional counterfactual instances; iii) allows for flexible feature range constraints for numerical and categorical attributes, including the immutability of protected features (e.g. gender, race); iv) is easily extended to other data modalities such as images.},
	journaltitle = {{arXiv}:2106.02597 [cs, stat]},
	author = {Samoilescu, Robert-Florian and Van Looveren, Arnaud and Klaise, Janis},
	urldate = {2021-09-28},
	date = {2021-06-04},
	eprinttype = {arxiv},
	eprint = {2106.02597},
	note = {{ZSCC}: 0000000 },
	keywords = {hot, skimmed, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\VT6KJXMP\\2106.html:text/html;Samoilescu et al_2021_Model-agnostic and Scalable Counterfactual Explanations via Reinforcement.pdf:C\:\\Users\\ohund\\Zotero\\storage\\2GNX2ZSY\\Samoilescu et al_2021_Model-agnostic and Scalable Counterfactual Explanations via Reinforcement.pdf:application/pdf}
}

@inproceedings{brown_deep_2019,
	title = {Deep Counterfactual Regret Minimization},
	url = {https://proceedings.mlr.press/v97/brown19b.html},
	abstract = {Counterfactual Regret Minimization ({CFR}) is the leading algorithm for solving large imperfect-information games. It converges to an equilibrium by iteratively traversing the game tree. In order to deal with extremely large games, abstraction is typically applied before running {CFR}. The abstracted game is solved with tabular {CFR}, and its solution is mapped back to the full game. This process can be problematic because aspects of abstraction are often manual and domain specific, abstraction algorithms may miss important strategic nuances of the game, and there is a chicken-and-egg problem because determining a good abstraction requires knowledge of the equilibrium of the game. This paper introduces Deep Counterfactual Regret Minimization, a form of {CFR} that obviates the need for abstraction by instead using deep neural networks to approximate the behavior of {CFR} in the full game. We show that Deep {CFR} is principled and achieves strong performance in large poker games. This is the first non-tabular variant of {CFR} to be successful in large games.},
	eventtitle = {International Conference on Machine Learning},
	pages = {793--802},
	booktitle = {Proceedings of the 36th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Brown, Noam and Lerer, Adam and Gross, Sam and Sandholm, Tuomas},
	urldate = {2021-09-29},
	date = {2019-05-24},
	langid = {english},
	note = {{ZSCC}: 0000084 
{ISSN}: 2640-3498},
	file = {Brown et al_2019_Deep Counterfactual Regret Minimization.pdf:C\:\\Users\\ohund\\Zotero\\storage\\67DSSIX7\\Brown et al_2019_Deep Counterfactual Regret Minimization.pdf:application/pdf;Supplementary PDF:C\:\\Users\\ohund\\Zotero\\storage\\VCINNVSB\\Brown et al. - 2019 - Deep Counterfactual Regret Minimization.pdf:application/pdf}
}

@inproceedings{van_der_aalst_process_2018,
	location = {San Diego, {CA}, {USA}},
	title = {Process mining and simulation: a match made in heaven!},
	series = {{SummerSim} '18},
	shorttitle = {Process mining and simulation},
	abstract = {Event data are collected everywhere: in logistics, manufacturing, finance, healthcare, e-learning, e-government, and many other domains. The events found in these domains typically refer to activities executed by resources at particular times and for particular cases. Process mining provides the means to discover the real processes, to detect deviations from normative processes, and to analyze bottlenecks and waste from such events. However, process mining tends to be backward-looking. Fortunately, simulation can be used to explore different design alternatives and to anticipate future performance problems. This keynote paper discusses the link between both types of analysis and elaborates on the challenges process discovery techniques are facing. Quality notions such as recall, precision, and generalization are discussed. Rather than introducing a specific process discovery or conformance checking algorithm, the paper provides a comprehensive set of conformance propositions. These conformance propositions serve two purposes: (1) introducing the essence of process mining by discussing the relation between event logs and process models, and (2) discussing possible requirements for the quantification of quality notions related to recall, precision, and generalization.},
	pages = {1--12},
	booktitle = {Proceedings of the 50th Computer Simulation Conference},
	publisher = {Society for Computer Simulation International},
	author = {van der Aalst, Wil M. P.},
	urldate = {2021-09-30},
	date = {2018-07-09},
	note = {{ZSCC}: 0000034},
	keywords = {done, hot, simulation, process mining, conformance checking, process discovery},
	file = {van der Aalst_2018_Process mining and simulation.pdf:C\:\\Users\\ohund\\Zotero\\storage\\A96465Y6\\van der Aalst_2018_Process mining and simulation.pdf:application/pdf}
}

@online{noauthor_celonis_nodate,
	title = {Celonis Academic Alliance - Process Mining Experts Program},
	url = {https://www.celonis.com/acal-thesis-support/},
	abstract = {Read how students use Process Mining to learn and research.},
	titleaddon = {Celonis},
	urldate = {2021-09-30},
	langid = {english},
	file = {Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\N8MGID9R\\acal-thesis-support.html:text/html}
}

@article{liu_survey_2020,
	title = {A Survey on Contextual Embeddings},
	url = {http://arxiv.org/abs/2003.07278},
	abstract = {Contextual embeddings, such as {ELMo} and {BERT}, move beyond global word representations like Word2Vec and achieve ground-breaking performance on a wide range of natural language processing tasks. Contextual embeddings assign each word a representation based on its context, thereby capturing uses of words across varied contexts and encoding knowledge that transfers across languages. In this survey, we review existing contextual embedding models, cross-lingual polyglot pre-training, the application of contextual embeddings in downstream tasks, model compression, and model analyses.},
	journaltitle = {{arXiv}:2003.07278 [cs]},
	author = {Liu, Qi and Kusner, Matt J. and Blunsom, Phil},
	urldate = {2021-09-30},
	date = {2020-04-13},
	eprinttype = {arxiv},
	eprint = {2003.07278},
	note = {{ZSCC}: 0000036 },
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\ohund\\Zotero\\storage\\3UPAYQAE\\Liu et al. - 2020 - A Survey on Contextual Embeddings.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\MXQ8UHI9\\2003.html:text/html}
}

@inproceedings{qafari_case_2021,
	location = {Cham},
	title = {Case Level Counterfactual Reasoning in Process Mining},
	isbn = {978-3-030-79108-7},
	doi = {10.1007/978-3-030-79108-7_7},
	series = {Lecture Notes in Business Information Processing},
	abstract = {Process mining is widely used to diagnose processes and uncover performance and compliance problems. It is also possible to see relations between different behavioral aspects, e.g., cases that deviate more at the beginning of the process tend to get delayed in the later part of the process. However, correlations do not necessarily reveal causalities. Moreover, standard process mining diagnostics do not indicate how to improve the process. This is the reason we advocate the use of structural equation models and counterfactual reasoning. We use results from causal inference and adapt these to be able to reason over event logs and process interventions. We have implemented the approach as a {ProM} plug-in and have evaluated it on several data sets.},
	pages = {55--63},
	booktitle = {Intelligent Information Systems},
	publisher = {Springer International Publishing},
	author = {Qafari, Mahnaz Sadat and van der Aalst, Wil M. P.},
	editor = {Nurcan, Selmin and Korthaus, Axel},
	date = {2021},
	langid = {english},
	note = {{ZSCC}: {NoCitationData}[s0]
tex.ids= qafari\_CaseLevelCounterfactual\_2021a},
	keywords = {hot, Process mining, Counterfactual statement, Structural equation model},
	file = {Qafari_van der Aalst_2021_Case Level Counterfactual Reasoning in Process Mining.pdf:C\:\\Users\\ohund\\Zotero\\storage\\G72NX9D2\\Qafari_van der Aalst_2021_Case Level Counterfactual Reasoning in Process Mining.pdf:application/pdf}
}

@article{bond-taylor_deep_2021,
	title = {Deep Generative Modelling: A Comparative Review of {VAEs}, {GANs}, Normalizing Flows, Energy-Based and Autoregressive Models},
	url = {http://arxiv.org/abs/2103.04922},
	shorttitle = {Deep Generative Modelling},
	abstract = {Deep generative modelling is a class of techniques that train deep neural networks to model the distribution of training samples. Research has fragmented into various interconnected approaches, each of which making trade-offs including run-time, diversity, and architectural restrictions. In particular, this compendium covers energy-based models, variational autoencoders, generative adversarial networks, autoregressive models, normalizing flows, in addition to numerous hybrid approaches. These techniques are drawn under a single cohesive framework, comparing and contrasting to explain the premises behind each, while reviewing current state-of-the-art advances and implementations.},
	journaltitle = {{arXiv}:2103.04922 [cs, stat]},
	author = {Bond-Taylor, Sam and Leach, Adam and Long, Yang and Willcocks, Chris G.},
	urldate = {2021-10-01},
	date = {2021-04-14},
	eprinttype = {arxiv},
	eprint = {2103.04922},
	note = {{ZSCC}: 0000010 },
	keywords = {done, hot, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, 68T01 (Primary), 68T07 (Secondary), G.3, I.4.0, I.5.0},
	file = {arXiv Fulltext PDF:C\:\\Users\\ohund\\Zotero\\storage\\TVCGFQZF\\Bond-Taylor et al. - 2021 - Deep Generative Modelling A Comparative Review of.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\ZJTHZSZK\\2103.html:text/html}
}

@article{otten_event_2021,
	title = {Event generation and statistical sampling for physics with deep generative models and a density information buffer},
	volume = {12},
	rights = {2021 The Author(s)},
	issn = {2041-1723},
	url = {http://www.nature.com/articles/s41467-021-22616-z},
	doi = {10.1038/s41467-021-22616-z},
	abstract = {Simulating nature and in particular processes in particle physics require expensive computations and sometimes would take much longer than scientists can afford. Here, we explore ways to a solution for this problem by investigating recent advances in generative modeling and present a study for the generation of events from a physical process with deep generative models. The simulation of physical processes requires not only the production of physical events, but to also ensure that these events occur with the correct frequencies. We investigate the feasibility of learning the event generation and the frequency of occurrence with several generative machine learning models to produce events like Monte Carlo generators. We study three processes: a simple two-body decay, the processes e+e− → Z → l+l− and \$\$pp{\textbackslash}to t{\textbackslash}bar\{t\}\$\$including the decay of the top quarks and a simulation of the detector response. By buffering density information of encoded Monte Carlo events given the encoder of a Variational Autoencoder we are able to construct a prior for the sampling of new events from the decoder that yields distributions that are in very good agreement with real Monte Carlo events and are generated several orders of magnitude faster. Applications of this work include generic density estimation and sampling, targeted event generation via a principal component analysis of encoded ground truth data, anomaly detection and more efficient importance sampling, e.g., for the phase space integration of matrix elements in quantum field theories.},
	pages = {2985},
	number = {1},
	journaltitle = {Nature Communications},
	shortjournal = {Nat Commun},
	author = {Otten, Sydney and Caron, Sascha and de Swart, Wieske and van Beekveld, Melissa and Hendriks, Luc and van Leeuwen, Caspar and Podareanu, Damian and Ruiz de Austri, Roberto and Verheyen, Rob},
	urldate = {2021-10-03},
	date = {2021-05-20},
	langid = {english},
	note = {{ZSCC}: {NoCitationData}[s0] 
Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Characterization and analytical techniques;Information theory and computation;Statistics
Subject\_term\_id: characterization-and-analytical-techniques;information-theory-and-computation;statistics},
	file = {Full Text PDF:C\:\\Users\\ohund\\Zotero\\storage\\RICY2KYN\\Otten et al. - 2021 - Event generation and statistical sampling for phys.pdf:application/pdf;Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\49CST9A6\\s41467-021-22616-z.html:text/html}
}

@online{salehi_variational_2021,
	title = {Variational Inference with Normalizing Flows on {MNIST}},
	url = {https://towardsdatascience.com/variational-inference-with-normalizing-flows-on-mnist-9258bbcf8810},
	abstract = {In this post, I will explain what normalizing flows are and how they can be used in variational inference and designing generative models…},
	titleaddon = {Medium},
	author = {Salehi, Mohammadreza},
	urldate = {2021-10-03},
	date = {2021-04-09},
	langid = {english},
	note = {{ZSCC}: {NoCitationData}[s0]},
	file = {Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\55UK2WHD\\variational-inference-with-normalizing-flows-on-mnist-9258bbcf8810.html:text/html}
}

@online{barla_how_2021,
	title = {How to Code {BERT} Using {PyTorch} - Tutorial With Examples},
	url = {https://neptune.ai/blog/how-to-code-bert-using-pytorch-tutorial},
	abstract = {If you are an {NLP} enthusiast then you might have heard about {BERT}. In this article, we are going to explore {BERT}: what it is? and how it works?, and learn how to code it using {PyTorch}. In 2018, Google published a paper titled “Pre-training of deep bidirectional transformers for language understanding”. In this paper, […]},
	titleaddon = {neptune.ai},
	author = {Barla, Nilesh},
	urldate = {2021-10-02},
	date = {2021-05-20},
	langid = {american},
	keywords = {blog, code},
	file = {Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\QAB77MC5\\how-to-code-bert-using-pytorch-tutorial.html:text/html}
}

@online{ramani_deep_2020,
	title = {Deep Causal Generative Modelling — A Brief Tutorial},
	url = {https://medium.com/swlh/causal-generative-modelling-a-brief-tutorial-with-game-character-images-728d3450b600},
	abstract = {Integrating a causal model into a deep learning architecture.},
	titleaddon = {The Startup},
	author = {Ramani, Harish},
	urldate = {2021-10-02},
	date = {2020-09-30},
	langid = {english},
	note = {{ZSCC}: {NoCitationData}[s0]},
	keywords = {hot}
}

@online{noauthor_representing_nodate,
	title = {Representing text as features: Tokenizers, {TextFields}, and {TextFieldEmbedders} · A Guide to Natural Language Processing With {AllenNLP}},
	url = {https://guide.allennlp.org/representing-text-as-features},
	shorttitle = {Representing text as features},
	abstract = {A deep dive into {AllenNLP}'s core abstraction: how exactly we represent textual inputs, both on the data side and the model side.},
	titleaddon = {A Guide to Natural Language Processing With {AllenNLP}},
	urldate = {2021-10-02},
	langid = {english},
	file = {Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\RGPJKYZ3\\representing-text-as-features.html:text/html}
}

@online{omray_introduction_2021,
	title = {Introduction to Normalizing Flows},
	url = {https://towardsdatascience.com/introduction-to-normalizing-flows-d002af262a4b},
	abstract = {Why and how to implement normalizing flows over {GANs} and {VAEs}},
	titleaddon = {Medium},
	author = {Omray, Aryansh},
	urldate = {2021-10-07},
	date = {2021-07-16},
	langid = {english},
	note = {{ZSCC}: {NoCitationData}[s0]},
	file = {Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\MU5LM2HL\\introduction-to-normalizing-flows-d002af262a4b.html:text/html}
}

@article{van_der_aalst_process_2012,
	title = {Process Mining: Overview and Opportunities},
	volume = {3},
	issn = {2158-656X},
	url = {https://doi.org/10.1145/2229156.2229157},
	doi = {10.1145/2229156.2229157},
	shorttitle = {Process Mining},
	abstract = {Over the last decade, process mining emerged as a new research field that focuses on the analysis of processes using event data. Classical data mining techniques such as classification, clustering, regression, association rule learning, and sequence/episode mining do not focus on business process models and are often only used to analyze a specific step in the overall process. Process mining focuses on end-to-end processes and is possible because of the growing availability of event data and new process discovery and conformance checking techniques. Process models are used for analysis (e.g., simulation and verification) and enactment by {BPM}/{WFM} systems. Previously, process models were typically made by hand without using event data. However, activities executed by people, machines, and software leave trails in so-called event logs. Process mining techniques use such logs to discover, analyze, and improve business processes. Recently, the Task Force on Process Mining released the Process Mining Manifesto. This manifesto is supported by 53 organizations and 77 process mining experts contributed to it. The active involvement of end-users, tool vendors, consultants, analysts, and researchers illustrates the growing significance of process mining as a bridge between data mining and business process modeling. The practical relevance of process mining and the interesting scientific challenges make process mining one of the “hot” topics in Business Process Management ({BPM}). This article introduces process mining as a new research field and summarizes the guiding principles and challenges described in the manifesto.},
	pages = {7:1--7:17},
	number = {2},
	journaltitle = {{ACM} Transactions on Management Information Systems},
	shortjournal = {{ACM} Trans. Manage. Inf. Syst.},
	author = {van der Aalst, Wil},
	urldate = {2021-10-15},
	date = {2012-07-01},
	note = {{ZSCC}: 0000351},
	keywords = {done, hot, data mining, Process mining, business intelligence, business process management},
	file = {van der Aalst_2012_Process Mining.pdf:C\:\\Users\\ohund\\Zotero\\storage\\323IRJ79\\van der Aalst_2012_Process Mining.pdf:application/pdf}
}

@incollection{hinton_practical_2012,
	location = {Berlin, Heidelberg},
	title = {A Practical Guide to Training Restricted Boltzmann Machines},
	isbn = {978-3-642-35289-8},
	url = {https://doi.org/10.1007/978-3-642-35289-8_32},
	series = {Lecture Notes in Computer Science},
	abstract = {Restricted Boltzmann machines ({RBMs}) have been used as generative models of many different types of data. {RBMs} are usually trained using the contrastive divergence learning procedure. This requires a certain amount of practical experience to decide how to set the values of numerical meta-parameters. Over the last few years, the machine learning group at the University of Toronto has acquired considerable expertise at training {RBMs} and this guide is an attempt to share this expertise with other machine learning researchers.},
	pages = {599--619},
	booktitle = {Neural Networks: Tricks of the Trade: Second Edition},
	publisher = {Springer},
	author = {Hinton, Geoffrey E.},
	editor = {Montavon, Grégoire and Orr, Geneviève B. and Müller, Klaus-Robert},
	urldate = {2021-10-13},
	date = {2012},
	langid = {english},
	doi = {10.1007/978-3-642-35289-8_32},
	note = {{ZSCC}: {NoCitationData}[s0] },
	keywords = {done, Hide Unit, Learning Rate, Reconstruction Error, Restrict Boltzmann Machine, Training Case},
	file = {Springer Full Text PDF:C\:\\Users\\ohund\\Zotero\\storage\\9LCK32ZV\\Hinton - 2012 - A Practical Guide to Training Restricted Boltzmann.pdf:application/pdf}
}

@article{grisold_adoption_2020,
	title = {Adoption, use and management of process mining in practice},
	volume = {27},
	issn = {1463-7154},
	url = {https://doi.org/10.1108/BPMJ-03-2020-0112},
	doi = {10.1108/BPMJ-03-2020-0112},
	abstract = {Purpose This study explores how process managers perceive the adoption, use and management of process mining in practice. While research in process mining predominantly focuses on the technical aspects, our work highlights organizational and managerial implications. Design/methodology/approach We report on a focus group study conducted with process managers from various industries in Central Europe. This setting allowed us to gain diverse and in-depth insights about the needs and expectations of practitioners in relation to the adoption, use and management of process mining. Findings We find that process managers face four central challenges. These challenges are largely related to four stages; (1) planning and business case calculation, (2) process selection, (3) implementation, and (4) process mining use. Research limitations/implications We point to research opportunities in relation to the adoption, use and management of process mining. We suggest that future research should apply interdisciplinary study designs to better understand the managerial and organizational implications of process mining. Practical implications The reported challenges have various practical implications at the organizational and managerial level. We explore how existing {BPM} frameworks can be extended to meet these challenges. Originality/value This study is among the first attempts to explore process mining from the perspective of process managers. It clarifies important challenges and points to avenues for future research.},
	pages = {369--387},
	number = {2},
	journaltitle = {Business Process Management Journal},
	author = {Grisold, Thomas and Mendling, Jan and Otto, Markus and vom Brocke, Jan},
	urldate = {2021-10-15},
	date = {2020-01-01},
	note = {{ZSCC}: 0000012 
Publisher: Emerald Publishing Limited},
	keywords = {Process mining, Data privacy, Focus group, Governance, Leadership, Organizational implications},
	file = {Grisold et al_2020_Adoption, use and management of process mining in practice.pdf:C\:\\Users\\ohund\\Zotero\\storage\\65Q2N8AN\\Grisold et al_2020_Adoption, use and management of process mining in practice.pdf:application/pdf}
}

@online{noauthor_stanford_nodate,
	title = {Stanford {CS}236 - Deep Generative Modelling},
	url = {https://deepgenerativemodels.github.io/notes/},
	urldate = {2021-10-17},
	file = {Contents:C\:\\Users\\ohund\\Zotero\\storage\\KKQLI8NU\\notes.html:text/html}
}

@article{oord_wavenet_2016,
	title = {{WaveNet}: A Generative Model for Raw Audio},
	url = {http://arxiv.org/abs/1609.03499},
	shorttitle = {{WaveNet}},
	abstract = {This paper introduces {WaveNet}, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single {WaveNet} can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
	journaltitle = {{arXiv}:1609.03499 [cs]},
	author = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
	urldate = {2021-10-18},
	date = {2016-09-19},
	eprinttype = {arxiv},
	eprint = {1609.03499},
	note = {{ZSCC}: 0003297 },
	keywords = {Computer Science - Machine Learning, Computer Science - Sound},
	file = {arXiv Fulltext PDF:C\:\\Users\\ohund\\Zotero\\storage\\TD83MDLF\\Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\NDXFNTH7\\1609.html:text/html}
}

@article{jozefowicz_exploring_2016,
	title = {Exploring the Limits of Language Modeling},
	url = {http://arxiv.org/abs/1602.02410},
	abstract = {In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7. We also release these models for the {NLP} and {ML} community to study and improve upon.},
	journaltitle = {{arXiv}:1602.02410 [cs]},
	author = {Jozefowicz, Rafal and Vinyals, Oriol and Schuster, Mike and Shazeer, Noam and Wu, Yonghui},
	urldate = {2021-10-18},
	date = {2016-02-11},
	eprinttype = {arxiv},
	eprint = {1602.02410},
	note = {{ZSCC}: 0000982 },
	keywords = {Computer Science - Computation and Language, done},
	file = {arXiv Fulltext PDF:C\:\\Users\\ohund\\Zotero\\storage\\3WTT45A2\\Jozefowicz et al. - 2016 - Exploring the Limits of Language Modeling.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\DIKLRQHR\\1602.html:text/html}
}

@article{oord_pixel_2016,
	title = {Pixel Recurrent Neural Networks},
	url = {http://arxiv.org/abs/1601.06759},
	abstract = {Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse {ImageNet} dataset. Samples generated from the model appear crisp, varied and globally coherent.},
	journaltitle = {{arXiv}:1601.06759 [cs]},
	author = {Oord, Aaron van den and Kalchbrenner, Nal and Kavukcuoglu, Koray},
	urldate = {2021-10-18},
	date = {2016-08-19},
	eprinttype = {arxiv},
	eprint = {1601.06759},
	note = {{ZSCC}: 0001610 },
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv.org Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\CCPGPVSG\\1601.html:text/html;Oord et al_2016_Pixel Recurrent Neural Networks.pdf:C\:\\Users\\ohund\\Zotero\\storage\\5IVW9HK6\\Oord et al_2016_Pixel Recurrent Neural Networks.pdf:application/pdf}
}

@video{nptel-noc_iitm_deep_2019,
	title = {Deep Learning Part - {II} ({CS}7015): Lec 21.2 Masked Autoencoder Density Estimator ({MADE})},
	url = {https://www.youtube.com/watch?v=lNW8T0W-xeE},
	shorttitle = {Deep Learning Part - {II} ({CS}7015)},
	abstract = {Deep Learning Part - {II} ({CS}7015): Lec 21.2 Masked Autoencoder Density Estimator ({MADE})},
	author = {{NPTEL-NOC IITM}},
	urldate = {2021-10-18},
	date = {2019-04-19},
	note = {{ZSCC}: {NoCitationData}[s0]}
}

@online{lee_tensorflow_2019,
	title = {[Tensorflow] Implementing Temporal Convolutional Networks},
	url = {https://medium.com/the-artificial-impostor/notes-understanding-tensorflow-part-3-7f6633fcc7c7},
	abstract = {Understanding Tensorflow Part 3},
	titleaddon = {Veritable},
	author = {Lee, Ceshine},
	urldate = {2021-10-18},
	date = {2019-03-14},
	langid = {english},
	note = {{ZSCC}: {NoCitationData}[s0]},
	file = {Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\UK5L7ZSG\\notes-understanding-tensorflow-part-3-7f6633fcc7c7.html:text/html}
}

@article{chen_probabilistic_2020,
	title = {Probabilistic forecasting with temporal convolutional neural network},
	volume = {399},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231220303441},
	doi = {10.1016/j.neucom.2020.03.011},
	abstract = {We present a probabilistic forecasting framework based on convolutional neural network ({CNN}) for multiple related time series forecasting. The framework can be applied to estimate probability density under both parametric and non-parametric settings. More specifically, stacked residual blocks based on dilated causal convolutional nets are constructed to capture the temporal dependencies of the series. Combined with representation learning, our approach is able to learn complex patterns such as seasonality, holiday effects within and across series, and to leverage those patterns for more accurate forecasts, especially when historical data is sparse or unavailable. Extensive empirical studies are performed on several real-world datasets, including datasets from {JD}.com, China’s largest online retailer. The results show that our framework compares favorably to the state-of-the-art in both point and probabilistic forecasting.},
	pages = {491--501},
	journaltitle = {Neurocomputing},
	shortjournal = {Neurocomputing},
	author = {Chen, Yitian and Kang, Yanfei and Chen, Yixiong and Wang, Zizhuo},
	urldate = {2021-10-19},
	date = {2020-07-25},
	langid = {english},
	note = {{ZSCC}: 0000050
tex.ids= chen\_ProbabilisticForecastingTemporal\_2020a},
	keywords = {Convolutional neural network, Demand forecasting, Dilated causal convolution, High-dimensional time series, Probabilistic forecasting},
	file = {Chen et al_2020_Probabilistic forecasting with temporal convolutional neural network.pdf:C\:\\Users\\ohund\\Zotero\\storage\\6BFV9DDJ\\Chen et al_2020_Probabilistic forecasting with temporal convolutional neural network.pdf:application/pdf;Eingereichte Version:C\:\\Users\\ohund\\Zotero\\storage\\KY7BWC9F\\Chen et al. - 2020 - Probabilistic forecasting with temporal convolutio.pdf:application/pdf}
}

@article{lea_temporal_2016,
	title = {Temporal Convolutional Networks: A Unified Approach to Action Segmentation},
	url = {http://arxiv.org/abs/1608.08242},
	shorttitle = {Temporal Convolutional Networks},
	abstract = {The dominant paradigm for video-based action segmentation is composed of two steps: first, for each frame, compute low-level features using Dense Trajectories or a Convolutional Neural Network that encode spatiotemporal information locally, and second, input these features into a classifier that captures high-level temporal relationships, such as a Recurrent Neural Network ({RNN}). While often effective, this decoupling requires specifying two separate models, each with their own complexities, and prevents capturing more nuanced long-range spatiotemporal relationships. We propose a unified approach, as demonstrated by our Temporal Convolutional Network ({TCN}), that hierarchically captures relationships at low-, intermediate-, and high-level time-scales. Our model achieves superior or competitive performance using video or sensor data on three public action segmentation datasets and can be trained in a fraction of the time it takes to train an {RNN}.},
	journaltitle = {{arXiv}:1608.08242 [cs]},
	author = {Lea, Colin and Vidal, Rene and Reiter, Austin and Hager, Gregory D.},
	urldate = {2021-10-19},
	date = {2016-08-29},
	eprinttype = {arxiv},
	eprint = {1608.08242},
	note = {{ZSCC}: 0000276 },
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\ohund\\Zotero\\storage\\G3S5IFE5\\Lea et al. - 2016 - Temporal Convolutional Networks A Unified Approac.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\Y4UYN6LD\\1608.html:text/html}
}

@online{noauthor_pytorch_nodate,
	title = {({PyTorch}) Temporal Convolutional Networks},
	url = {https://kaggle.com/ceshine/pytorch-temporal-convolutional-networks},
	abstract = {Explore and run machine learning code with Kaggle Notebooks {\textbar} Using data from Don't call me turkey!},
	urldate = {2021-10-19},
	langid = {english},
	keywords = {code},
	file = {Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\GF6U6KH2\\pytorch-temporal-convolutional-networks.html:text/html}
}

@online{noauthor_tcn_nodate,
	title = {{TCN} (Temporal Convolutional Network) {\textbar} tsai},
	url = {https://timeseriesai.github.io/tsai/models.TCN.html},
	urldate = {2021-10-19},
	keywords = {code},
	file = {TCN (Temporal Convolutional Network) | tsai:C\:\\Users\\ohund\\Zotero\\storage\\3YQ7MKST\\models.TCN.html:text/html}
}

@software{noauthor_sequence_2021,
	title = {Sequence Modeling Benchmarks and Temporal Convolutional Networks ({TCN})},
	rights = {{MIT}},
	url = {https://github.com/locuslab/TCN/blob/2221de3323032faef7bde744b0e211bd274cb2d0/TCN/tcn.py},
	abstract = {Sequence modeling benchmarks and temporal convolutional networks},
	publisher = {{CMU} Locus Lab},
	urldate = {2021-10-19},
	date = {2021-10-19},
	note = {original-date: 2018-03-02T22:56:39Z},
	keywords = {code}
}

@online{noauthor_allennlp_nodate,
	title = {{AllenNLP}},
	url = {https://allennlp.org/%PUBLIC_URL%},
	abstract = {{AllenNLP} is a free, open-source natural language processing platform for building state of the art models.},
	urldate = {2021-10-20},
	langid = {english},
	file = {Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\SIDXVJZR\\allennlp.org.html:text/html}
}

@online{noauthor_next_token_lm_nodate,
	title = {next\_token\_lm - {AllenNLP} Models v2.7.0},
	url = {https://docs.allennlp.org/models/main/models/lm/models/next_token_lm/},
	urldate = {2021-10-20},
	file = {next_token_lm - AllenNLP Models v2.7.0:C\:\\Users\\ohund\\Zotero\\storage\\LEBQZ5BB\\next_token_lm.html:text/html}
}

@online{noauthor_allennlp-modelsallennlp_modelslmdataset_readers_nodate,
	title = {allennlp-models/allennlp\_models/lm/dataset\_readers at main · allenai/allennlp-models},
	url = {https://github.com/allenai/allennlp-models},
	abstract = {Officially supported {AllenNLP} models. Contribute to allenai/allennlp-models development by creating an account on {GitHub}.},
	titleaddon = {{GitHub}},
	urldate = {2021-10-20},
	langid = {english},
	file = {Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\KGPX58WC\\dataset_readers.html:text/html}
}

@software{noauthor_allenaiallennlp-models_2021,
	title = {allenai/allennlp-models},
	rights = {Apache-2.0},
	url = {https://github.com/allenai/allennlp-models/blob/c814aa1ab48b94b18d894d9746d966627c2d3958/allennlp_models/lm/dataset_readers/next_token_lm.py},
	abstract = {Officially supported {AllenNLP} models},
	publisher = {{AI}2},
	urldate = {2021-10-20},
	date = {2021-10-12},
	note = {original-date: 2020-03-10T00:22:21Z}
}

@online{noauthor_convolutions_nodate,
	title = {Convolutions in Autoregressive Neural Networks},
	url = {https://theblog.github.io},
	abstract = {This post explains how to use one-dimensional causal and dilated convolutions in autoregressive neural networks such as {WaveNet}.},
	titleaddon = {The Blog},
	urldate = {2021-10-24},
	langid = {english},
	file = {Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\9Z9QBB87\\convolution-in-autoregressive-neural-networks.html:text/html}
}

@inproceedings{hsieh_dice4el_2021,
	location = {Eindhoven, Netherlands},
	title = {{DiCE}4EL: Interpreting Process Predictions using a Milestone-Aware Counterfactual Approach},
	isbn = {978-1-66543-514-7},
	url = {https://ieeexplore.ieee.org/document/9576881/},
	doi = {10.1109/ICPM53251.2021.9576881},
	shorttitle = {{DiCE}4EL},
	abstract = {Predictive process analytics often apply machine learning to predict the future states of a running business process. However, the internal mechanisms of many existing predictive algorithms are opaque and a human decision-maker is unable to understand why a certain activity was predicted. Recently, counterfactuals have been proposed in the literature to derive human-understandable explanations from predictive models. Current counterfactual approaches consist of ﬁnding the minimum feature change that can make a certain prediction ﬂip its outcome. Although many algorithms have been proposed, their application to multi-dimensional sequence data like event logs has not been explored in the literature.},
	eventtitle = {2021 3rd International Conference on Process Mining ({ICPM})},
	pages = {88--95},
	booktitle = {2021 3rd International Conference on Process Mining ({ICPM})},
	publisher = {{IEEE}},
	author = {Hsieh, Chihcheng and Moreira, Catarina and Ouyang, Chun},
	urldate = {2021-11-04},
	date = {2021-10-31},
	langid = {english},
	note = {{ZSCC}: 0000001
tex.ids= hsieh\_DiCE4ELInterpretingProcess\_2021a},
	keywords = {done, Machine learning, interpretability, Machine learning algorithms, Training, Counterfactual, explainable {AI}, Prediction algorithms, Predictive models, predictive process analytics, Protocols, Search problems},
	file = {Hsieh et al_2021_DiCE4EL.pdf:C\:\\Users\\ohund\\Zotero\\storage\\HFUNGZXL\\Hsieh et al_2021_DiCE4EL.pdf:application/pdf;Hsieh et al. - 2021 - DiCE4EL Interpreting Process Predictions using a .pdf:C\:\\Users\\ohund\\Zotero\\storage\\SKPNU5B3\\Hsieh et al. - 2021 - DiCE4EL Interpreting Process Predictions using a .pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\ohund\\Zotero\\storage\\R77U3DQB\\9576881.html:text/html}
}

@article{carvalho_machine_2019,
	title = {Machine Learning Interpretability: A Survey on Methods and Metrics},
	volume = {8},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2079-9292/8/8/832},
	doi = {10.3390/electronics8080832},
	shorttitle = {Machine Learning Interpretability},
	abstract = {Machine learning systems are becoming increasingly ubiquitous. These systems\&rsquo;s adoption has been expanding, accelerating the shift towards a more algorithmic society, meaning that algorithmically informed decisions have greater potential for significant social impact. However, most of these accurate decision support systems remain complex black boxes, meaning their internal logic and inner workings are hidden to the user and even experts cannot fully understand the rationale behind their predictions. Moreover, new regulations and highly regulated domains have made the audit and verifiability of decisions mandatory, increasing the demand for the ability to question, understand, and trust machine learning systems, for which interpretability is indispensable. The research community has recognized this interpretability problem and focused on developing both interpretable models and explanation methods over the past few years. However, the emergence of these methods shows there is no consensus on how to assess the explanation quality. Which are the most suitable metrics to assess the quality of an explanation? The aim of this article is to provide a review of the current state of the research field on machine learning interpretability while focusing on the societal impact and on the developed methods and metrics. Furthermore, a complete literature review is presented in order to identify future directions of work on this field.},
	pages = {832},
	number = {8},
	journaltitle = {Electronics},
	author = {Carvalho, Diogo V. and Pereira, Eduardo M. and Cardoso, Jaime S.},
	urldate = {2021-11-09},
	date = {2019-08},
	langid = {english},
	note = {{ZSCC}: 0000348 
Number: 8
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {irrelevant, {XAI}, explainability, interpretability, machine learning},
	file = {Full Text PDF:C\:\\Users\\ohund\\Zotero\\storage\\I8J98D29\\Carvalho et al. - 2019 - Machine Learning Interpretability A Survey on Met.pdf:application/pdf;Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\EVQWHZHQ\\832.html:text/html}
}

@article{wang_controllable_2019,
	title = {Controllable Unsupervised Text Attribute Transfer via Editing Entangled Latent Representation},
	url = {http://arxiv.org/abs/1905.12926},
	abstract = {Unsupervised text attribute transfer automatically transforms a text to alter a specific attribute (e.g. sentiment) without using any parallel data, while simultaneously preserving its attribute-independent content. The dominant approaches are trying to model the content-independent attribute separately, e.g., learning different attributes' representations or using multiple attribute-specific decoders. However, it may lead to inflexibility from the perspective of controlling the degree of transfer or transferring over multiple aspects at the same time. To address the above problems, we propose a more flexible unsupervised text attribute transfer framework which replaces the process of modeling attribute with minimal editing of latent representations based on an attribute classifier. Specifically, we first propose a Transformer-based autoencoder to learn an entangled latent representation for a discrete text, then we transform the attribute transfer task to an optimization problem and propose the Fast-Gradient-Iterative-Modification algorithm to edit the latent representation until conforming to the target attribute. Extensive experimental results demonstrate that our model achieves very competitive performance on three public data sets. Furthermore, we also show that our model can not only control the degree of transfer freely but also allow to transfer over multiple aspects at the same time.},
	journaltitle = {{arXiv}:1905.12926 [cs]},
	author = {Wang, Ke and Hua, Hang and Wan, Xiaojun},
	urldate = {2021-11-09},
	date = {2019-12-12},
	eprinttype = {arxiv},
	eprint = {1905.12926},
	note = {{ZSCC}: 0000038 },
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ohund\\Zotero\\storage\\VWK23IEV\\Wang et al. - 2019 - Controllable Unsupervised Text Attribute Transfer .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\XIPP5NHC\\1905.html:text/html}
}

@online{noauthor_multivariate_2018,
	title = {Multivariate Time Series {\textbar} Vector Auto Regression ({VAR})},
	url = {https://www.analyticsvidhya.com/blog/2018/09/multivariate-time-series-guide-forecasting-modeling-python-codes/},
	abstract = {Vector Auto Regression method for forecasting multivariate time series uses vectors to represent the relationship between variables and past values.},
	titleaddon = {Analytics Vidhya},
	urldate = {2021-11-09},
	date = {2018-09-27},
	langid = {english},
	file = {Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\EKRS97CW\\multivariate-time-series-guide-forecasting-modeling-python-codes.html:text/html}
}

@online{lendave_how_2021,
	title = {How To Do Multivariate Time Series Forecasting Using {LSTM}},
	url = {https://analyticsindiamag.com/how-to-do-multivariate-time-series-forecasting-using-lstm/},
	abstract = {Time series forecasting is also an important area in machine learning. However, it is neglected due to its complexity, and this complexity.},
	titleaddon = {Analytics India Magazine},
	author = {Lendave, Vijaysinh},
	urldate = {2021-11-09},
	date = {2021-07-11},
	langid = {american},
	note = {{ZSCC}: {NoCitationData}[s0]}
}

@online{grigsby_multivariate_2021,
	title = {Multivariate Time Series Forecasting with Transformers},
	url = {https://towardsdatascience.com/multivariate-time-series-forecasting-with-transformers-384dc6ce989b},
	abstract = {Spatiotemporal Learning Without a Graph},
	titleaddon = {Medium},
	author = {Grigsby, Jake},
	urldate = {2021-11-09},
	date = {2021-10-29},
	langid = {english},
	note = {{ZSCC}: {NoCitationData}[s0]
tex.ids= grigsby\_MultivariateTimeSeries\_2021a, grigsby\_MultivariateTimeSeries\_2021b},
	file = {Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\A5DW2UEB\\multivariate-time-series-forecasting-with-transformers-384dc6ce989b.html:text/html;Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\RW4PU6UK\\multivariate-time-series-forecasting-with-transformers-384dc6ce989b.html:text/html;Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\2AZ7D8D8\\multivariate-time-series-forecasting-with-transformers-384dc6ce989b.html:text/html}
}

@online{noauthor_papers_nodate,
	title = {Papers with Code - {BPI} challenge '12 Benchmark (Multivariate Time Series Forecasting)},
	url = {https://paperswithcode.com/sota/multivariate-time-series-forecasting-on-bpi},
	abstract = {The current state-of-the-art on {BPI} challenge '12 is {QuerySelector}. See a full comparison of 2 papers with code.},
	urldate = {2021-11-09},
	langid = {english},
	file = {Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\CG7IAFHF\\multivariate-time-series-forecasting-on-bpi.html:text/html}
}

@article{klimek_long-term_2021,
	title = {Long-term series forecasting with Query Selector -- efficient model of sparse attention},
	url = {http://arxiv.org/abs/2107.08687},
	abstract = {Various modifications of {TRANSFORMER} were recently used to solve time-series forecasting problem. We propose Query Selector - an efficient, deterministic algorithm for sparse attention matrix. Experiments show it achieves state-of-the art results on {ETT}, Helpdesk and {BPI}'12 datasets.},
	journaltitle = {{arXiv}:2107.08687 [cs]},
	author = {Klimek, Jacek and Klimek, Jakub and Kraskiewicz, Witold and Topolewski, Mateusz},
	urldate = {2021-11-09},
	date = {2021-08-17},
	eprinttype = {arxiv},
	eprint = {2107.08687},
	note = {{ZSCC}: 0000001
tex.ids= klimek\_LongtermSeriesForecasting\_2021a
{versionNumber}: 2},
	keywords = {Computer Science - Machine Learning, Code Available},
	file = {arXiv Fulltext PDF:C\:\\Users\\ohund\\Zotero\\storage\\8H8NYGFM\\Klimek et al. - 2021 - Long-term series forecasting with Query Selector -.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\CMJM2V4Q\\2107.html:text/html;arXiv.org Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\KJ77LFCD\\2107.html:text/html;Klimek et al_2021_Long-term series forecasting with Query Selector -- efficient model of sparse.pdf:C\:\\Users\\ohund\\Zotero\\storage\\5RGPVPYH\\Klimek et al_2021_Long-term series forecasting with Query Selector -- efficient model of sparse.pdf:application/pdf}
}

@article{graves_generating_2014,
	title = {Generating Sequences With Recurrent Neural Networks},
	url = {http://arxiv.org/abs/1308.0850},
	abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
	journaltitle = {{arXiv}:1308.0850 [cs]},
	author = {Graves, Alex},
	urldate = {2021-12-09},
	date = {2014-06-05},
	eprinttype = {arxiv},
	eprint = {1308.0850},
	note = {{ZSCC}: 0000001 },
	keywords = {Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv.org Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\G3MA22UR\\1308.html:text/html;Graves_2014_Generating Sequences With Recurrent Neural Networks.pdf:C\:\\Users\\ohund\\Zotero\\storage\\NAZX7VIE\\Graves_2014_Generating Sequences With Recurrent Neural Networks.pdf:application/pdf}
}

@inproceedings{tax_predictive_2017,
	location = {Cham},
	title = {Predictive Business Process Monitoring with {LSTM} Neural Networks},
	isbn = {978-3-319-59536-8},
	doi = {10.1007/978-3-319-59536-8_30},
	series = {Lecture Notes in Computer Science},
	abstract = {Predictive business process monitoring methods exploit logs of completed cases of a process in order to make predictions about running cases thereof. Existing methods in this space are tailor-made for specific prediction tasks. Moreover, their relative accuracy is highly sensitive to the dataset at hand, thus requiring users to engage in trial-and-error and tuning when applying them in a specific setting. This paper investigates Long Short-Term Memory ({LSTM}) neural networks as an approach to build consistently accurate models for a wide range of predictive process monitoring tasks. First, we show that {LSTMs} outperform existing techniques to predict the next event of a running case and its timestamp. Next, we show how to use models for predicting the next task in order to predict the full continuation of a running case. Finally, we apply the same approach to predict the remaining time, and show that this approach outperforms existing tailor-made methods.},
	pages = {477--492},
	booktitle = {Advanced Information Systems Engineering},
	publisher = {Springer International Publishing},
	author = {Tax, Niek and Verenich, Ilya and La Rosa, Marcello and Dumas, Marlon},
	editor = {Dubois, Eric and Pohl, Klaus},
	date = {2017},
	langid = {english},
	note = {{ZSCC}: {NoCitationData}[s0]},
	keywords = {done, hot, Activity Prediction, Levenshtein Distance, Prediction Point, Recurrent Neural Network, Transition System State},
	file = {Tax et al_2017_Predictive Business Process Monitoring with LSTM Neural Networks.pdf:C\:\\Users\\ohund\\Zotero\\storage\\WR2GFPG9\\Tax et al_2017_Predictive Business Process Monitoring with LSTM Neural Networks.pdf:application/pdf}
}

@article{waterman_biological_1976,
	title = {Some biological sequence metrics},
	volume = {20},
	issn = {0001-8708},
	url = {https://www.sciencedirect.com/science/article/pii/0001870876902024},
	doi = {10.1016/0001-8708(76)90202-4},
	abstract = {Some new metrics are introduced to measure the distance between biological sequences, such as amino acid sequences or nucleotide sequences. These metrics generalize a metric of Sellers, who considered only single deletions, mutations, and insertions. The present metrics allow, for example, multiple deletions and insertions and single mutations. They also allow computation of the distance among more than two sequences. Algorithms for computing the values of the metrics are given which also compute best alignments. The connection with the information theory approach of Reichert, Cohen, and Wong is discussed.},
	pages = {367--387},
	number = {3},
	journaltitle = {Advances in Mathematics},
	shortjournal = {Advances in Mathematics},
	author = {Waterman, M. S and Smith, T. F and Beyer, W. A},
	urldate = {2021-12-16},
	date = {1976-06-01},
	langid = {english},
	note = {{ZSCC}: 0000540},
	file = {Waterman et al_1976_Some biological sequence metrics.pdf:C\:\\Users\\ohund\\Zotero\\storage\\UB9C7BBS\\Waterman et al_1976_Some biological sequence metrics.pdf:application/pdf}
}

@article{augusto_automated_2019,
	title = {Automated Discovery of Process Models from Event Logs: Review and Benchmark},
	volume = {31},
	issn = {1558-2191},
	doi = {10.1109/TKDE.2018.2841877},
	shorttitle = {Automated Discovery of Process Models from Event Logs},
	abstract = {Process mining allows analysts to exploit logs of historical executions of business processes to extract insights regarding the actual performance of these processes. One of the most widely studied process mining operations is automated process discovery. An automated process discovery method takes as input an event log, and produces as output a business process model that captures the control-flow relations between tasks that are observed in or implied by the event log. Various automated process discovery methods have been proposed in the past two decades, striking different tradeoffs between scalability, accuracy, and complexity of the resulting models. However, these methods have been evaluated in an ad-hoc manner, employing different datasets, experimental setups, evaluation measures, and baselines, often leading to incomparable conclusions and sometimes unreproducible results due to the use of closed datasets. This article provides a systematic review and comparative evaluation of automated process discovery methods, using an open-source benchmark and covering 12 publicly-available real-life event logs, 12 proprietary real-life event logs, and nine quality metrics. The results highlight gaps and unexplored tradeoffs in the field, including the lack of scalability of some methods and a strong divergence in their performance with respect to the different quality metrics used.},
	pages = {686--705},
	number = {4},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	author = {Augusto, Adriano and Conforti, Raffaele and Dumas, Marlon and Rosa, Marcello La and Maggi, Fabrizio Maria and Marrella, Andrea and Mecella, Massimo and Soo, Allar},
	date = {2019-04},
	note = {{ZSCC}: {NoCitationData}[s0] 
Conference Name: {IEEE} Transactions on Knowledge and Data Engineering},
	keywords = {done, hot, benchmark, Data mining, survey, Process mining, Task analysis, Process control, automated process discovery, Benchmark testing, Data models, Systematics},
	file = {Augusto et al_2019_Automated Discovery of Process Models from Event Logs.pdf:C\:\\Users\\ohund\\Zotero\\storage\\ISDGHY99\\Augusto et al_2019_Automated Discovery of Process Models from Event Logs.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\ohund\\Zotero\\storage\\EUJJ46HH\\8368306.html:text/html}
}

@article{wang_measurement_2020,
	title = {Measurement of Text Similarity: A Survey},
	volume = {11},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2078-2489/11/9/421},
	doi = {10.3390/info11090421},
	shorttitle = {Measurement of Text Similarity},
	abstract = {Text similarity measurement is the basis of natural language processing tasks, which play an important role in information retrieval, automatic question answering, machine translation, dialogue systems, and document matching. This paper systematically combs the research status of similarity measurement, analyzes the advantages and disadvantages of current methods, develops a more comprehensive classification description system of text similarity measurement algorithms, and summarizes the future development direction. With the aim of providing reference for related research and application, the text similarity measurement method is described by two aspects: text distance and text representation. The text distance can be divided into length distance, distribution distance, and semantic distance; text representation is divided into string-based, corpus-based, single-semantic text, multi-semantic text, and graph-structure-based representation. Finally, the development of text similarity is also summarized in the discussion section.},
	pages = {421},
	number = {9},
	journaltitle = {Information},
	author = {Wang, Jiapeng and Dong, Yihong},
	urldate = {2021-12-17},
	date = {2020-09},
	langid = {english},
	note = {{ZSCC}: 0000014 
Number: 9
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {done, hot, text distance, text representation, text similarity measure},
	file = {Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\TKWW5Z44\\htm.html:text/html;Wang_Dong_2020_Measurement of Text Similarity.pdf:C\:\\Users\\ohund\\Zotero\\storage\\G34AXHI3\\Wang_Dong_2020_Measurement of Text Similarity.pdf:application/pdf}
}

@inproceedings{mk_survey_2016,
	title = {A Survey on Similarity Measures in Text Mining},
	doi = {10.5121/MLAIJ.2016.3103},
	abstract = {Semantic Scholar extracted view of "A Survey on Similarity Measures in Text Mining" by Vijaymeena M.K et al.},
	author = {M.K, Vijaymeena and Kavitha, K.},
	date = {2016},
	note = {{ZSCC}: 0000183},
	keywords = {irrelevant},
	file = {M.K_Kavitha_2016_A Survey on Similarity Measures in Text Mining.pdf:C\:\\Users\\ohund\\Zotero\\storage\\TMN2PAZB\\M.K_Kavitha_2016_A Survey on Similarity Measures in Text Mining.pdf:application/pdf}
}

@software{malteos_awesome_2021,
	title = {Awesome Document Similarity Measures},
	rights = {{MIT}},
	url = {https://github.com/malteos/awesome-document-similarity},
	abstract = {A curated list of resources on document similarity measures (papers, tutorials, code, ...)},
	author = {malteos},
	urldate = {2021-12-17},
	date = {2021-12-16},
	note = {{ZSCC}: {NoCitationData}[s0] 
original-date: 2019-10-04T08:44:11Z}
}

@inproceedings{nolle_binet_2018,
	location = {Cham},
	title = {{BINet}: Multivariate Business Process Anomaly Detection Using Deep Learning},
	isbn = {978-3-319-98648-7},
	doi = {10.1007/978-3-319-98648-7_16},
	series = {Lecture Notes in Computer Science},
	shorttitle = {{BINet}},
	abstract = {In this paper, we propose {BINet}, a neural network architecture for real-time multivariate anomaly detection in business process event logs. {BINet} has been designed to handle both the control flow and the data perspective of a business process. Additionally, we propose a heuristic for setting the threshold of an anomaly detection algorithm automatically. We demonstrate that {BINet} can be used to detect anomalies in event logs not only on a case level, but also on event attribute level. We compare {BINet} to 6 other state-of-the-art anomaly detection algorithms and evaluate their performance on an elaborate data corpus of 60 synthetic and 21 real life event logs using artificial anomalies. {BINet} reached an average F1F1F\_1 score over all detection levels of 0.83, whereas the next best approach, a denoising autoencoder, reached only 0.74. This F1F1F\_1 score is calculated over two different levels of detection, namely case and attribute level. {BINet} reached 0.84 on case and 0.82 on attribute level, whereas the next best approach reached 0.78 and 0.71 respectively.},
	pages = {271--287},
	booktitle = {Business Process Management},
	publisher = {Springer International Publishing},
	author = {Nolle, Timo and Seeliger, Alexander and Mühlhäuser, Max},
	editor = {Weske, Mathias and Montali, Marco and Weber, Ingo and vom Brocke, Jan},
	date = {2018},
	langid = {english},
	note = {{ZSCC}: {NoCitationData}[s0]},
	keywords = {Deep learning, Anomaly detection, Artificial process intelligence, Business process management, Recurrent neural networks},
	file = {Nolle et al_2018_BINet.pdf:C\:\\Users\\ohund\\Zotero\\storage\\ES8IV56F\\Nolle et al_2018_BINet.pdf:application/pdf}
}

@article{hazem_comparison_nodate,
	title = {A Comparison of Smoothing Techniques for Bilingual Lexicon Extraction from Comparable Corpora},
	abstract = {Smoothing is a central issue in language modeling and a prior step in different natural language processing ({NLP}) tasks. However, less attention has been given to it for bilingual lexicon extraction from comparable corpora. If a ﬁrst work to improve the extraction of low frequency words showed signiﬁcant improvement while using distance-based averaging (Pekar et al., 2006), no investigation of the many smoothing techniques has been carried out so far. In this paper, we present a study of some widelyused smoothing algorithms for language n-gram modeling (Laplace, Good-Turing, Kneser-Ney...). Our main contribution is to investigate how the different smoothing techniques affect the performance of the standard approach (Fung, 1998) traditionally used for bilingual lexicon extraction. We show that using smoothing as a preprocessing step of the standard approach increases its performance signiﬁcantly.},
	pages = {10},
	author = {Hazem, Amir and Morin, Emmanuel},
	langid = {english},
	note = {{ZSCC}: 0000012},
	file = {Hazem und Morin - A Comparison of Smoothing Techniques for Bilingual.pdf:C\:\\Users\\ohund\\Zotero\\storage\\A9I39PHM\\Hazem und Morin - A Comparison of Smoothing Techniques for Bilingual.pdf:application/pdf}
}

@article{chen_empirical_nodate,
	title = {An Empirical Study of Smoothing Techniques for Language Modeling},
	abstract = {We present a tutorial introduction to n-gram models for language modeling and survey the most widely-used smoothing algorithms for such models. We then present an extensive empirical comparison of several of these smoothing techniques, including those described by Jelinek and Mercer (1980), Katz (1987), Bell, Cleary, and Witten (1990), Ney, Essen, and Kneser (1994), and Kneser and Ney (1995). We investigate how factors such as training data size, training corpus ( , e.g. Brown versus Wall Street Journal), count cuto s, and n-gram order (bigram versus trigram) a ect the relative performance of these methods, which is measured through the cross-entropy of test data. Our results show that previous comparisons have not been complete enough to fully characterize smoothing algorithm performance. We introduce methodologies for analyzing smoothing algorithm e cacy in detail, and using these techniques we motivate a novel variation of Kneser-Ney smoothing that consistently outperforms all other algorithms evaluated. Finally, results showing that improved language model smoothing leads to improved speech recognition performance are presented.},
	pages = {64},
	author = {Chen, Stanley F},
	langid = {english},
	note = {{ZSCC}: 0004046},
	keywords = {hot},
	file = {Chen - An Empirical Study of Smoothing Techniques for Lan.pdf:C\:\\Users\\ohund\\Zotero\\storage\\YKGI5QQ2\\Chen - An Empirical Study of Smoothing Techniques for Lan.pdf:application/pdf}
}

@article{khaleghi_consistent_2016,
	title = {Consistent Algorithms for Clustering Time Series},
	volume = {17},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v17/khaleghi16a.html},
	abstract = {The problem of clustering is considered for the case where every point is a time series. The time series are either given in one batch (offline setting), or they are allowed to grow with time and new time series can be added along the way (online setting). We propose a natural notion of consistency for this problem, and show that there are simple, computationally efficient algorithms that are asymptotically consistent under extremely weak assumptions on the distributions that generate the data. The notion of consistency is as follows. A clustering algorithm is called consistent if it places two time series into the same cluster if and only if the distribution that generates them is the same. In the considered framework the time series are allowed to be highly dependent, and the dependence can have arbitrary form. If the number of clusters is known, the only assumption we make is that the (marginal) distribution of each time series is stationary ergodic. No parametric, memory or mixing assumptions are made. When the number of clusters is unknown, stronger assumptions are provably necessary, but it is still possible to devise nonparametric algorithms that are consistent under very general conditions. The theoretical findings of this work are illustrated with experiments on both synthetic and real data.},
	pages = {1--32},
	number = {3},
	journaltitle = {Journal of Machine Learning Research},
	author = {Khaleghi, Azadeh and Ryabko, Daniil and Mary, Jérémie and Preux, Philippe},
	urldate = {2022-02-02},
	date = {2016},
	note = {{ZSCC}: 0000049},
	keywords = {skimmed},
	file = {Khaleghi et al_2016_Consistent Algorithms for Clustering Time Series.pdf:C\:\\Users\\ohund\\Zotero\\storage\\4Z69RFVJ\\Khaleghi et al_2016_Consistent Algorithms for Clustering Time Series.pdf:application/pdf}
}

@article{olszewski_generalized_2001,
	title = {Generalized feature extraction for structural pattern recognition in time-series data},
	url = {https://www.semanticscholar.org/paper/Generalized-feature-extraction-for-structural-in-Olszewski-Maxion/d74fa52a7ddd41b175378dbe0604e635dab8a708},
	abstract = {The ability of the suite of structure detectors to generate features useful for structural pattern recognition is evaluated by comparing the classification accuracies achieved when using the structure detectors versus commonly-used statistical feature extractors, thus demonstrating that the {suiteOf} structure detectors effectively performs generalized feature extraction {forStructural} pattern recognition in time-series data. Pattern recognition encompasses two fundamental tasks: description and classification. Given an object to analyze, a pattern recognition system first generates a description of it (i.e., the pattern) and then classifies the object based on that description (i.e., the recognition). Two general approaches for implementing pattern recognition systems, statistical and structural, employ different techniques for description and classification. Statistical approaches to pattern recognition use decision-theoretic concepts to discriminate among objects belonging to different groups based upon their quantitative features. Structural approaches to pattern recognition use syntactic grammars to discriminate among objects belonging to different groups based upon the arrangement of their morphological (i.e., shape-based or structural) features. Hybrid approaches to pattern recognition combine aspects of both statistical and structural pattern recognition. 
Structural pattern recognition systems are difficult to apply to new domains because implementation of both the description and classification tasks requires domain knowledge. Knowledge acquisition techniques necessary to obtain domain knowledge from experts are tedious and often fail to produce a complete and accurate knowledge base. Consequently, applications of structural pattern recognition have been primarily restricted to domains in which the set of useful morphological features has been established in the literature (e.g., speech recognition and character recognition) and the syntactic grammars can be composed by hand (e.g., electrocardiogram diagnosis). To overcome this limitation, a domain-independent approach to structural pattern recognition is needed that is capable of extracting morphological features and performing classification without relying on domain knowledge. A hybrid system that employs a statistical classification technique to perform discrimination based on structural features is a natural solution. While a statistical classifier is inherently domain independent, the domain knowledge necessary to support the description task can be eliminated with a set of generally-useful morphological features. Such a set of morphological features is suggested as the foundation for the development of a suite of structure detectors to perform generalized feature extraction for structural pattern recognition in time-series data. 
The ability of the suite of structure detectors to generate features useful for structural pattern recognition is evaluated by comparing the classification accuracies achieved when using the structure detectors versus commonly-used statistical feature extractors. Two real-world databases with markedly different characteristics and established ground truth serve as sources of data for the evaluation. The classification accuracies achieved using the features extracted by the structure detectors were consistently as good as or better than the classification accuracies achieved when using the features generated by the statistical feature extractors, thus demonstrating that the suite of structure detectors effectively performs generalized feature extraction for structural pattern recognition in time-series data.},
	journaltitle = {undefined},
	author = {Olszewski, R. and Maxion, R. and Siewiorek, D.},
	urldate = {2022-02-02},
	date = {2001},
	langid = {english},
	note = {{ZSCC}: {NoCitationData}[s0]},
	keywords = {skimmed},
	file = {Olszewski et al_2001_Generalized feature extraction for structural pattern recognition in.pdf:C\:\\Users\\ohund\\Zotero\\storage\\NGUGFC92\\Olszewski et al_2001_Generalized feature extraction for structural pattern recognition in.pdf:application/pdf;Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\NC8ZTTTH\\d74fa52a7ddd41b175378dbe0604e635dab8a708.html:text/html}
}

@inreference{noauthor_granger_2022,
	title = {Granger causality},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Granger_causality&oldid=1064850191},
	abstract = {The Granger causality test is a statistical hypothesis test for determining whether one time series is useful in forecasting another, first proposed in 1969. Ordinarily, regressions reflect "mere" correlations, but Clive Granger argued that causality in economics could be tested for by measuring the ability to predict the future values of a time series using prior values of another time series. Since the question of "true causality" is deeply philosophical, and because of the post hoc ergo propter hoc fallacy of assuming that one thing preceding another can be used as a proof of causation, econometricians assert that the Granger test finds only "predictive causality". Using the term "causality" alone is a misnomer, as Granger-causality is better described as  "precedence", or, as Granger himself later claimed in 1977, "temporally related". Rather than testing whether X causes Y, the Granger causality tests whether X forecasts Y.A time series X is said to Granger-cause Y if it can be shown, usually through a series of t-tests and F-tests on lagged values of X (and with lagged values of Y also included), that those X values provide statistically significant information about future values of Y.
Granger also stressed that some studies using "Granger causality" testing in areas outside economics reached "ridiculous" conclusions. "Of course, many ridiculous papers appeared", he said in his Nobel lecture. However, it remains a popular method for causality analysis in time series due to its computational simplicity. The original definition of Granger causality does not account for latent confounding effects and does not capture instantaneous and non-linear causal relationships, though several extensions have been proposed to address these issues.},
	booktitle = {Wikipedia},
	urldate = {2022-02-02},
	date = {2022-01-10},
	langid = {english},
	note = {Page Version {ID}: 1064850191},
	file = {Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\P9BV74PH\\Granger_causality.html:text/html}
}

@online{herkert_multivariate_2022,
	title = {Multivariate Time Series Forecasting with Deep Learning},
	url = {https://towardsdatascience.com/multivariate-time-series-forecasting-with-deep-learning-3e7b3e2d2bcf},
	abstract = {Using {LSTM} networks for time series prediction and interpreting the results},
	titleaddon = {Medium},
	author = {Herkert, Daniel},
	urldate = {2022-02-02},
	date = {2022-01-07},
	langid = {english},
	note = {{ZSCC}: {NoCitationData}[s0]},
	file = {Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\DXNJIMYL\\multivariate-time-series-forecasting-with-deep-learning-3e7b3e2d2bcf.html:text/html}
}

@online{noauthor_645_nodate,
	title = {6.4.5. Multivariate Time Series Models},
	url = {https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc45.htm},
	urldate = {2022-02-02},
	file = {6.4.5. Multivariate Time Series Models:C\:\\Users\\ohund\\Zotero\\storage\\Y73DAWBE\\pmc45.html:text/html}
}

@article{shrikumar_learning_2019,
	title = {Learning Important Features Through Propagating Activation Differences},
	url = {http://arxiv.org/abs/1704.02685},
	abstract = {The purported "black box" nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present {DeepLIFT} (Deep Learning Important {FeaTures}), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. {DeepLIFT} compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, {DeepLIFT} can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply {DeepLIFT} to models trained on {MNIST} and simulated genomic data, and show significant advantages over gradient-based methods. Video tutorial: http://goo.gl/{qKb}7pL, {ICML} slides: bit.ly/deeplifticmlslides, {ICML} talk: https://vimeo.com/238275076, code: http://goo.gl/{RM}8jvH.},
	journaltitle = {{arXiv}:1704.02685 [cs]},
	author = {Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
	urldate = {2022-02-02},
	date = {2019-10-12},
	eprinttype = {arxiv},
	eprint = {1704.02685},
	note = {{ZSCC}: {NoCitationData}[s0] },
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv.org Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\HRMNAY8G\\1704.html:text/html;Shrikumar et al_2019_Learning Important Features Through Propagating Activation Differences.pdf:C\:\\Users\\ohund\\Zotero\\storage\\8TX9M3N7\\Shrikumar et al_2019_Learning Important Features Through Propagating Activation Differences.pdf:application/pdf}
}

@article{krishnan_structured_2016,
	title = {Structured Inference Networks for Nonlinear State Space Models},
	url = {http://arxiv.org/abs/1609.09869},
	abstract = {Gaussian state space models have been used for decades as generative models of sequential data. They admit an intuitive probabilistic interpretation, have a simple functional form, and enjoy widespread adoption. We introduce a unified algorithm to efficiently learn a broad class of linear and non-linear state space models, including variants where the emission and transition distributions are modeled by deep neural networks. Our learning algorithm simultaneously learns a compiled inference network and the generative model, leveraging a structured variational approximation parameterized by recurrent neural networks to mimic the posterior distribution. We apply the learning algorithm to both synthetic and real-world datasets, demonstrating its scalability and versatility. We find that using the structured approximation to the posterior results in models with significantly higher held-out likelihood.},
	journaltitle = {{arXiv}:1609.09869 [cs, stat]},
	author = {Krishnan, Rahul G. and Shalit, Uri and Sontag, David},
	urldate = {2022-02-02},
	date = {2016-12-05},
	eprinttype = {arxiv},
	eprint = {1609.09869},
	note = {{ZSCC}: {NoCitationData}[s0] 
version: 2},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\NJDSVZU5\\1609.html:text/html;Krishnan et al_2016_Structured Inference Networks for Nonlinear State Space Models.pdf:C\:\\Users\\ohund\\Zotero\\storage\\6TP4HNA9\\Krishnan et al_2016_Structured Inference Networks for Nonlinear State Space Models.pdf:application/pdf}
}

@article{bishnoi_survey_2020,
	title = {A survey of distance measures for mixed variables},
	volume = {8},
	issn = {2321-4902},
	url = {https://www.chemijournal.com/special-issue?year=2020&vol=8&issue=4&ArticleId=10087&si=true},
	doi = {10.22271/chemi.2020.v8.i4f.10087},
	abstract = {Distance measures are base for many statistical and data science methods with their applicability in various fields of science. Mixed variables data which is combination of continuous and categorical variables occurs frequently in fields such as medical, agriculture, remote sensing, biology, marketing, ecology etc., but a little work has been done for evaluating distance for such type of data. As there is not much literature available on distance measures for mixed data, therefore the fundamental sources that provide a comprehensive detail of a particular measure for mixed variables data were studied and reviewed in this paper.},
	pages = {338--343},
	number = {4},
	journaltitle = {International Journal of Chemical Studies},
	shortjournal = {Int. J. Chem. Stud.},
	author = {Bishnoi, Sudha and Hooda, B. K.},
	urldate = {2022-02-03},
	date = {2020},
	langid = {english},
	note = {{ZSCC}: 0000000 
Publisher: {AkiNik} Publications},
	keywords = {hot},
	file = {Bishnoi_Hooda_2020_A survey of distance measures for mixed variables.pdf:C\:\\Users\\ohund\\Zotero\\storage\\XP3D98H5\\Bishnoi_Hooda_2020_A survey of distance measures for mixed variables.pdf:application/pdf;Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\IJ459Q9D\\special-issue.html:text/html}
}

@article{sabbaghan_statistical_2020,
	title = {Statistical measurement of trees’ similarity},
	volume = {54},
	issn = {1573-7845},
	url = {https://doi.org/10.1007/s11135-019-00957-8},
	doi = {10.1007/s11135-019-00957-8},
	abstract = {Diagnostic theories are fundamental to Information Systems practice and are represented in trees. One way of creating diagnostic trees is by employing independent experts to construct such trees and compare them. However, good measures of similarity to compare diagnostic trees have not been identified. This paper presents an analysis of the suitability of various measures of association to determine the similarity of two diagnostic trees using bootstrap simulations. We find that three measures of association, Goodman and Kruskal’s Lambda, Cohen’s Kappa, and Goodman and Kruskal’s Gamma (J Am Stat Assoc 49(268):732–764, 1954) each behave differently depending on what is inconsistent between the two trees thus providing both measures for assessing alignment between two trees developed by independent experts as well as identifying the causes of the differences.},
	pages = {781--806},
	number = {3},
	journaltitle = {Quality \& Quantity},
	shortjournal = {Qual Quant},
	author = {Sabbaghan, Sahar and Chua, Cecil Eng Huang and Gardner, Lesley A.},
	urldate = {2022-02-03},
	date = {2020-06-01},
	langid = {english},
	note = {{ZSCC}: 0000001},
	keywords = {Tree, Path},
	file = {Sabbaghan et al_2020_Statistical measurement of trees’ similarity.pdf:C\:\\Users\\ohund\\Zotero\\storage\\WRP9AZHT\\Sabbaghan et al_2020_Statistical measurement of trees’ similarity.pdf:application/pdf}
}

@article{sologub_measuring_nodate,
	title = {On Measuring of Similarity between Tree Nodes},
	abstract = {In this paper, a survey of similarity measures between vertices of a graph is presented. Distance-based and structural equivalence measures are described. It is demonstrated that most of them degenerate if applied directly to the tree nodes. Adjusted path-based similarity measure is proposed as well as a new method for representing tree nodes as binary vectors that is based on using of an ancestor matrix. It is shown that application of ordinary similarity measures to this representation gives desired non-trivial results.},
	pages = {9},
	author = {Sologub, Gleb B},
	langid = {english},
	note = {{ZSCC}: 0000006},
	keywords = {Tree, Path},
	file = {Sologub - On Measuring of Similarity between Tree Nodes.pdf:C\:\\Users\\ohund\\Zotero\\storage\\RZF7B3WZ\\Sologub - On Measuring of Similarity between Tree Nodes.pdf:application/pdf}
}

@article{anastasiou_causality_2021,
	title = {Causality Distance Measures for Multivariate Time Series with Applications},
	volume = {9},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2227-7390},
	url = {https://www.mdpi.com/2227-7390/9/21/2708},
	doi = {10.3390/math9212708},
	abstract = {In this work, we focus on the development of new distance measure algorithms, namely, the Causality Within Groups ({CAWG}), the Generalized Causality Within Groups ({GCAWG}) and the Causality Between Groups ({CABG}), all of which are based on the well-known Granger causality. The proposed distances together with the associated algorithms are suitable for multivariate statistical data analysis including unsupervised classiﬁcation (clustering) purposes for the analysis of multivariate time series data with emphasis on ﬁnancial and economic data where causal relationships are frequently present. For exploring the appropriateness of the proposed methodology, we implement, for illustrative purposes, the proposed algorithms to hierarchical clustering for the classiﬁcation of 19 {EU} countries based on seven variables related to health resources in healthcare systems.},
	pages = {2708},
	number = {21},
	journaltitle = {Mathematics},
	shortjournal = {Mathematics},
	author = {Anastasiou, Achilleas and Hatzopoulos, Peter and Karagrigoriou, Alex and Mavridoglou, George},
	urldate = {2022-02-03},
	date = {2021-10-25},
	langid = {english},
	note = {{ZSCC}: 0000000
number: 21
publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {hot, skimmed, clustering, distance, classification, divergence, Granger causality, healthcare systems, multivariate time series, pattern recognition},
	file = {Anastasiou et al_2021_Causality Distance Measures for Multivariate Time Series with Applications.pdf:C\:\\Users\\ohund\\Zotero\\storage\\WVVTRN35\\Anastasiou et al_2021_Causality Distance Measures for Multivariate Time Series with Applications.pdf:application/pdf;Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\4F8FW5VE\\2708.html:text/html}
}

@article{zhao_towards_2017,
	title = {Towards Deeper Understanding of Variational Autoencoding Models},
	url = {http://arxiv.org/abs/1702.08658},
	abstract = {We propose a new family of optimization criteria for variational auto-encoding models, generalizing the standard evidence lower bound. We provide conditions under which they recover the data distribution and learn latent features, and formally show that common issues such as blurry samples and uninformative latent features arise when these conditions are not met. Based on these new insights, we propose a new sequential {VAE} model that can generate sharp samples on the {LSUN} image dataset based on pixel-wise reconstruction loss, and propose an optimization criterion that encourages unsupervised learning of informative latent features.},
	journaltitle = {{arXiv}:1702.08658 [cs, stat]},
	author = {Zhao, Shengjia and Song, Jiaming and Ermon, Stefano},
	urldate = {2022-02-03},
	date = {2017-02-28},
	eprinttype = {arxiv},
	eprint = {1702.08658},
	note = {{ZSCC}: 0000118 },
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, {VAE}},
	file = {arXiv.org Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\Q8AYM9G9\\1702.html:text/html;Zhao et al_2017_Towards Deeper Understanding of Variational Autoencoding Models.pdf:C\:\\Users\\ohund\\Zotero\\storage\\CB526M43\\Zhao et al_2017_Towards Deeper Understanding of Variational Autoencoding Models.pdf:application/pdf}
}

@inproceedings{chung_recurrent_2016,
	location = {Cambridge, {MA}, {USA}},
	title = {A Recurrent Latent Variable Model for Sequential Data},
	url = {http://arxiv.org/abs/1506.02216},
	series = {{NIPS}'15},
	abstract = {It is argued that through the use of high-level latent random variables, the variational {RNN} ({VRNN})1 can model the kind of variability observed in highly structured sequential data such as natural speech. In this paper, we explore the inclusion of latent random variables into the hidden state of a recurrent neural network ({RNN}) by combining the elements of the variational autoencoder. We argue that through the use of high-level latent random variables, the variational {RNN} ({VRNN})1 can model the kind of variability observed in highly structured sequential data such as natural speech. We empirically evaluate the proposed model against other related sequential models on four speech datasets and one handwriting dataset. Our results show the important roles that latent random variables can play in the {RNN} dynamics.},
	pages = {2980--2988},
	booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
	publisher = {{MIT} Press},
	author = {Chung, Junyoung and Kastner, Kyle and Dinh, Laurent and Goel, Kratarth and Courville, Aaron and Bengio, Yoshua},
	urldate = {2022-02-03},
	date = {2016-04-06},
	note = {{VRNN} \& {VRNN}-I 
{ZSCC}: 0000909
tex.ids= chung\_RecurrentLatentVariable\_2015},
	keywords = {done, hot, Computer Science - Machine Learning, {VAE}, named\_model, M2M, most-complete-causally-dependent-{DVAE}},
	file = {arXiv.org Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\6LMCUZNS\\1506.html:text/html;Chung et al_2015_A Recurrent Latent Variable Model for Sequential Data.pdf:C\:\\Users\\ohund\\Zotero\\storage\\6IE3AW2Y\\Chung et al_2015_A Recurrent Latent Variable Model for Sequential Data.pdf:application/pdf}
}

@article{zhu_s3vae_2020,
	title = {S3VAE: Self-Supervised Sequential {VAE} for Representation Disentanglement and Data Generation},
	url = {http://arxiv.org/abs/2005.11437},
	shorttitle = {S3VAE},
	abstract = {We propose a sequential variational autoencoder to learn disentangled representations of sequential data (e.g., videos and audios) under self-supervision. Specifically, we exploit the benefits of some readily accessible supervisory signals from input data itself or some off-the-shelf functional models and accordingly design auxiliary tasks for our model to utilize these signals. With the supervision of the signals, our model can easily disentangle the representation of an input sequence into static factors and dynamic factors (i.e., time-invariant and time-varying parts). Comprehensive experiments across videos and audios verify the effectiveness of our model on representation disentanglement and generation of sequential data, and demonstrate that, our model with self-supervision performs comparable to, if not better than, the fully-supervised model with ground truth labels, and outperforms state-of-the-art unsupervised models by a large margin.},
	journaltitle = {{arXiv}:2005.11437 [cs]},
	author = {Zhu, Yizhe and Min, Martin Renqiang and Kadav, Asim and Graf, Hans Peter},
	urldate = {2022-02-03},
	date = {2020-05-22},
	eprinttype = {arxiv},
	eprint = {2005.11437},
	note = {{ZSCC}: 0000023 },
	keywords = {skimmed, irrelevant, Computer Science - Computer Vision and Pattern Recognition, {VAE}},
	file = {arXiv.org Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\QUYWE8F6\\2005.html:text/html;Zhu et al_2020_S3VAE.pdf:C\:\\Users\\ohund\\Zotero\\storage\\YAPRZ7VV\\Zhu et al_2020_S3VAE.pdf:application/pdf}
}

@article{yang_causalvae_2021,
	title = {{CausalVAE}: Disentangled Representation Learning via Neural Structural Causal Models},
	url = {http://arxiv.org/abs/2004.08697},
	shorttitle = {{CausalVAE}},
	abstract = {Learning disentanglement aims at finding a low dimensional representation which consists of multiple explanatory and generative factors of the observational data. The framework of variational autoencoder ({VAE}) is commonly used to disentangle independent factors from observations. However, in real scenarios, factors with semantics are not necessarily independent. Instead, there might be an underlying causal structure which renders these factors dependent. We thus propose a new {VAE} based framework named {CausalVAE}, which includes a Causal Layer to transform independent exogenous factors into causal endogenous ones that correspond to causally related concepts in data. We further analyze the model identifiabitily, showing that the proposed model learned from observations recovers the true one up to a certain degree. Experiments are conducted on various datasets, including synthetic and real word benchmark {CelebA}. Results show that the causal representations learned by {CausalVAE} are semantically interpretable, and their causal relationship as a Directed Acyclic Graph ({DAG}) is identified with good accuracy. Furthermore, we demonstrate that the proposed {CausalVAE} model is able to generate counterfactual data through "do-operation" to the causal factors.},
	journaltitle = {{arXiv}:2004.08697 [cs, stat]},
	author = {Yang, Mengyue and Liu, Furui and Chen, Zhitang and Shen, Xinwei and Hao, Jianye and Wang, Jun},
	urldate = {2022-02-03},
	date = {2021-03-23},
	eprinttype = {arxiv},
	eprint = {2004.08697},
	note = {{ZSCC}: 0000019 },
	keywords = {hot, skimmed, Computer Science - Machine Learning, Statistics - Machine Learning, {VAE}},
	file = {arXiv.org Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\P4JGGQEK\\2004.html:text/html;Yang et al_2021_CausalVAE.pdf:C\:\\Users\\ohund\\Zotero\\storage\\287JXWRK\\Yang et al_2021_CausalVAE.pdf:application/pdf}
}

@online{cerliani_time_2020,
	title = {Time Series generation with {VAE} {LSTM}},
	url = {https://towardsdatascience.com/time-series-generation-with-vae-lstm-5a6426365a1c},
	abstract = {Filling Time Series with Generative Deep Learning Models},
	titleaddon = {Medium},
	author = {Cerliani, Marco},
	urldate = {2022-02-04},
	date = {2020-12-29},
	langid = {english},
	note = {{ZSCC}: {NoCitationData}[s0]},
	keywords = {done, irrelevant, {VAE}},
	file = {Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\TECNLPCW\\time-series-generation-with-vae-lstm-5a6426365a1c.html:text/html}
}

@online{noauthor_genetic_2011,
	title = {Genetic Algorithms and Evolutionary Algorithms - Introduction},
	url = {https://www.solver.com/genetic-evolutionary-introduction},
	abstract = {Welcome to our tutorial on genetic and evolutionary algorithms -- from Frontline Systems, developers of the Solver in Microsoft Excel. You can use genetic algorithms in Excel to solve optimization problems, using our advanced Evolutionary Solver, by downloading a free trial version of our Premium Solver Platform.},
	titleaddon = {solver},
	urldate = {2022-02-04},
	date = {2011-01-11},
	langid = {english},
	file = {Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\IAAWL933\\genetic-evolutionary-introduction.html:text/html}
}

@online{noauthor_what_nodate,
	title = {What is the difference between genetic and evolutionary algorithms?},
	url = {https://stackoverflow.com/questions/2890061/what-is-the-difference-between-genetic-and-evolutionary-algorithms},
	titleaddon = {Stack Overflow},
	urldate = {2022-02-04},
	file = {Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\KUZ2YJIU\\what-is-the-difference-between-genetic-and-evolutionary-algorithms.html:text/html}
}

@incollection{sloss_2019_2020,
	location = {Cham},
	title = {2019 Evolutionary Algorithms Review},
	isbn = {978-3-030-39958-0},
	url = {https://doi.org/10.1007/978-3-030-39958-0_16},
	series = {Genetic and Evolutionary Computation},
	abstract = {Evolutionary algorithm research and applications began over 50 years ago. Like other artificial intelligence techniques, evolutionary algorithms will likely see increased use and development due to the increased availability of computation, more robust and available open source software libraries, and the increasing demand for artificial intelligence techniques. As these techniques become more adopted and capable, it is the right time to take a perspective of their ability to integrate into society and the human processes they intend to augment. In this review, we explore a new taxonomy of evolutionary algorithms and resulting classifications that look at five main areas: the ability to manage the control of the environment with limiters, the ability to explain and repeat the search process, the ability to understand input and output causality within a solution, the ability to manage algorithm bias due to data or user design, and lastly, the ability to add corrective measures. These areas are motivated by today’s pressures on industry to conform to both societies concerns and new government regulatory rules. As many reviews of evolutionary algorithms exist, after motivating this new taxonomy, we briefly classify a broad range of algorithms and identify areas of future research.},
	pages = {307--344},
	booktitle = {Genetic Programming Theory and Practice {XVII}},
	publisher = {Springer International Publishing},
	author = {Sloss, Andrew N. and Gustafson, Steven},
	editor = {Banzhaf, Wolfgang and Goodman, Erik and Sheneman, Leigh and Trujillo, Leonardo and Worzel, Bill},
	urldate = {2022-02-04},
	date = {2020},
	langid = {english},
	doi = {10.1007/978-3-030-39958-0_16},
	note = {{ZSCC}: {NoCitationData}[s0] },
	file = {Sloss_Gustafson_2020_2019 Evolutionary Algorithms Review.pdf:C\:\\Users\\ohund\\Zotero\\storage\\PNS223J8\\Sloss_Gustafson_2020_2019 Evolutionary Algorithms Review.pdf:application/pdf}
}

@online{jangid_evolutionary_2019,
	title = {Evolutionary Algorithms: A Critical Review and its Future Prospects},
	url = {https://www.semanticscholar.org/paper/Evolutionary-Algorithms%3A-A-Critical-Review-and-its-Jangid-Puri/a84d26d8fbb9a6d4019cc8c547eb674964857eda},
	shorttitle = {Evolutionary Algorithms},
	abstract = {Evolutionary algorithm ({EA}) appears as asearch technique and important optimizationin the last period. {EA} is belonging to set of modern heuristics-based search method and it is a subgroup of Evolutionary Computations ({EC}). Due torobust behavior and flexible nature inherited from Evolutionary Computation.It becomes efficient means of problem-solvingway for broadly used global optimization problems. It can be used successfully in many applications of high complexity. This paper presents a critical overview of Evolutionary algorithms.It is generalmethod for implementation. It further discusses the various practical advantages using evolutionary algorithms over classical methods of optimization. It also comprises unusualstudy of various invariants of {EA} like Genetic Algorithm ({GA}), Genetic Programming ({GP}),Evolution Strategies ({ES}) and Evolutionary Programming ({EP}). Extensions of {EAs} in the form of Memetic algorithms ({MA}) and distributed {EA} are also discussed. Also the paper concentrates on various refinements done in area of {EA} to solve real life problems[19].},
	author = {Jangid, S. and Puri, R. and Kumar, T.},
	urldate = {2022-02-04},
	date = {2019},
	langid = {english},
	note = {{ZSCC}: 0000002},
	file = {Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\NCWR3FNE\\a84d26d8fbb9a6d4019cc8c547eb674964857eda.html:text/html}
}

@incollection{hildebrandt_earth_2019,
	location = {Cham},
	title = {Earth Movers’ Stochastic Conformance Checking},
	volume = {360},
	isbn = {978-3-030-26642-4 978-3-030-26643-1},
	url = {http://link.springer.com/10.1007/978-3-030-26643-1_8},
	abstract = {Process Mining aims to support Business Process Management ({BPM}) by extracting information about processes from real-life process executions recorded in event logs. In particular, conformance checking aims to measure the quality of a process model by quantifying diﬀerences between the model and an event log or another model. Even though event logs provide insights into the likelihood of observed behaviour, most state-of-the-art conformance checking techniques ignore this point of view. In this paper, we propose a conformance measure that considers the stochastic characteristics of both the event log and the process model. It is based on the “earth movers’ distance” and measures the eﬀort to transform the distributions of traces of the event log into the distribution of traces of the model. We formalize this intuitive conformance metric and provide an approximation and a simpliﬁed variant. The latter two have been implemented in {ProM} and we evaluate them using several real-life examples.},
	pages = {127--143},
	booktitle = {Business Process Management Forum},
	publisher = {Springer International Publishing},
	author = {Leemans, Sander J. J. and Syring, Anja F. and van der Aalst, Wil M. P.},
	editor = {Hildebrandt, Thomas and van Dongen, Boudewijn F. and Röglinger, Maximilian and Mendling, Jan},
	urldate = {2022-02-04},
	date = {2019},
	langid = {english},
	doi = {10.1007/978-3-030-26643-1_8},
	note = {{ZSCC}: {NoCitationData}[s0] 
Series Title: Lecture Notes in Business Information Processing},
	file = {Leemans et al. - 2019 - Earth Movers’ Stochastic Conformance Checking.pdf:C\:\\Users\\ohund\\Zotero\\storage\\NDACZE2R\\Leemans et al. - 2019 - Earth Movers’ Stochastic Conformance Checking.pdf:application/pdf}
}

@video{simon_leglaive_dynamical_2021,
	title = {Dynamical Variational Autoencoders (2/5)},
	url = {https://www.youtube.com/watch?v=rz76gYgxySo},
	abstract = {Video 2/5: Dynamical {VAEs}

Tutorial presented at {IEEE} {ICASSP} 2021

More resources available at https://dynamicalvae.github.io},
	author = {{Simon Leglaive}},
	urldate = {2022-02-07},
	date = {2021-06-22},
	note = {{ZSCC}: {NoCitationData}[s0]}
}

@article{fabius_variational_2015,
	title = {Variational Recurrent Auto-Encoders},
	url = {http://arxiv.org/abs/1412.6581},
	abstract = {In this paper we propose a model that combines the strengths of {RNNs} and {SGVB}: the Variational Recurrent Auto-Encoder ({VRAE}). Such a model can be used for efficient, large scale unsupervised learning on time series data, mapping the time series data to a latent vector representation. The model is generative, such that data can be generated from samples of the latent space. An important contribution of this work is that the model can make use of unlabeled data in order to facilitate supervised training of {RNNs} by initialising the weights and network state.},
	journaltitle = {{arXiv}:1412.6581 [cs, stat]},
	author = {Fabius, Otto and van Amersfoort, Joost R.},
	urldate = {2022-02-07},
	date = {2015-06-15},
	eprinttype = {arxiv},
	eprint = {1412.6581},
	note = {{ZSCC}: 0000005 },
	keywords = {done, irrelevant, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv.org Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\KMM5AJKI\\1412.html:text/html;Fabius_van Amersfoort_2015_Variational Recurrent Auto-Encoders.pdf:C\:\\Users\\ohund\\Zotero\\storage\\8FU4IGI2\\Fabius_van Amersfoort_2015_Variational Recurrent Auto-Encoders.pdf:application/pdf}
}

@article{bayer_learning_2015,
	title = {Learning Stochastic Recurrent Networks},
	url = {http://arxiv.org/abs/1411.7610},
	abstract = {Leveraging advances in variational inference, we propose to enhance recurrent neural networks with latent variables, resulting in Stochastic Recurrent Networks ({STORNs}). The model i) can be trained with stochastic gradient methods, ii) allows structured and multi-modal conditionals at each time step, iii) features a reliable estimator of the marginal likelihood and iv) is a generalisation of deterministic recurrent neural networks. We evaluate the method on four polyphonic musical data sets and motion capture data.},
	journaltitle = {{arXiv}:1411.7610 [cs, stat]},
	author = {Bayer, Justin and Osendorfer, Christian},
	urldate = {2022-02-07},
	date = {2015-03-05},
	eprinttype = {arxiv},
	eprint = {1411.7610},
	note = {{ZSCC}: 0000002 },
	keywords = {done, hot, Computer Science - Machine Learning, Statistics - Machine Learning, named\_model, M2M},
	file = {arXiv.org Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\EZEB95EW\\1411.html:text/html;Bayer_Osendorfer_2015_Learning Stochastic Recurrent Networks.pdf:C\:\\Users\\ohund\\Zotero\\storage\\CAI8L99Q\\Bayer_Osendorfer_2015_Learning Stochastic Recurrent Networks.pdf:application/pdf}
}

@inproceedings{fraccaro_sequential_2016,
	location = {Red Hook, {NY}, {USA}},
	title = {Sequential neural models with stochastic layers},
	isbn = {978-1-5108-3881-9},
	series = {{NIPS}'16},
	abstract = {How can we efficiently propagate uncertainty in a latent state representation with recurrent neural networks? This paper introduces stochastic recurrent neural networks which glue a deterministic recurrent neural network and a state space model together to form a stochastic and sequential neural generative model. The clear separation of deterministic and stochastic layers allows a structured variational inference network to track the factorization of the model's posterior distribution. By retaining both the nonlinear recursive structure of a recurrent neural network and averaging over the uncertainty in a latent path, like a state space model, we improve the state of the art results on the Blizzard and {TTMIT} speech modeling data sets by a large margin, while achieving comparable performances to competing methods on polyphonic music modeling.},
	pages = {2207--2215},
	booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
	publisher = {Curran Associates Inc.},
	author = {Fraccaro, Marco and Sønderby, Søren Kaae and Paquet, Ulrich and Winther, Ole},
	urldate = {2022-02-07},
	date = {2016},
	note = {{ZSCC}: 0000286},
	keywords = {hot, skimmed, named\_model, M2M},
	file = {Fraccaro et al_2016_Sequential neural models with stochastic layers.pdf:C\:\\Users\\ohund\\Zotero\\storage\\WADVPS34\\Fraccaro et al_2016_Sequential neural models with stochastic layers.pdf:application/pdf}
}

@article{leglaive_recurrent_2020,
	title = {A Recurrent Variational Autoencoder for Speech Enhancement},
	url = {http://arxiv.org/abs/1910.10942},
	abstract = {This paper presents a generative approach to speech enhancement based on a recurrent variational autoencoder ({RVAE}). The deep generative speech model is trained using clean speech signals only, and it is combined with a nonnegative matrix factorization noise model for speech enhancement. We propose a variational expectation-maximization algorithm where the encoder of the {RVAE} is fine-tuned at test time, to approximate the distribution of the latent variables given the noisy speech observations. Compared with previous approaches based on feed-forward fully-connected architectures, the proposed recurrent deep generative speech model induces a posterior temporal dynamic over the latent variables, which is shown to improve the speech enhancement results.},
	journaltitle = {{arXiv}:1910.10942 [cs, eess]},
	author = {Leglaive, Simon and Alameda-Pineda, Xavier and Girin, Laurent and Horaud, Radu},
	urldate = {2022-02-07},
	date = {2020-02-10},
	eprinttype = {arxiv},
	eprint = {1910.10942},
	note = {{ZSCC}: 0000023 },
	keywords = {Computer Science - Artificial Intelligence, hot, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Sound, named\_model, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv.org Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\JI28KZJH\\1910.html:text/html;Leglaive et al_2020_A Recurrent Variational Autoencoder for Speech Enhancement.pdf:C\:\\Users\\ohund\\Zotero\\storage\\JB5QY92T\\Leglaive et al_2020_A Recurrent Variational Autoencoder for Speech Enhancement.pdf:application/pdf}
}

@article{li_disentangled_2018,
	title = {Disentangled Sequential Autoencoder},
	url = {http://arxiv.org/abs/1803.02991},
	abstract = {We present a {VAE} architecture for encoding and generating high dimensional sequential data, such as video or audio. Our deep generative model learns a latent representation of the data which is split into a static and dynamic part, allowing us to approximately disentangle latent time-dependent features (dynamics) from features which are preserved over time (content). This architecture gives us partial control over generating content and dynamics by conditioning on either one of these sets of features. In our experiments on artificially generated cartoon video clips and voice recordings, we show that we can convert the content of a given sequence into another one by such content swapping. For audio, this allows us to convert a male speaker into a female speaker and vice versa, while for video we can separately manipulate shapes and dynamics. Furthermore, we give empirical evidence for the hypothesis that stochastic {RNNs} as latent state models are more efficient at compressing and generating long sequences than deterministic ones, which may be relevant for applications in video compression.},
	journaltitle = {{arXiv}:1803.02991 [cs]},
	author = {Li, Yingzhen and Mandt, Stephan},
	urldate = {2022-02-07},
	date = {2018-06-12},
	eprinttype = {arxiv},
	eprint = {1803.02991},
	note = {{ZSCC}: 0000087 },
	keywords = {hot, Computer Science - Machine Learning, named\_model, M2M},
	file = {arXiv.org Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\3PFLNLJA\\1803.html:text/html;Li_Mandt_2018_Disentangled Sequential Autoencoder.pdf:C\:\\Users\\ohund\\Zotero\\storage\\3A6IZNPM\\Li_Mandt_2018_Disentangled Sequential Autoencoder.pdf:application/pdf}
}

@software{cerliani_medium_notebook_2022,
	title = {{MEDIUM}\_NoteBook},
	url = {https://github.com/cerlymarco/MEDIUM_NoteBook/blob/209a458c36df850545ea097f21f77c699bda0bc1/VAE_TimeSeries/VAE_TimeSeries.ipynb},
	abstract = {Repository containing notebooks of my posts on Medium},
	author = {Cerliani, Marco},
	urldate = {2022-02-07},
	date = {2022-02-07},
	note = {{ZSCC}: {NoCitationData}[s0] 
original-date: 2019-04-22T20:45:20Z}
}

@online{jing_must-have_2021,
	title = {A must-have training trick for {VAE}(variational autoencoder)},
	url = {https://medium.com/mlearning-ai/a-must-have-training-trick-for-vae-variational-autoencoder-d28ff53b0023},
	abstract = {How Cyclical {KL} Annealing Schedule save your {VAE} model in a second.},
	titleaddon = {{MLearning}.ai},
	author = {Jing, Cheng},
	urldate = {2022-02-08},
	date = {2021-11-07},
	langid = {english},
	note = {{ZSCC}: {NoCitationData}[s0]},
	file = {Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\SDM73SAY\\a-must-have-training-trick-for-vae-variational-autoencoder-d28ff53b0023.html:text/html}
}

@online{noauthor_probability_nodate,
	title = {probability - Variational autoencoder: Why reconstruction term is same to square loss?},
	url = {https://stats.stackexchange.com/questions/347378/variational-autoencoder-why-reconstruction-term-is-same-to-square-loss},
	shorttitle = {probability - Variational autoencoder},
	titleaddon = {Cross Validated},
	urldate = {2022-02-08},
	file = {Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\XXNMP9VU\\variational-autoencoder-why-reconstruction-term-is-same-to-square-loss.html:text/html}
}

@online{falcon_variational_2020,
	title = {Variational Autoencoder Demystified With {PyTorch} Implementation.},
	url = {https://towardsdatascience.com/variational-autoencoder-demystified-with-pytorch-implementation-3a06bee395ed},
	abstract = {This tutorial implements a variational autoencoder for non-black and white images using {PyTorch}.},
	titleaddon = {Medium},
	author = {Falcon, William},
	urldate = {2022-02-08},
	date = {2020-12-15},
	langid = {english},
	note = {{ZSCC}: {NoCitationData}[s0]},
	file = {Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\75AJ6QYY\\variational-autoencoder-demystified-with-pytorch-implementation-3a06bee395ed.html:text/html}
}

@online{noauthor_autoencoder_2018,
	title = {From Autoencoder to Beta-{VAE}},
	url = {https://lilianweng.github.io/2018/08/12/from-autoencoder-to-beta-vae.html},
	abstract = {Autocoders are a family of neural network models aiming to learn compressed latent variables of high-dimensional data. Starting from the basic autocoder model, this post reviews several variations, including denoising, sparse, and contractive autoencoders, and then Variational Autoencoder ({VAE}) and its modification beta-{VAE}.},
	titleaddon = {Lil'Log},
	urldate = {2022-02-08},
	date = {2018-08-12},
	langid = {english},
	file = {Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\QU2EY475\\from-autoencoder-to-beta-vae.html:text/html}
}

@online{kubler_extensible_2021,
	title = {An extensible Evolutionary Algorithm Example in Python},
	url = {https://towardsdatascience.com/an-extensible-evolutionary-algorithm-example-in-python-7372c56a557b},
	abstract = {Learning how to write an easy Evolutionary Algorithm from scratch in less than 50 lines of code that you can use for your projects.},
	titleaddon = {Medium},
	author = {Kübler, Dr Robert},
	urldate = {2022-02-08},
	date = {2021-08-13},
	langid = {english},
	note = {{ZSCC}: {NoCitationData}[s0]},
	file = {Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\3IP5S636\\an-extensible-evolutionary-algorithm-example-in-python-7372c56a557b.html:text/html}
}

@online{noauthor_10_nodate,
	title = {10 Python library for evolutionary and genetic algorithm {\textbar} Data Science and Machine Learning},
	url = {https://www.kaggle.com/getting-started/a},
	abstract = {10 Python library for evolutionary and genetic algorithm.},
	urldate = {2022-02-08},
	langid = {english},
	file = {Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\MB8RDTWK\\112297.html:text/html}
}

@software{wilde_daffidwildeedo_2022,
	title = {daffidwilde/edo},
	rights = {{MIT}},
	url = {https://github.com/daffidwilde/edo},
	abstract = {A library for generating artificial datasets through genetic evolution.},
	author = {Wilde, Henry},
	urldate = {2022-02-08},
	date = {2022-02-07},
	note = {{ZSCC}: {NoCitationData}[s0] 
original-date: 2018-07-04T09:59:27Z},
	keywords = {data-generation, evolutionary-algorithms, optimisation}
}

@software{noauthor_deap_2022,
	title = {{DEAP}},
	rights = {{LGPL}-3.0},
	url = {https://github.com/DEAP/deap},
	abstract = {Distributed Evolutionary Algorithms in Python},
	publisher = {Distributed Evolutionary Algorithms in Python},
	urldate = {2022-02-08},
	date = {2022-02-08},
	note = {original-date: 2014-05-21T20:07:39Z}
}

@article{naumann_consequence-aware_2021,
	title = {Consequence-aware Sequential Counterfactual Generation},
	volume = {12976},
	url = {http://arxiv.org/abs/2104.05592},
	doi = {10.1007/978-3-030-86520-7_42},
	abstract = {Counterfactuals have become a popular technique nowadays for interacting with black-box machine learning models and understanding how to change a particular instance to obtain a desired outcome from the model. However, most existing approaches assume instant materialization of these changes, ignoring that they may require effort and a specific order of application. Recently, methods have been proposed that also consider the order in which actions are applied, leading to the so-called sequential counterfactual generation problem. In this work, we propose a model-agnostic method for sequential counterfactual generation. We formulate the task as a multi-objective optimization problem and present a genetic algorithm approach to find optimal sequences of actions leading to the counterfactuals. Our cost model considers not only the direct effect of an action, but also its consequences. Experimental results show that compared to state-of-the-art, our approach generates less costly solutions, is more efficient and provides the user with a diverse set of solutions to choose from.},
	pages = {682--698},
	journaltitle = {{arXiv}:2104.05592 [cs]},
	author = {Naumann, Philip and Ntoutsi, Eirini},
	urldate = {2022-02-08},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2104.05592},
	note = {{ZSCC}: 0000001 },
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\UPIVJZ3J\\2104.html:text/html;Naumann_Ntoutsi_2021_Consequence-aware Sequential Counterfactual Generation.pdf:C\:\\Users\\ohund\\Zotero\\storage\\9MTEDR9K\\Naumann_Ntoutsi_2021_Consequence-aware Sequential Counterfactual Generation.pdf:application/pdf}
}

@online{noauthor_counterfactual_nodate,
	title = {counterfactual adjective - Definition, pictures, pronunciation and usage notes {\textbar} Oxford Advanced American Dictionary at {OxfordLearnersDictionaries}.com},
	url = {https://www.oxfordlearnersdictionaries.com/definition/american_english/counterfactual},
	urldate = {2022-02-09},
	file = {counterfactual adjective - Definition, pictures, pronunciation and usage notes | Oxford Advanced American Dictionary at OxfordLearnersDictionaries.com:C\:\\Users\\ohund\\Zotero\\storage\\G8PXBWPY\\counterfactual.html:text/html}
}

@incollection{starr_counterfactuals_2021,
	edition = {Summer 2021},
	title = {Counterfactuals},
	url = {https://plato.stanford.edu/archives/sum2021/entries/counterfactuals/},
	abstract = {Modal discourse concerns alternative ways things can be, e.g., whatmight be true, what isn’t true but could have been, what shouldbe done. This entry focuses on counterfactualmodality which concerns what is not, but could or would havebeen. What if Martin Luther King had died when he was stabbed in 1958(Byrne 2005: 1)? What if the Americashad never been colonized? What if I were to put that box over here andthis one over there? These modes of thought and speech have been thesubject of extensive study in philosophy, linguistics, psychology,artificial intelligence, history, and many other allied fields. Thesediverse investigations are united by the fact that counterfactualmodality crops up at the center of foundational questions in thesefields., In philosophy, counterfactual modality has given rise to difficultsemantic, epistemological, and metaphysical questions:, These questions have attracted significant attention in recentdecades, revealing a wealth of puzzles and insights. While otherentries address the epistemic—the epistemology of modality—and metaphysical questions—possible worlds and actualism—this entry focuses on the semantic question. It will aim to refine thisquestion, explain its central role in certain philosophical debates,and outline the main semantic analyses of counterfactuals.,  Section 1 begins with a working definition of counterfactual conditionals (§1.1), and then surveys how counterfactuals feature in theories of agency,mental representation, and rationality (§1.2), and how they are used in metaphysical analysis and scientificexplanation (§1.3). Section 1.4 then details several ways in which the logic andtruth-conditions of counterfactuals are puzzling. This sets the stagefor the sections 2 and 3, which survey semantic analyses of counterfactuals that attempt toexplain this puzzling behavior.,  Section 2 focuses on two related analyses that were primarily developed tostudy the logic of counterfactuals: strict conditionalanalyses and similarity analyses. These analyses were not originallyconcerned with saying what the truth-conditions of particularcounterfactuals are. Attempts to extend them to that domain, however,have attracted intense criticism. Section 3 surveys more recent analyses that offer more explicit models of whencounterfactuals are true. These analyses include premise semantics (§3.1), conditional probability analyses (§3.2) and structural equations/causal models (§3.3). They are more closely connected to work on counterfactuals inpsychology, artificial intelligence, and the philosophy ofscience., Sections 2 and 3 of this entry employ some basic tools from set theory and logicalsemantics. But these sections also provide intuitive characterizationsalongside formal definitions, so familiarity with these tools is not apre-requisite. Readers interested in more familiarity with these toolswill find basic set theory, as well as Gamut (1991) and Sider(2010) useful.},
	booktitle = {The Stanford Encyclopedia of Philosophy},
	publisher = {Metaphysics Research Lab, Stanford University},
	author = {Starr, William},
	editor = {Zalta, Edward N.},
	urldate = {2022-02-09},
	date = {2021},
	note = {{ZSCC}: {NoCitationData}[s0]
tex.ids= starr\_Counterfactuals\_2021a},
	file = {SEP - Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\JD8KY8HL\\counterfactuals.html:text/html;SEP - Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\IIF3XSVT\\counterfactuals.html:text/html}
}

@online{noauthor_counterfactual_nodate-1,
	title = {counterfactual},
	url = {https://www-oxfordreference-com.proxy.library.uu.nl/view/10.1093/oi/authority.20110803095642948},
	abstract = {"counterfactual" published on  by null.},
	titleaddon = {Oxford Reference},
	urldate = {2022-02-10},
	langid = {english},
	doi = {10.1093/oi/authority.20110803095642948},
	file = {Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\ZRB6ALUS\\authority.html:text/html}
}

@incollection{hitchcock_causal_2020,
	edition = {Summer 2020},
	title = {Causal Models},
	url = {https://plato.stanford.edu/archives/sum2020/entries/causal-models/},
	abstract = {Causal models are mathematical models representing causalrelationships within an individual system or population. Theyfacilitate inferences about causal relationships from statisticaldata. They can teach us a good deal about the epistemology ofcausation, and about the relationship between causation andprobability. They have also been applied to topics of interest tophilosophers, such as the logic of counterfactuals, decision theory,and the analysis of actual causation.},
	booktitle = {The Stanford Encyclopedia of Philosophy},
	publisher = {Metaphysics Research Lab, Stanford University},
	author = {Hitchcock, Christopher},
	editor = {Zalta, Edward N.},
	urldate = {2022-02-10},
	date = {2020},
	note = {{ZSCC}: {NoCitationData}[s0]
tex.ids= hitchcock\_CausalModels\_2020a},
	file = {SEP - Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\66XSDPF9\\causal-models.html:text/html;SEP - Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\GFVZ4I3T\\causal-models.html:text/html}
}

@book{molnar2019,
	title = {Interpretable machine learning. A Guide for Making Black Box Models Explainable},
	url = {https://christophm.github.io/interpretable-ml-book/},
	author = {Molnar, Christoph},
	date = {2019},
	langid = {english},
	note = {{ZSCC}: 0001930
tex.ids= molnar\_InterpretableMachineLearning\_},
	file = {Molnar - Interpretable Machine Learning.pdf:C\:\\Users\\ohund\\Zotero\\storage\\42DZ6UIM\\Molnar - Interpretable Machine Learning.pdf:application/pdf}
}

@inproceedings{van_der_aalst_process_2012-1,
	location = {Berlin, Heidelberg},
	title = {Process Mining Manifesto},
	isbn = {978-3-642-28108-2},
	doi = {10.1007/978-3-642-28108-2_19},
	series = {Lecture Notes in Business Information Processing},
	abstract = {Process mining techniques are able to extract knowledge from event logs commonly available in today’s information systems. These techniques provide new means to discover, monitor, and improve processes in a variety of application domains. There are two main drivers for the growing interest in process mining. On the one hand, more and more events are being recorded, thus, providing detailed information about the history of processes. On the other hand, there is a need to improve and support business processes in competitive and rapidly changing environments. This manifesto is created by the {IEEE} Task Force on Process Mining and aims to promote the topic of process mining. Moreover, by defining a set of guiding principles and listing important challenges, this manifesto hopes to serve as a guide for software developers, scientists, consultants, business managers, and end-users. The goal is to increase the maturity of process mining as a new tool to improve the (re)design, control, and support of operational business processes.},
	pages = {169--194},
	booktitle = {Business Process Management Workshops},
	publisher = {Springer},
	author = {van der Aalst, Wil and Adriansyah, Arya and de Medeiros, Ana Karla Alves and Arcieri, Franco and Baier, Thomas and Blickle, Tobias and Bose, Jagadeesh Chandra and van den Brand, Peter and Brandtjen, Ronald and Buijs, Joos and Burattin, Andrea and Carmona, Josep and Castellanos, Malu and Claes, Jan and Cook, Jonathan and Costantini, Nicola and Curbera, Francisco and Damiani, Ernesto and de Leoni, Massimiliano and Delias, Pavlos and van Dongen, Boudewijn F. and Dumas, Marlon and Dustdar, Schahram and Fahland, Dirk and Ferreira, Diogo R. and Gaaloul, Walid and van Geffen, Frank and Goel, Sukriti and Günther, Christian and Guzzo, Antonella and Harmon, Paul and ter Hofstede, Arthur and Hoogland, John and Ingvaldsen, Jon Espen and Kato, Koki and Kuhn, Rudolf and Kumar, Akhil and La Rosa, Marcello and Maggi, Fabrizio and Malerba, Donato and Mans, Ronny S. and Manuel, Alberto and {McCreesh}, Martin and Mello, Paola and Mendling, Jan and Montali, Marco and Motahari-Nezhad, Hamid R. and zur Muehlen, Michael and Munoz-Gama, Jorge and Pontieri, Luigi and Ribeiro, Joel and Rozinat, Anne and Seguel Pérez, Hugo and Seguel Pérez, Ricardo and Sepúlveda, Marcos and Sinur, Jim and Soffer, Pnina and Song, Minseok and Sperduti, Alessandro and Stilo, Giovanni and Stoel, Casper and Swenson, Keith and Talamo, Maurizio and Tan, Wei and Turner, Chris and Vanthienen, Jan and Varvaressos, George and Verbeek, Eric and Verdonk, Marc and Vigo, Roberto and Wang, Jianmin and Weber, Barbara and Weidlich, Matthias and Weijters, Ton and Wen, Lijie and Westergaard, Michael and Wynn, Moe},
	editor = {Daniel, Florian and Barkaoui, Kamel and Dustdar, Schahram},
	date = {2012},
	langid = {english},
	note = {{ZSCC}: {NoCitationData}[s0]},
	keywords = {done, hot, Process Instance, Business Intelligence, Business Process Management, Concept Drift, Process Mining},
	file = {van der Aalst et al_2012_Process Mining Manifesto.pdf:C\:\\Users\\ohund\\Zotero\\storage\\HJ95L8HP\\van der Aalst et al_2012_Process Mining Manifesto.pdf:application/pdf}
}

@online{noauthor_definition_nodate,
	title = {Definition of {PROCESS}},
	url = {https://www.merriam-webster.com/dictionary/process},
	abstract = {progress, advance; something going on : proceeding; a natural phenomenon marked by gradual changes that lead toward a particular result… See the full definition},
	urldate = {2022-02-17},
	langid = {english},
	file = {Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\R3HXYZ8T\\process.html:text/html}
}

@article{ismail_fawaz_deep_2019,
	title = {Deep learning for time series classification: a review},
	volume = {33},
	issn = {1573-756X},
	url = {https://doi.org/10.1007/s10618-019-00619-1},
	doi = {10.1007/s10618-019-00619-1},
	shorttitle = {Deep learning for time series classification},
	abstract = {Time Series Classification ({TSC}) is an important and challenging problem in data mining. With the increase of time series data availability, hundreds of {TSC} algorithms have been proposed. Among these methods, only a few have considered Deep Neural Networks ({DNNs}) to perform this task. This is surprising as deep learning has seen very successful applications in the last years. {DNNs} have indeed revolutionized the field of computer vision especially with the advent of novel deeper architectures such as Residual and Convolutional Neural Networks. Apart from images, sequential data such as text and audio can also be processed with {DNNs} to reach state-of-the-art performance for document classification and speech recognition. In this article, we study the current state-of-the-art performance of deep learning algorithms for {TSC} by presenting an empirical study of the most recent {DNN} architectures for {TSC}. We give an overview of the most successful deep learning applications in various time series domains under a unified taxonomy of {DNNs} for {TSC}. We also provide an open source deep learning framework to the {TSC} community where we implemented each of the compared approaches and evaluated them on a univariate {TSC} benchmark (the {UCR}/{UEA} archive) and 12 multivariate time series datasets. By training 8730 deep learning models on 97 time series datasets, we propose the most exhaustive study of {DNNs} for {TSC} to date.},
	pages = {917--963},
	number = {4},
	journaltitle = {Data Mining and Knowledge Discovery},
	shortjournal = {Data Min Knowl Disc},
	author = {Ismail Fawaz, Hassan and Forestier, Germain and Weber, Jonathan and Idoumghar, Lhassane and Muller, Pierre-Alain},
	urldate = {2022-02-20},
	date = {2019-07-01},
	langid = {english},
	note = {{ZSCC}: 0001247},
	file = {Ismail Fawaz et al_2019_Deep learning for time series classification.pdf:C\:\\Users\\ohund\\Zotero\\storage\\YARQQ2A6\\Ismail Fawaz et al_2019_Deep learning for time series classification.pdf:application/pdf}
}

@article{rabiner_introduction_1986,
	title = {An introduction to hidden Markov models},
	volume = {3},
	issn = {1558-1284},
	doi = {10.1109/MASSP.1986.1165342},
	abstract = {The basic theory of Markov chains has been known to mathematicians and engineers for close to 80 years, but it is only in the past decade that it has been applied explicitly to problems in speech processing. One of the major reasons why speech models, based on Markov chains, have not been developed until recently was the lack of a method for optimizing the parameters of the Markov model to match observed signal patterns. Such a method was proposed in the late 1960's and was immediately applied to speech processing in several research institutions. Continued refinements in the theory and implementation of Markov modelling techniques have greatly enhanced the method, leading to a wide range of applications of these models. It is the purpose of this tutorial paper to give an introduction to the theory of Markov models, and to illustrate how they have been applied to problems in speech recognition.},
	pages = {4--16},
	number = {1},
	journaltitle = {{IEEE} {ASSP} Magazine},
	author = {Rabiner, L. and Juang, B.},
	date = {1986-01},
	note = {{ZSCC}: 0006244 
Conference Name: {IEEE} {ASSP} Magazine},
	keywords = {Fluctuations, Hidden Markov models, Linear systems, Mathematical model, Optimization methods, Pattern matching, Speech processing, Speech recognition, Time varying systems},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\ohund\\Zotero\\storage\\6HACKMXF\\1165342.html:text/html}
}

@book{commandeur_introduction_2007,
	location = {Oxford ; New York},
	title = {An introduction to state space time series analysis},
	isbn = {978-0-19-922887-4},
	series = {Practical econometrics},
	pagetotal = {174},
	publisher = {Oxford University Press},
	author = {Commandeur, Jacques J. F. and Koopman, S. J.},
	date = {2007},
	langid = {english},
	note = {{ZSCC}: 0000496 
{OCLC}: ocn123374304},
	keywords = {State-space methods, Time-series analysis},
	file = {Commandeur und Koopman - 2007 - An introduction to state space time series analysi.pdf:C\:\\Users\\ohund\\Zotero\\storage\\SLB48B98\\Commandeur und Koopman - 2007 - An introduction to state space time series analysi.pdf:application/pdf}
}

@inreference{noauthor_state-space_2021,
	title = {State-space representation},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=State-space_representation&oldid=1053783244},
	abstract = {In control engineering, a state-space representation is a mathematical model of a physical system as a set of input, output and state variables related by first-order differential equations or difference equations. State variables are variables whose values evolve over time in a way that depends on the values they have at any given time and on the externally imposed values of input variables. Output variables’ values depend on the values of the state variables.
The "state space" is the Euclidean space in which the variables on the axes are the state variables. The state of the system can be represented as a state vector within that space.
To abstract from the number of inputs, outputs and states, these variables are expressed as vectors. 
If the dynamical system is linear, time-invariant, and finite-dimensional, then the differential and algebraic equations may be written in matrix form.
The state-space method is characterized by significant algebraization of general system theory, which makes it possible to use Kronecker vector-matrix structures. The capacity of these structures can be efficiently applied to research systems with modulation or without it. 
The state-space representation (also known as the "time-domain approach") provides a convenient and compact way to model and analyze systems with multiple inputs and outputs. With 
  
    
      
        p
      
    
    \{{\textbackslash}displaystyle p\}
   inputs and 
  
    
      
        q
      
    
    \{{\textbackslash}displaystyle q\}
   outputs, we would otherwise have to write down 
  
    
      
        q
        ×
        p
      
    
    \{{\textbackslash}displaystyle q{\textbackslash}times p\}
   Laplace transforms to encode all the information about a system. Unlike the frequency domain approach, the use of the state-space representation is not limited to systems with linear components and zero initial conditions.
The state-space model can be applied in subjects such as economics, statistics, computer science and electrical engineering, and neuroscience. In econometrics, for example, state-space models can be used to decompose a time series into trend and cycle, compose individual indicators into a composite index, identify turning points of the business cycle, and estimate {GDP} using latent and unobserved time series. Many applications rely on the Kalman Filter to produce estimates of the current unknown state variables using their previous observations.},
	booktitle = {Wikipedia},
	urldate = {2022-02-20},
	date = {2021-11-06},
	langid = {english},
	note = {Page Version {ID}: 1053783244},
	file = {Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\3U3BYU3C\\State-space_representation.html:text/html}
}

@article{kalman_new_1960,
	title = {A New Approach to Linear Filtering and Prediction Problems},
	volume = {82},
	pages = {35--45},
	issue = {Series D},
	journaltitle = {Transactions of the {ASME}–Journal of Basic Engineering},
	author = {Kalman, Rudolph Emil},
	date = {1960},
	note = {{ZSCC}: 0038460},
	keywords = {hot},
	file = {Kalman1960.pdf:C\:\\Users\\ohund\\Zotero\\storage\\72UBAQSU\\Kalman1960.pdf:application/pdf}
}

@article{krishnan_structured_2017,
	title = {Structured Inference Networks for Nonlinear State Space Models},
	volume = {31},
	rights = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/10779},
	abstract = {Gaussian state space models have been used for decades as generative models of sequential data. They admit an intuitive probabilistic interpretation, have a simple functional form, and enjoy widespread adoption. We introduce a unified algorithm to efficiently learn a broad class of linear and non-linear state space models, including variants where the emission and transition distributions are modeled by deep neural networks. Our learning algorithm simultaneously learns a compiled inference network and the generative model, leveraging a structured variational approximation parameterized by recurrent neural networks to mimic the posterior distribution. We apply the learning algorithm to both synthetic and real-world datasets, demonstrating its scalability and versatility. We find that using the structured approximation to the posterior results in models with significantly higher held-out likelihood.},
	number = {1},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	author = {Krishnan, Rahul and Shalit, Uri and Sontag, David},
	urldate = {2022-02-22},
	date = {2017-02-13},
	langid = {english},
	note = {{ZSCC}: 0000321 
Number: 1},
	keywords = {hot, skimmed, named\_model, M2M, Hidden Markov Models},
	file = {Krishnan et al_2017_Structured Inference Networks for Nonlinear State Space Models.pdf:C\:\\Users\\ohund\\Zotero\\storage\\I47WD4QT\\Krishnan et al_2017_Structured Inference Networks for Nonlinear State Space Models.pdf:application/pdf}
}

@software{noauthor_structuredinf_2022,
	title = {{structuredInf}},
	rights = {{MIT}},
	url = {https://github.com/clinicalml/structuredinference},
	abstract = {Structured Inference Networks for Nonlinear State Space Models},
	publisher = {Sontag Lab},
	urldate = {2022-02-24},
	date = {2022-01-28},
	note = {original-date: 2016-05-24T18:12:10Z}
}

@inproceedings{mattei_leveraging_2018,
	title = {Leveraging the Exact Likelihood of Deep Latent Variable Models},
	volume = {31},
	url = {https://papers.nips.cc/paper/2018/hash/0609154fa35b3194026346c9cac2a248-Abstract.html},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Mattei, Pierre-Alexandre and Frellsen, Jes},
	urldate = {2022-02-24},
	date = {2018},
	note = {{ZSCC}: 0000035},
	file = {Mattei_Frellsen_2018_Leveraging the Exact Likelihood of Deep Latent Variable Models.pdf:C\:\\Users\\ohund\\Zotero\\storage\\TPJUSQ4E\\Mattei_Frellsen_2018_Leveraging the Exact Likelihood of Deep Latent Variable Models.pdf:application/pdf}
}

@article{kingma_auto-encoding_2014,
	title = {Auto-Encoding Variational Bayes},
	url = {https://dare.uva.nl/search?identifier=cf65ba0f-d88f-4a49-8ebd-3a7fce86edd7},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	journaltitle = {{arXiv}:1312.6114 [cs, stat]},
	author = {Kingma, Diederik P. and Welling, Max},
	urldate = {2022-02-24},
	date = {2014-05-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1312.6114},
	note = {{ZSCC}: 0018901
tex.ids= kingma\_AutoEncodingVariationalBayes\_2014a
publisher: Ithaca, {NYarXiv}.org},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Kingma_Welling_2014_Auto-Encoding Variational Bayes.pdf:C\:\\Users\\ohund\\Zotero\\storage\\79P5F6VI\\Kingma_Welling_2014_Auto-Encoding Variational Bayes.pdf:application/pdf;Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\PVC4GYJH\\search.html:text/html}
}

@online{noauthor_tutorial_nodate,
	title = {Tutorial \#5: variational autoencoders},
	url = {https://www.borealisai.com/en/blog/tutorial-5-variational-auto-encoders/},
	urldate = {2022-02-24},
	file = {Tutorial #5\: variational autoencoders:C\:\\Users\\ohund\\Zotero\\storage\\DISKEMHY\\tutorial-5-variational-auto-encoders.html:text/html}
}

@article{bowman_generating_2016,
	title = {Generating Sentences from a Continuous Space},
	url = {http://arxiv.org/abs/1511.06349},
	abstract = {The standard recurrent neural network language model ({RNNLM}) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an {RNN}-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling.},
	journaltitle = {{arXiv}:1511.06349 [cs]},
	author = {Bowman, Samuel R. and Vilnis, Luke and Vinyals, Oriol and Dai, Andrew M. and Jozefowicz, Rafal and Bengio, Samy},
	urldate = {2022-02-24},
	date = {2016-05-12},
	eprinttype = {arxiv},
	eprint = {1511.06349},
	note = {{ZSCC}: 0000003 },
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\UZ6T7D78\\1511.html:text/html;Bowman et al_2016_Generating Sentences from a Continuous Space.pdf:C\:\\Users\\ohund\\Zotero\\storage\\JPZ5H6J3\\Bowman et al_2016_Generating Sentences from a Continuous Space.pdf:application/pdf}
}

@article{gregor_temporal_2019,
	title = {Temporal Difference Variational Auto-Encoder},
	url = {http://arxiv.org/abs/1806.03107},
	abstract = {To act and plan in complex environments, we posit that agents should have a mental simulator of the world with three characteristics: (a) it should build an abstract state representing the condition of the world; (b) it should form a belief which represents uncertainty on the world; (c) it should go beyond simple step-by-step simulation, and exhibit temporal abstraction. Motivated by the absence of a model satisfying all these requirements, we propose {TD}-{VAE}, a generative sequence model that learns representations containing explicit beliefs about states several steps into the future, and that can be rolled out directly without single-step transitions. {TD}-{VAE} is trained on pairs of temporally separated time points, using an analogue of temporal difference learning used in reinforcement learning.},
	journaltitle = {{arXiv}:1806.03107 [cs, stat]},
	author = {Gregor, Karol and Papamakarios, George and Besse, Frederic and Buesing, Lars and Weber, Theophane},
	urldate = {2022-02-24},
	date = {2019-01-02},
	eprinttype = {arxiv},
	eprint = {1806.03107},
	note = {{ZSCC}: {NoCitationData}[s0] },
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\MM26JGGH\\1806.html:text/html;Gregor et al_2019_Temporal Difference Variational Auto-Encoder.pdf:C\:\\Users\\ohund\\Zotero\\storage\\8GYNCZT7\\Gregor et al_2019_Temporal Difference Variational Auto-Encoder.pdf:application/pdf}
}

@incollection{molnar_counterfactual_nodate,
	title = {Counterfactual Explanations},
	url = {https://christophm.github.io/interpretable-ml-book/counterfactual.html},
	booktitle = {Interpretable machine learning. A Guide for Making Black Box Models Explainable},
	author = {Molnar, Christoph and Dandl, Susanne},
	langid = {english}
}

@article{wachter_counterfactual_2017,
	title = {Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the {GDPR}},
	doi = {10.2139/ssrn.3063289},
	shorttitle = {Counterfactual Explanations Without Opening the Black Box},
	abstract = {It is suggested data controllers should offer a particular type of explanation, unconditional counterfactual explanations, to support these three aims, which describe the smallest change to the world that can be made to obtain a desirable outcome, or to arrive at the closest possible world, without needing to explain the internal logic of the system. There has been much discussion of the right to explanation in the {EU} General Data Protection Regulation, and its existence, merits, and disadvantages. Implementing a right to explanation that opens the black box of algorithmic decision-making faces major legal and technical barriers. Explaining the functionality of complex algorithmic decision-making systems and their rationale in specific cases is a technically challenging problem. Some explanations may offer little meaningful information to data subjects, raising questions around their value. Explanations of automated decisions need not hinge on the general public understanding how algorithmic systems function. Even though such interpretability is of great importance and should be pursued, explanations can, in principle, be offered without opening the black box. Looking at explanations as a means to help a data subject act rather than merely understand, one could gauge the scope and content of explanations according to the specific goal or action they are intended to support. From the perspective of individuals affected by automated decision-making, we propose three aims for explanations: (1) to inform and help the individual understand why a particular decision was reached, (2) to provide grounds to contest the decision if the outcome is undesired, and (3) to understand what would need to change in order to receive a desired result in the future, based on the current decision-making model. We assess how each of these goals finds support in the {GDPR}. We suggest data controllers should offer a particular type of explanation, unconditional counterfactual explanations, to support these three aims. These counterfactual explanations describe the smallest change to the world that can be made to obtain a desirable outcome, or to arrive at the closest possible world, without needing to explain the internal logic of the system.},
	journaltitle = {{ArXiv}},
	author = {Wachter, Sandra and Mittelstadt, B. and Russell, Chris},
	date = {2017},
	note = {{ZSCC}: 0001096},
	file = {Wachter et al_2017_Counterfactual Explanations Without Opening the Black Box.pdf:C\:\\Users\\ohund\\Zotero\\storage\\GVX7D2L7\\Wachter et al_2017_Counterfactual Explanations Without Opening the Black Box.pdf:application/pdf}
}

@inproceedings{dandl_multi-objective_2020,
	location = {Cham},
	title = {Multi-Objective Counterfactual Explanations},
	isbn = {978-3-030-58112-1},
	doi = {10.1007/978-3-030-58112-1_31},
	series = {Lecture Notes in Computer Science},
	abstract = {Counterfactual explanations are one of the most popular methods to make predictions of black box machine learning models interpretable by providing explanations in the form of ‘what-if scenarios’. Most current approaches optimize a collapsed, weighted sum of multiple objectives, which are naturally difficult to balance a-priori. We propose the Multi-Objective Counterfactuals ({MOC}) method, which translates the counterfactual search into a multi-objective optimization problem. Our approach not only returns a diverse set of counterfactuals with different trade-offs between the proposed objectives, but also maintains diversity in feature space. This enables a more detailed post-hoc analysis to facilitate better understanding and also more options for actionable user responses to change the predicted outcome. Our approach is also model-agnostic and works for numerical and categorical input features. We show the usefulness of {MOC} in concrete cases and compare our approach with state-of-the-art methods for counterfactual explanations.},
	pages = {448--469},
	booktitle = {Parallel Problem Solving from Nature – {PPSN} {XVI}},
	publisher = {Springer International Publishing},
	author = {Dandl, Susanne and Molnar, Christoph and Binder, Martin and Bischl, Bernd},
	editor = {Bäck, Thomas and Preuss, Mike and Deutz, André and Wang, Hao and Doerr, Carola and Emmerich, Michael and Trautmann, Heike},
	date = {2020},
	langid = {english},
	note = {{ZSCC}: 0000055},
	keywords = {Counterfactual explanations, Interpretability, Interpretable machine learning, Multi-objective optimization, {NSGA}-{II}},
	file = {Dandl et al_2020_Multi-Objective Counterfactual Explanations.pdf:C\:\\Users\\ohund\\Zotero\\storage\\RL4FDYQH\\Dandl et al_2020_Multi-Objective Counterfactual Explanations.pdf:application/pdf}
}

@inproceedings{narendra_counterfactual_2019,
	location = {Cham},
	title = {Counterfactual Reasoning for Process Optimization Using Structural Causal Models},
	isbn = {978-3-030-26643-1},
	doi = {10.1007/978-3-030-26643-1_6},
	series = {Lecture Notes in Business Information Processing},
	abstract = {Business processes are complex and involve the execution of various steps using different resources that can be shared across various tasks. Processes require analysis and process owners need to constantly look for methods to improve process performance indicators. It is non-trivial to quantify the improvement of a proposed change, without implementing or conducting randomized controlled trials. In several cases, the cost and time for implementing and evaluating the benefits of these changes are high. To address this, we propose a principled framework using Structural Causal Models which formally codify existing cause-effect assumptions about the process, control confounding and answer “what if” questions with observational data. We formally define an end to end methodology which takes process execution logs and specified {BPMN} model as inputs for structural causal model discovery and for performing counterfactual reasoning. We show that exploiting the process specification for causal discovery automatically ensures the inclusion of subject matter expertise, and also provides an effective computational methodology. We illustrate the effectiveness of our approach by answering intervention and counterfactual questions on example process models.},
	pages = {91--106},
	booktitle = {Business Process Management Forum},
	publisher = {Springer International Publishing},
	author = {Narendra, Tanmayee and Agarwal, Prerna and Gupta, Monika and Dechu, Sampath},
	editor = {Hildebrandt, Thomas and van Dongen, Boudewijn F. and Röglinger, Maximilian and Mendling, Jan},
	date = {2019},
	langid = {english},
	note = {{ZSCC}: {NoCitationData}[s0]},
	keywords = {Counterfactual reasoning, Process optimization, Process redesign, Structural causal model, What-if analysis},
	file = {Narendra et al_2019_Counterfactual Reasoning for Process Optimization Using Structural Causal Models.pdf:C\:\\Users\\ohund\\Zotero\\storage\\LFE9B4XV\\Narendra et al_2019_Counterfactual Reasoning for Process Optimization Using Structural Causal Models.pdf:application/pdf}
}

@article{feder_causal_2021,
	title = {Causal Inference in Natural Language Processing: Estimation, Prediction, Interpretation and Beyond},
	url = {http://arxiv.org/abs/2109.00725},
	shorttitle = {Causal Inference in Natural Language Processing},
	abstract = {A fundamental goal of scientific research is to learn about causal relationships. However, despite its critical role in the life and social sciences, causality has not had the same importance in Natural Language Processing ({NLP}), which has traditionally placed more emphasis on predictive tasks. This distinction is beginning to fade, with an emerging area of interdisciplinary research at the convergence of causal inference and language processing. Still, research on causality in {NLP} remains scattered across domains without unified definitions, benchmark datasets and clear articulations of the remaining challenges. In this survey, we consolidate research across academic areas and situate it in the broader {NLP} landscape. We introduce the statistical challenge of estimating causal effects, encompassing settings where text is used as an outcome, treatment, or as a means to address confounding. In addition, we explore potential uses of causal inference to improve the performance, robustness, fairness, and interpretability of {NLP} models. We thus provide a unified overview of causal inference for the computational linguistics community.},
	journaltitle = {{arXiv}:2109.00725 [cs]},
	author = {Feder, Amir and Keith, Katherine A. and Manzoor, Emaad and Pryzant, Reid and Sridhar, Dhanya and Wood-Doughty, Zach and Eisenstein, Jacob and Grimmer, Justin and Reichart, Roi and Roberts, Margaret E. and Stewart, Brandon M. and Veitch, Victor and Yang, Diyi},
	urldate = {2022-02-26},
	date = {2021-09-02},
	eprinttype = {arxiv},
	eprint = {2109.00725},
	note = {{ZSCC}: {NoCitationData}[s0] },
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\IY8GNV8N\\2109.html:text/html;Feder et al_2021_Causal Inference in Natural Language Processing.pdf:C\:\\Users\\ohund\\Zotero\\storage\\Y99EVZ2K\\Feder et al_2021_Causal Inference in Natural Language Processing.pdf:application/pdf}
}

@inproceedings{robeer_generating_2021,
	location = {Punta Cana, Dominican Republic},
	title = {Generating Realistic Natural Language Counterfactuals},
	url = {https://aclanthology.org/2021.findings-emnlp.306},
	doi = {10.18653/v1/2021.findings-emnlp.306},
	abstract = {Counterfactuals are a valuable means for understanding decisions made by {ML} systems. However, the counterfactuals generated by the methods currently available for natural language text are either unrealistic or introduce imperceptible changes. We propose {CounterfactualGAN}: a method that combines a conditional {GAN} and the embeddings of a pretrained {BERT} encoder to model-agnostically generate realistic natural language text counterfactuals for explaining regression and classification tasks. Experimental results show that our method produces perceptibly distinguishable counterfactuals, while outperforming four baseline methods on fidelity and human judgments of naturalness, across multiple datasets and multiple predictive models.},
	eventtitle = {{EMNLP}-Findings 2021},
	pages = {3611--3625},
	booktitle = {Findings of the Association for Computational Linguistics: {EMNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Robeer, Marcel and Bex, Floris and Feelders, Ad},
	urldate = {2022-02-26},
	date = {2021-11},
	langid = {english},
	note = {{ZSCC}: 0000001
tex.ids= robeer\_GeneratingRealisticNatural\_},
	keywords = {skimmed, required},
	file = {Robeer et al_2021_Generating Realistic Natural Language Counterfactuals.pdf:C\:\\Users\\ohund\\Zotero\\storage\\R8ERX49Y\\Robeer et al_2021_Generating Realistic Natural Language Counterfactuals.pdf:application/pdf;Robeer et al. - Generating Realistic Natural Language Counterfactu.pdf:C\:\\Users\\ohund\\Zotero\\storage\\A88SBILJ\\Robeer et al. - Generating Realistic Natural Language Counterfactu.pdf:application/pdf}
}

@inproceedings{fern_text_2021,
	location = {Online and Punta Cana, Dominican Republic},
	title = {Text Counterfactuals via Latent Optimization and Shapley-Guided Search},
	url = {https://aclanthology.org/2021.emnlp-main.452},
	doi = {10.18653/v1/2021.emnlp-main.452},
	abstract = {We study the problem of generating counterfactual text for a classifier as a means for understanding and debugging classification. Given a textual input and a classification model, we aim to minimally alter the text to change the model's prediction. White-box approaches have been successfully applied to similar problems in vision where one can directly optimize the continuous input. Optimization-based approaches become difficult in the language domain due to the discrete nature of text. We bypass this issue by directly optimizing in the latent space and leveraging a language model to generate candidate modifications from optimized latent representations. We additionally use Shapley values to estimate the combinatoric effect of multiple changes. We then use these estimates to guide a beam search for the final counterfactual text. We achieve favorable performance compared to recent white-box and black-box baselines using human and automatic evaluations. Ablation studies show that both latent optimization and the use of Shapley values improve success rate and the quality of the generated counterfactuals.},
	eventtitle = {{EMNLP} 2021},
	pages = {5578--5593},
	booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Fern, Xiaoli and Pope, Quintin},
	urldate = {2022-02-26},
	date = {2021-11},
	note = {{ZSCC}: 0000001},
	file = {Fern_Pope_2021_Text Counterfactuals via Latent Optimization and Shapley-Guided Search.pdf:C\:\\Users\\ohund\\Zotero\\storage\\T9H47JW3\\Fern_Pope_2021_Text Counterfactuals via Latent Optimization and Shapley-Guided Search.pdf:application/pdf}
}

@online{doshi_foundations_2021,
	title = {Foundations of {NLP} Explained Visually: Beam Search, How it Works},
	url = {https://towardsdatascience.com/foundations-of-nlp-explained-visually-beam-search-how-it-works-1586b9849a24},
	shorttitle = {Foundations of {NLP} Explained Visually},
	abstract = {A Gentle Guide to how Beam Search enhances predictions, in Plain English},
	titleaddon = {Medium},
	author = {Doshi, Ketan},
	urldate = {2022-02-26},
	date = {2021-05-21},
	langid = {english},
	note = {{ZSCC}: {NoCitationData}[s0]},
	file = {Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\5PVKI3V5\\foundations-of-nlp-explained-visually-beam-search-how-it-works-1586b9849a24.html:text/html}
}

@article{martens_explaining_2014,
	title = {Explaining data-driven document classifications},
	volume = {38},
	issn = {0276-7783},
	url = {https://doi.org/10.25300/MISQ/2014/38.1.04},
	doi = {10.25300/MISQ/2014/38.1.04},
	abstract = {Many document classification applications require human understanding of the reasons for data-driven classification decisions by managers, client-facing employees, and the technical team. Predictive models treat documents as data to be classified, and document data are characterized by very high dimensionality, often with tens of thousands to millions of variables (words). Unfortunately, due to the high dimensionality, understanding the decisions made by document classifiers is very difficult. This paper begins by extending the most relevant prior theoretical model of explanations for intelligent systems to account for some missing elements. The main theoretical contribution is the definition of a new sort of explanation as a minimal set of words (terms, generally), such that removing all words within this set from the document changes the predicted class from the class of interest. We present an algorithm to find such explanations, as well as a framework to assess such an algorithm's performance. We demonstrate the value of the new approach with a case study from a real-world document classification task: classifying web pages as containing objectionable content, with the goal of allowing advertisers to choose not to have their ads appear on those pages. A second empirical demonstration on news-story topic classification shows the explanations to be concise and document-specific, and to be capable of providing understanding of the exact reasons for the classification decisions, of the workings of the classification models, and of the business application itself. We also illustrate how explaining the classifications of documents can help to improve data quality and model performance.},
	pages = {73--100},
	number = {1},
	journaltitle = {{MIS} Quarterly},
	shortjournal = {{MIS} Q.},
	author = {Martens, David and Provost, Foster},
	urldate = {2022-02-26},
	date = {2014},
	note = {{ZSCC}: 0000232},
	keywords = {comprehensibility, document classification, instance level explanation, text mining}
}

@inproceedings{krause_interacting_2016,
	location = {New York, {NY}, {USA}},
	title = {Interacting with Predictions: Visual Inspection of Black-box Machine Learning Models},
	isbn = {978-1-4503-3362-7},
	url = {https://doi.org/10.1145/2858036.2858529},
	doi = {10.1145/2858036.2858529},
	series = {{CHI} '16},
	shorttitle = {Interacting with Predictions},
	abstract = {Understanding predictive models, in terms of interpreting and identifying actionable insights, is a challenging task. Often the importance of a feature in a model is only a rough estimate condensed into one number. However, our research goes beyond these naïve estimates through the design and implementation of an interactive visual analytics system, Prospector. By providing interactive partial dependence diagnostics, data scientists can understand how features affect the prediction overall. In addition, our support for localized inspection allows data scientists to understand how and why specific datapoints are predicted as they are, as well as support for tweaking feature values and seeing how the prediction responds. Our system is then evaluated using a case study involving a team of data scientists improving predictive models for detecting the onset of diabetes from electronic medical records.},
	pages = {5686--5697},
	booktitle = {Proceedings of the 2016 {CHI} Conference on Human Factors in Computing Systems},
	publisher = {Association for Computing Machinery},
	author = {Krause, Josua and Perer, Adam and Ng, Kenney},
	urldate = {2022-02-26},
	date = {2016},
	note = {{ZSCC}: 0000326},
	keywords = {interactive machine learning, partial dependence, predictive modeling},
	file = {Krause et al_2016_Interacting with Predictions.pdf:C\:\\Users\\ohund\\Zotero\\storage\\RYD5E2M8\\Krause et al_2016_Interacting with Predictions.pdf:application/pdf}
}

@article{wang_controllable_2019-1,
	title = {Controllable Unsupervised Text Attribute Transfer via Editing Entangled Latent Representation},
	url = {http://arxiv.org/abs/1905.12926},
	abstract = {Unsupervised text attribute transfer automatically transforms a text to alter a specific attribute (e.g. sentiment) without using any parallel data, while simultaneously preserving its attribute-independent content. The dominant approaches are trying to model the content-independent attribute separately, e.g., learning different attributes' representations or using multiple attribute-specific decoders. However, it may lead to inflexibility from the perspective of controlling the degree of transfer or transferring over multiple aspects at the same time. To address the above problems, we propose a more flexible unsupervised text attribute transfer framework which replaces the process of modeling attribute with minimal editing of latent representations based on an attribute classifier. Specifically, we first propose a Transformer-based autoencoder to learn an entangled latent representation for a discrete text, then we transform the attribute transfer task to an optimization problem and propose the Fast-Gradient-Iterative-Modification algorithm to edit the latent representation until conforming to the target attribute. Extensive experimental results demonstrate that our model achieves very competitive performance on three public data sets. Furthermore, we also show that our model can not only control the degree of transfer freely but also allow to transfer over multiple aspects at the same time.},
	journaltitle = {{arXiv}:1905.12926 [cs]},
	author = {Wang, Ke and Hua, Hang and Wan, Xiaojun},
	urldate = {2022-02-28},
	date = {2019-12-12},
	eprinttype = {arxiv},
	eprint = {1905.12926},
	note = {{ZSCC}: 0000044 },
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\7GQKC94P\\1905.html:text/html;Wang et al_2019_Controllable Unsupervised Text Attribute Transfer via Editing Entangled Latent.pdf:C\:\\Users\\ohund\\Zotero\\storage\\WP7FMK5H\\Wang et al_2019_Controllable Unsupervised Text Attribute Transfer via Editing Entangled Latent.pdf:application/pdf}
}

@article{melnyk_improved_2017,
	title = {Improved Neural Text Attribute Transfer with Non-parallel Data},
	url = {http://arxiv.org/abs/1711.09395},
	abstract = {Text attribute transfer using non-parallel data requires methods that can perform disentanglement of content and linguistic attributes. In this work, we propose multiple improvements over the existing approaches that enable the encoder-decoder framework to cope with the text attribute transfer from non-parallel data. We perform experiments on the sentiment transfer task using two datasets. For both datasets, our proposed method outperforms a strong baseline in two of the three employed evaluation metrics.},
	journaltitle = {{arXiv}:1711.09395 [cs]},
	author = {Melnyk, Igor and Santos, Cicero Nogueira dos and Wadhawan, Kahini and Padhi, Inkit and Kumar, Abhishek},
	urldate = {2022-02-28},
	date = {2017-12-04},
	eprinttype = {arxiv},
	eprint = {1711.09395},
	note = {{ZSCC}: 0000019 },
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\NH3SAE42\\1711.html:text/html;Melnyk et al_2017_Improved Neural Text Attribute Transfer with Non-parallel Data.pdf:C\:\\Users\\ohund\\Zotero\\storage\\6FD8A7BV\\Melnyk et al_2017_Improved Neural Text Attribute Transfer with Non-parallel Data.pdf:application/pdf}
}

@article{wang_deconfounded_2019,
	title = {The Deconfounded Recommender: A Causal Inference Approach to Recommendation},
	url = {http://arxiv.org/abs/1808.06581},
	shorttitle = {The Deconfounded Recommender},
	abstract = {The goal of recommendation is to show users items that they will like. Though usually framed as a prediction, the spirit of recommendation is to answer an interventional question---for each user and movie, what would the rating be if we "forced" the user to watch the movie? To this end, we develop a causal approach to recommendation, one where watching a movie is a "treatment" and a user's rating is an "outcome." The problem is there may be unobserved confounders, variables that affect both which movies the users watch and how they rate them; unobserved confounders impede causal predictions with observational data. To solve this problem, we develop the deconfounded recommender, a way to use classical recommendation models for causal recommendation. Following Wang \& Blei [23], the deconfounded recommender involves two probabilistic models. The first models which movies the users watch; it provides a substitute for the unobserved confounders. The second one models how each user rates each movie; it employs the substitute to help account for confounders. This two-stage approach removes bias due to confounding. It improves recommendation and enjoys stable performance against interventions on test sets.},
	journaltitle = {{arXiv}:1808.06581 [cs, stat]},
	author = {Wang, Yixin and Liang, Dawen and Charlin, Laurent and Blei, David M.},
	urldate = {2022-02-28},
	date = {2019-05-27},
	eprinttype = {arxiv},
	eprint = {1808.06581},
	note = {{ZSCC}: {NoCitationData}[s0] },
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Information Retrieval},
	file = {arXiv.org Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\5TF4XR8G\\1808.html:text/html;Wang et al_2019_The Deconfounded Recommender.pdf:C\:\\Users\\ohund\\Zotero\\storage\\XAQ2BUS6\\Wang et al_2019_The Deconfounded Recommender.pdf:application/pdf}
}

@article{shook_assessment_2004,
	title = {An assessment of the use of structural equation modeling in strategic management research},
	volume = {25},
	issn = {1097-0266},
	url = {http://onlinelibrary.wiley.com/doi/abs/10.1002/smj.385},
	doi = {10.1002/smj.385},
	abstract = {Structural equation modeling ({SEM}) is a powerful, yet complex, analytical technique. The use of {SEM} to examine strategic management phenomena has increased dramatically in recent years, suggesting that a critical evaluation of the technique's implementation is needed. We compared the use of {SEM} in 92 strategic management studies published in nine prominent journals from 1984 to 2002 to guidelines culled from methodological research. We found that the use and reporting of {SEM} often have been less than ideal, indicating that authors may be drawing erroneous conclusions about relationships among variables. Given these results, we offer suggestions for researchers on how to better deploy {SEM} within future inquiry. Copyright © 2004 John Wiley \& Sons, Ltd.},
	pages = {397--404},
	number = {4},
	journaltitle = {Strategic Management Journal},
	author = {Shook, Christopher L. and Ketchen Jr., David J. and Hult, G. Tomas M. and Kacmar, K. Michele},
	urldate = {2022-03-01},
	date = {2004},
	langid = {english},
	note = {{ZSCC}: 0001113 
\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/smj.385},
	keywords = {data analysis, statistical methods, structural equation modeling},
	file = {Shook et al_2004_An assessment of the use of structural equation modeling in strategic.pdf:C\:\\Users\\ohund\\Zotero\\storage\\UFXYDQKW\\Shook et al_2004_An assessment of the use of structural equation modeling in strategic.pdf:application/pdf;Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\8JXQ58NB\\smj.html:text/html}
}

@inproceedings{hompes_discovering_2017,
	location = {Cham},
	title = {Discovering Causal Factors Explaining Business Process Performance Variation},
	isbn = {978-3-319-59536-8},
	doi = {10.1007/978-3-319-59536-8_12},
	series = {Lecture Notes in Computer Science},
	abstract = {Business process performance may be affected by a range of factors, such as the volume and characteristics of ongoing cases or the performance and availability of individual resources. Event logs collected by modern information systems provide a wealth of data about the execution of business processes. However, extracting root causes for performance issues from these event logs is a major challenge. Processes may change continuously due to internal and external factors. Moreover, there may be many resources and case attributes influencing performance. This paper introduces a novel approach based on time series analysis to detect cause-effect relations between a range of business process characteristics and process performance indicators. The scalability and practical relevance of the approach has been validated by a case study involving a real-life insurance claims handling process.},
	pages = {177--192},
	booktitle = {Advanced Information Systems Engineering},
	publisher = {Springer International Publishing},
	author = {Hompes, Bart F. A. and Maaradji, Abderrahmane and La Rosa, Marcello and Dumas, Marlon and Buijs, Joos C. A. M. and van der Aalst, Wil M. P.},
	editor = {Dubois, Eric and Pohl, Klaus},
	date = {2017},
	langid = {english},
	note = {{ZSCC}: {NoCitationData}[s0]},
	keywords = {Process mining, Performance analysis, Root cause analysis},
	file = {Hompes et al_2017_Discovering Causal Factors Explaining Business Process Performance Variation.pdf:C\:\\Users\\ohund\\Zotero\\storage\\DFSF34T8\\Hompes et al_2017_Discovering Causal Factors Explaining Business Process Performance Variation.pdf:application/pdf}
}

@article{baker_closing_2017,
	title = {Closing the Loop: An Empirical Investigation of Causality in {IT} Business Value},
	url = {https://www.semanticscholar.org/paper/Closing-the-Loop%3A-An-Empirical-Investigation-of-in-Baker-Song/df210060211bdc598f2d3382c68c615319287f71},
	shorttitle = {Closing the Loop},
	abstract = {Researchers have established that information technology ({IT}) can improve firms’ productivity. Whether improved productivity leads to additional investment in {IT}, however, remains largely uninvestigated. In this paper, we consider whether the relationship between productivity and subsequent {IT} investment might be positive, negative, or ad hoc, and hypothesize that this relationship is positive. We analyze seven years of panel data from 1,223 healthcare firms and present empirical evidence supporting our hypothesis. When our finding is combined with extant research, it becomes reasonable to propose that unidirectional causality does not fully describe the process of {IT} business value creation. Instead, we argue that existing static models of {IT} business value with unidirectional causality can be recast as dynamic models that explicitly incorporate multiple time periods and a positive feedback relationship to more accurately capture the complexity of this process. The creation of {IT} business value can thus be understood as a positive feedback model where productivity in a given time period leads to {IT} investment in a future time period, where {IT} investment builds the stock of {IT} inputs, and where those {IT} inputs then impact productivity, beginning the cycle anew.},
	journaltitle = {undefined},
	author = {Baker, J. and Song, Jaeki and Jones, Donald R.},
	urldate = {2022-03-01},
	date = {2017},
	langid = {english},
	note = {{ZSCC}: 0000000},
	file = {Baker et al_2017_Closing the Loop.pdf:C\:\\Users\\ohund\\Zotero\\storage\\5IR5FJ47\\Baker et al_2017_Closing the Loop.pdf:application/pdf;Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\C32WXTBT\\df210060211bdc598f2d3382c68c615319287f71.html:text/html}
}

@inproceedings{ates_counterfactual_2021,
	location = {Halden, Norway},
	title = {Counterfactual Explanations for Multivariate Time Series},
	isbn = {978-1-72815-934-8},
	url = {https://ieeexplore.ieee.org/document/9462056/},
	doi = {10.1109/ICAPAI49758.2021.9462056},
	abstract = {Multivariate time series are used in many science and engineering domains, including health-care, astronomy, and high-performance computing. A recent trend is to use machine learning ({ML}) to process this complex data and these {ML}-based frameworks are starting to play a critical role for a variety of applications. However, barriers such as user distrust or difﬁculty of debugging need to be overcome to enable widespread adoption of such frameworks in production systems. To address this challenge, we propose a novel explainability technique, {CoMTE}, that provides counterfactual explanations for supervised machine learning frameworks on multivariate time series data. Using various machine learning frameworks and data sets, we compare {CoMTE} with several state-of-the-art explainability methods and show that we outperform existing methods in comprehensibility and robustness. We also show how {CoMTE} can be used to debug machine learning frameworks and gain a better understanding of the underlying multivariate time series data.},
	eventtitle = {2021 International Conference on Applied Artificial Intelligence ({ICAPAI})},
	pages = {1--8},
	booktitle = {2021 International Conference on Applied Artificial Intelligence ({ICAPAI})},
	publisher = {{IEEE}},
	author = {Ates, Emre and Aksar, Burak and Leung, Vitus J. and Coskun, Ayse K.},
	urldate = {2022-03-01},
	date = {2021-05-19},
	langid = {english},
	note = {{ZSCC}: 0000011},
	file = {Ates et al. - 2021 - Counterfactual Explanations for Multivariate Time .pdf:C\:\\Users\\ohund\\Zotero\\storage\\M4IJ8LNR\\Ates et al. - 2021 - Counterfactual Explanations for Multivariate Time .pdf:application/pdf}
}

@inproceedings{delaney_instance-based_2021,
	location = {Cham},
	title = {Instance-Based Counterfactual Explanations for Time Series Classification},
	isbn = {978-3-030-86957-1},
	doi = {10.1007/978-3-030-86957-1_3},
	series = {Lecture Notes in Computer Science},
	abstract = {In recent years, there has been a rapidly expanding focus on explaining the predictions made by black-box {AI} systems that handle image and tabular data. However, considerably less attention has been paid to explaining the predictions of opaque {AI} systems handling time series data. In this paper, we advance a novel model-agnostic, case-based technique – Native Guide – that generates counterfactual explanations for time series classifiers. Given a query time series, {TqTqT}\_\{q\}, for which a black-box classification system predicts class, c, a counterfactual time series explanation shows how {TqTqT}\_\{q\} could change, such that the system predicts an alternative class, c′c′c'. The proposed instance-based technique adapts existing counterfactual instances in the case-base by highlighting and modifying discriminative areas of the time series that underlie the classification. Quantitative and qualitative results from two comparative experiments indicate that Native Guide generates plausible, proximal, sparse and diverse explanations that are better than those produced by key benchmark counterfactual methods.},
	pages = {32--47},
	booktitle = {Case-Based Reasoning Research and Development},
	publisher = {Springer International Publishing},
	author = {Delaney, Eoin and Greene, Derek and Keane, Mark T.},
	editor = {Sánchez-Ruiz, Antonio A. and Floyd, Michael W.},
	date = {2021},
	langid = {english},
	note = {{ZSCC}: {NoCitationData}[s0]},
	keywords = {Counterfactual explanation, Time series, {XCBR}},
	file = {Delaney et al_2021_Instance-Based Counterfactual Explanations for Time Series Classification.pdf:C\:\\Users\\ohund\\Zotero\\storage\\SP9YN7AL\\Delaney et al_2021_Instance-Based Counterfactual Explanations for Time Series Classification.pdf:application/pdf}
}

@article{girin_dynamical_2021,
	title = {Dynamical Variational Autoencoders: A Comprehensive Review},
	volume = {15},
	issn = {1935-8237, 1935-8245},
	url = {http://arxiv.org/abs/2008.12595},
	doi = {10.1561/2200000089},
	shorttitle = {Dynamical Variational Autoencoders},
	abstract = {The Variational Autoencoder ({VAE}) is a powerful deep generative model that is now extensively used to represent high-dimensional complex data via a low-dimensional latent space learned in an unsupervised manner. In the original {VAE} model, input data vectors are processed independently. In recent years, a series of papers have presented different extensions of the {VAE} to process sequential data, that not only model the latent space, but also model the temporal dependencies within a sequence of data vectors and corresponding latent vectors, relying on recurrent neural networks or state space models. In this paper we perform an extensive literature review of these models. Importantly, we introduce and discuss a general class of models called Dynamical Variational Autoencoders ({DVAEs}) that encompasses a large subset of these temporal {VAE} extensions. Then we present in detail seven different instances of {DVAE} that were recently proposed in the literature, with an effort to homogenize the notations and presentation lines, as well as to relate these models with existing classical temporal models. We reimplemented those seven {DVAE} models and we present the results of an experimental benchmark conducted on the speech analysis-resynthesis task (the {PyTorch} code is made publicly available). The paper is concluded with an extensive discussion on important issues concerning the {DVAE} class of models and future research guidelines.},
	pages = {1--175},
	number = {1},
	journaltitle = {Foundations and Trends® in Machine Learning},
	shortjournal = {{MAL}},
	author = {Girin, Laurent and Leglaive, Simon and Bie, Xiaoyu and Diard, Julien and Hueber, Thomas and Alameda-Pineda, Xavier},
	urldate = {2022-03-02},
	date = {2021-12-01},
	eprinttype = {arxiv},
	eprint = {2008.12595},
	note = {{ZSCC}: {NoCitationData}[s0]
tex.ids= girin\_DynamicalVariationalAutoencoders\_2021
publisher: Now Publishers, Inc.},
	keywords = {hot, skimmed, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\I4JBA56T\\2008.html:text/html;Girin et al_2021_Dynamical Variational Autoencoders.pdf:C\:\\Users\\ohund\\Zotero\\storage\\IGKHL2PE\\Girin et al_2021_Dynamical Variational Autoencoders.pdf:application/pdf}
}

@article{kingma_introduction_2019,
	title = {An Introduction to Variational Autoencoders},
	volume = {12},
	issn = {1935-8237, 1935-8245},
	url = {http://arxiv.org/abs/1906.02691},
	doi = {10.1561/2200000056},
	abstract = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
	pages = {307--392},
	number = {4},
	journaltitle = {Foundations and Trends® in Machine Learning},
	shortjournal = {{MAL}},
	author = {Kingma, Diederik P. and Welling, Max},
	urldate = {2022-03-02},
	date = {2019-11-27},
	note = {{ZSCC}: 0000697
tex.ids= kingma\_IntroductionVariationalAutoencoders\_2019a
publisher: Now Publishers, Inc.},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, tutorial},
	file = {arXiv.org Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\SVHSVARW\\1906.html:text/html;Kingma_Welling_2019_An Introduction to Variational Autoencoders.pdf:C\:\\Users\\ohund\\Zotero\\storage\\J3XPIN78\\Kingma_Welling_2019_An Introduction to Variational Autoencoders.pdf:application/pdf;Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\5QJNNIFW\\MAL-056.html:text/html}
}

@inproceedings{koorn_looking_2020,
	location = {Cham},
	title = {Looking for Meaning: Discovering Action-Response-Effect Patterns in Business Processes},
	isbn = {978-3-030-58666-9},
	doi = {10.1007/978-3-030-58666-9_10},
	series = {Lecture Notes in Computer Science},
	shorttitle = {Looking for Meaning},
	abstract = {Process mining enables organizations to capture and improve their processes based on fact-based process execution data. A key question in the context of process improvement is how response s to an event (action) result in desired or undesired outcomes (effects). From a process perspective, this requires understanding the action-response patterns that occur. Current discovery techniques do not allow organizations to gain such insights. In this paper we present a novel approach to tackle this problem. We propose and formalize a technique to discover action-response-effect patterns. In this technique we use well-established statistical tests to uncover potential dependency relations between each response and its effect s on the cases. The goal of this technique is to provide organizations with processes that are: (1) appropriately represented, and (2) effectively filtered to show meaningful relations. The approach is evaluated on a real-world data set from a Dutch healthcare facility in the context of aggressive behavior of clients and the response s of caretakers.},
	pages = {167--183},
	booktitle = {Business Process Management},
	publisher = {Springer International Publishing},
	author = {Koorn, Jelmer J. and Lu, Xixi and Leopold, Henrik and Reijers, Hajo A.},
	editor = {Fahland, Dirk and Ghidini, Chiara and Becker, Jörg and Dumas, Marlon},
	date = {2020},
	langid = {english},
	note = {{ZSCC}: {NoCitationData}[s0]},
	keywords = {Effect measurement, Healthcare, Patterns, Process discovery},
	file = {Koorn et al_2020_Looking for Meaning.pdf:C\:\\Users\\ohund\\Zotero\\storage\\PHEFW86J\\Koorn et al_2020_Looking for Meaning.pdf:application/pdf}
}

@inproceedings{mothilal_explaining_2020,
	location = {New York, {NY}, {USA}},
	title = {Explaining machine learning classifiers through diverse counterfactual explanations},
	isbn = {978-1-4503-6936-7},
	url = {https://doi.org/10.1145/3351095.3372850},
	doi = {10.1145/3351095.3372850},
	series = {{FAT}* '20},
	abstract = {Post-hoc explanations of machine learning models are crucial for people to understand and act on algorithmic predictions. An intriguing class of explanations is through counterfactuals, hypothetical examples that show people how to obtain a different prediction. We posit that effective counterfactual explanations should satisfy two properties: feasibility of the counterfactual actions given user context and constraints, and diversity among the counterfactuals presented. To this end, we propose a framework for generating and evaluating a diverse set of counterfactual explanations based on determinantal point processes. To evaluate the actionability of counterfactuals, we provide metrics that enable comparison of counterfactual-based methods to other local explanation methods. We further address necessary tradeoffs and point to causal implications in optimizing for counterfactuals. Our experiments on four real-world datasets show that our framework can generate a set of counterfactuals that are diverse and well approximate local decision boundaries, outperforming prior approaches to generating diverse counterfactuals. We provide an implementation of the framework at https://github.com/microsoft/{DiCE}.},
	pages = {607--617},
	booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
	publisher = {Association for Computing Machinery},
	author = {Mothilal, Ramaravind K. and Sharma, Amit and Tan, Chenhao},
	urldate = {2022-03-07},
	date = {2020-01-27},
	note = {{ZSCC}: 0000268},
	file = {Mothilal et al_2020_Explaining machine learning classifiers through diverse counterfactual.pdf:C\:\\Users\\ohund\\Zotero\\storage\\PZ5WCKIC\\Mothilal et al_2020_Explaining machine learning classifiers through diverse counterfactual.pdf:application/pdf}
}

@article{schat_data_2020,
	title = {The data representativeness criterion: Predicting the performance of supervised classification based on data set similarity},
	volume = {15},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0237009},
	doi = {10.1371/journal.pone.0237009},
	shorttitle = {The data representativeness criterion},
	abstract = {In a broad range of fields it may be desirable to reuse a supervised classification algorithm and apply it to a new data set. However, generalization of such an algorithm and thus achieving a similar classification performance is only possible when the training data used to build the algorithm is similar to new unseen data one wishes to apply it to. It is often unknown in advance how an algorithm will perform on new unseen data, being a crucial reason for not deploying an algorithm at all. Therefore, tools are needed to measure the similarity of data sets. In this paper, we propose the Data Representativeness Criterion ({DRC}) to determine how representative a training data set is of a new unseen data set. We present a proof of principle, to see whether the {DRC} can quantify the similarity of data sets and whether the {DRC} relates to the performance of a supervised classification algorithm. We compared a number of magnetic resonance imaging ({MRI}) data sets, ranging from subtle to severe difference is acquisition parameters. Results indicate that, based on the similarity of data sets, the {DRC} is able to give an indication as to when the performance of a supervised classifier decreases. The strictness of the {DRC} can be set by the user, depending on what one considers to be an acceptable underperformance.},
	pages = {e0237009},
	number = {8},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Schat, Evelien and Schoot, Rens van de and Kouw, Wouter M. and Veen, Duco and Mendrik, Adriënne M.},
	urldate = {2022-03-14},
	date = {2020-11-08},
	langid = {english},
	note = {{ZSCC}: 0000008 
Publisher: Public Library of Science},
	keywords = {Machine learning, Algorithms, Machine learning algorithms, Computer vision, Probability distribution, Central nervous system, Data acquisition, Magnetic resonance imaging},
	file = {Schat et al_2020_The data representativeness criterion.pdf:C\:\\Users\\ohund\\Zotero\\storage\\R2XMHJSA\\Schat et al_2020_The data representativeness criterion.pdf:application/pdf;Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\F7H9A73U\\article.html:text/html}
}

@online{noauthor_categorical_2018,
	title = {Categorical Variational Auto-Encoders and the Gumbel Trick • David Stutz},
	url = {https://davidstutz.de/categorical-variational-auto-encoders-and-the-gumbel-trick/},
	abstract = {This article introduces categorical variational auto-encoders which allow to learn a latent space of discrete variables through the Gumbel reparameterization trick.},
	titleaddon = {David Stutz},
	urldate = {2022-03-23},
	date = {2018-08-15},
	langid = {american},
	file = {Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\NQTQEDQU\\categorical-variational-auto-encoders-and-the-gumbel-trick.html:text/html}
}

@software{davahli_stochastic_recurrent_neural_networks_2021,
	title = {stochastic\_recurrent\_neural\_networks},
	url = {https://github.com/RezaDavahli/stochastic_recurrent_neural_networks/blob/e2f64e7e5a2bc8306024c7ada104c647af2722bb/rnn_fw/models.py},
	author = {Davahli, Mohammad Reza},
	urldate = {2022-03-24},
	date = {2021-04-18},
	note = {original-date: 2021-04-12T16:47:30Z}
}

@software{davahli_stochastic_recurrent_neural_networks_2021-1,
	title = {stochastic\_recurrent\_neural\_networks},
	url = {https://github.com/RezaDavahli/stochastic_recurrent_neural_networks},
	author = {Davahli, Mohammad Reza},
	urldate = {2022-03-24},
	date = {2021-04-18},
	note = {original-date: 2021-04-12T16:47:30Z}
}

@article{davahli_optimizing_2021,
	title = {Optimizing {COVID}-19 vaccine distribution across the United States using deterministic and stochastic recurrent neural networks},
	volume = {16},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0253925},
	doi = {10.1371/journal.pone.0253925},
	abstract = {Optimizing {COVID}-19 vaccine distribution can help plan around the limited production and distribution of vaccination, particularly in early stages. One of the main criteria for equitable vaccine distribution is predicting the geographic distribution of active virus at the time of vaccination. This research developed sequence-learning models to predict the behavior of the {COVID}-19 pandemic across the {US}, based on previously reported information. For this objective, we used two time-series datasets of confirmed {COVID}-19 cases and {COVID}-19 effective reproduction numbers from January 22, 2020 to November 26, 2020 for all states in the {US}. The datasets have 310 time-steps (days) and 50 features ({US} states). To avoid training the models for all states, we categorized {US} states on the basis of their similarity to previously reported {COVID}-19 behavior. For this purpose, we used an unsupervised self-organizing map to categorize all states of the {US} into four groups on the basis of the similarity of their effective reproduction numbers. After selecting a leading state (the state with earliest outbreaks) in each group, we developed deterministic and stochastic Long Short Term Memory ({LSTM}) and Mixture Density Network ({MDN}) models. We trained the models with data from each leading state to make predictions, then compared the models with a baseline linear regression model. We also remove seasonality and trends from a dataset of non-stationary {COVID}-19 cases to determine the effects on prediction. We showed that the deterministic {LSTM} model trained on the {COVID}-19 effective reproduction numbers outperforms other prediction methods.},
	pages = {e0253925},
	number = {7},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Davahli, Mohammad Reza and Karwowski, Waldemar and Fiok, Krzysztof},
	urldate = {2022-03-24},
	date = {2021-06-07},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Forecasting, Recurrent neural networks, {COVID} 19, Linear regression analysis, Pandemics, United States, Vaccines, Virus testing},
	file = {Davahli et al_2021_Optimizing COVID-19 vaccine distribution across the United States using.pdf:C\:\\Users\\ohund\\Zotero\\storage\\CNFU666I\\Davahli et al_2021_Optimizing COVID-19 vaccine distribution across the United States using.pdf:application/pdf;Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\G7AZ3VRU\\article.html:text/html}
}

@video{simon_leglaive_dynamical_2021-1,
	title = {Dynamical Variational Autoencoders (3/5)},
	url = {https://www.youtube.com/watch?v=KA0Eq3_SSpg},
	abstract = {Video 3/5: Dynamical {VAEs} (continued)

Tutorial presented at {IEEE} {ICASSP} 2021

More resources available at https://dynamicalvae.github.io},
	author = {{Simon Leglaive}},
	urldate = {2022-03-24},
	date = {2021-06-22}
}

@software{hr_mrhazvrnn_2019,
	title = {mrhaz/{VRNN}},
	url = {https://github.com/mrhaz/VRNN},
	abstract = {Variational recurrent neural network models},
	author = {{HR}},
	urldate = {2022-03-24},
	date = {2019-06-10},
	note = {original-date: 2016-11-08T06:44:17Z}
}

@software{emmanuel_variationalrecurrentneuralnetwork_2022,
	title = {{VariationalRecurrentNeuralNetwork}},
	url = {https://github.com/emited/VariationalRecurrentNeuralNetwork},
	abstract = {Pytorch implementation of the Variational Recurrent Neural Network ({VRNN}).},
	author = {emmanuel},
	urldate = {2022-03-24},
	date = {2022-03-08},
	note = {original-date: 2017-03-19T15:51:35Z}
}

@software{mccolgan_tensorflow-vrnn_2021,
	title = {tensorflow-vrnn},
	url = {https://github.com/phreeza/tensorflow-vrnn},
	abstract = {A variational recurrent neural network implementation in tensorflow},
	author = {{McColgan}, Thomas},
	urldate = {2022-03-24},
	date = {2021-08-20},
	note = {original-date: 2016-10-22T16:56:40Z},
	keywords = {done, hot}
}

@software{harder_vrnn_2021,
	title = {{VRNN} Project},
	url = {https://github.com/frhrdr/vrnn},
	abstract = {{MSc} {AI} project on variational recurrent networks by J. Chung et al.},
	author = {Harder, Frederik},
	urldate = {2022-03-24},
	date = {2021-04-21},
	note = {original-date: 2016-09-12T09:44:55Z}
}

@online{mollaysa_summary_2020,
	title = {Summary of the recurrent latent variable model: {VRNN}},
	url = {https://medium.com/@aminamollaysa/summary-of-the-recurrent-latent-variable-model-vrnn-4096b52e731},
	shorttitle = {Summary of the recurrent latent variable model},
	abstract = {In this article we will be focusing on some insights of the paper presented by Junyoung et all:},
	titleaddon = {Medium},
	author = {Mollaysa, Amina},
	urldate = {2022-03-24},
	date = {2020-04-10},
	langid = {english},
	file = {Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\B76MQIYE\\summary-of-the-recurrent-latent-variable-model-vrnn-4096b52e731.html:text/html}
}

@online{noauthor_model_nodate,
	title = {Model Zoo - {VRNN} {PyTorch} Model},
	url = {https://modelzoo.co/model/vrnn},
	urldate = {2022-03-24},
	file = {Model Zoo - VRNN PyTorch Model:C\:\\Users\\ohund\\Zotero\\storage\\3A3WMBP2\\vrnn.html:text/html}
}

@article{gulrajani_pixelvae_2016,
	title = {{PixelVAE}: A Latent Variable Model for Natural Images},
	url = {http://arxiv.org/abs/1611.05013},
	shorttitle = {{PixelVAE}},
	abstract = {Natural image modeling is a landmark challenge of unsupervised learning. Variational Autoencoders ({VAEs}) learn a useful latent representation and model global structure well but have difficulty capturing small details. {PixelCNN} models details very well, but lacks a latent code and is difficult to scale for capturing large structures. We present {PixelVAE}, a {VAE} model with an autoregressive decoder based on {PixelCNN}. Our model requires very few expensive autoregressive layers compared to {PixelCNN} and learns latent codes that are more compressed than a standard {VAE} while still capturing most non-trivial structure. Finally, we extend our model to a hierarchy of latent variables at different scales. Our model achieves state-of-the-art performance on binarized {MNIST}, competitive performance on 64x64 {ImageNet}, and high-quality samples on the {LSUN} bedrooms dataset.},
	journaltitle = {{arXiv}:1611.05013 [cs]},
	author = {Gulrajani, Ishaan and Kumar, Kundan and Ahmed, Faruk and Taiga, Adrien Ali and Visin, Francesco and Vazquez, David and Courville, Aaron},
	urldate = {2022-03-25},
	date = {2016-11-15},
	eprinttype = {arxiv},
	eprint = {1611.05013},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\RZ59SA3Q\\1611.html:text/html;Gulrajani et al_2016_PixelVAE.pdf:C\:\\Users\\ohund\\Zotero\\storage\\IIIGPABC\\Gulrajani et al_2016_PixelVAE.pdf:application/pdf}
}

@article{hochreiter_long_1997,
	title = {Long Short-Term Memory},
	volume = {9},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory ({LSTM}). Truncating the gradient where this does not do harm, {LSTM} can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. {LSTM} is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, {LSTM} leads to many more successful runs, and learns much faster. {LSTM} also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	pages = {1735--1780},
	number = {8},
	journaltitle = {Neural Computation},
	shortjournal = {Neural Comput.},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	urldate = {2022-03-27},
	date = {1997-11-01}
}

@artwork{fdeloche_english_2017,
	title = {English:  A diagram for a one-unit recurrent neural network ({RNN}). From bottom to top : input state, hidden state, output state. U, V, W are the weights of the network. Compressed diagram on the left and the unfold version of it on the right.},
	url = {https://commons.wikimedia.org/wiki/File:Recurrent_neural_network_unfold.svg},
	shorttitle = {English},
	author = {{fdeloche}},
	urldate = {2022-03-27},
	date = {2017-06-19},
	file = {Wikimedia Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\HKIFTFYC\\FileRecurrent_neural_network_unfold.html:text/html}
}

@article{vaswani_attention_2017,
	title = {Attention Is All You Need},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	journaltitle = {{arXiv}:1706.03762 [cs]},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2022-03-27},
	date = {2017-12-05},
	eprinttype = {arxiv},
	eprint = {1706.03762},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\EA38I6IJ\\1706.html:text/html;Vaswani et al_2017_Attention Is All You Need.pdf:C\:\\Users\\ohund\\Zotero\\storage\\YY7ATS65\\Vaswani et al_2017_Attention Is All You Need.pdf:application/pdf}
}

@online{noauthor_implementing_2020,
	title = {Implementing The Levenshtein Distance in Python},
	url = {https://blog.paperspace.com/implementing-levenshtein-distance-word-autocomplete-autocorrect/},
	abstract = {This tutorial works through a step-by-step example of how to implement the Levenshtein distance in Python for word autocorrection and autocompletion.},
	titleaddon = {Paperspace Blog},
	urldate = {2022-03-27},
	date = {2020-03-15},
	langid = {english},
	file = {Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\ZAI2RZDQ\\implementing-levenshtein-distance-word-autocomplete-autocorrect.html:text/html}
}

@article{damerau_technique_1964,
	title = {A technique for computer detection and correction of spelling errors},
	volume = {7},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/363958.363994},
	doi = {10.1145/363958.363994},
	abstract = {The method described assumes that a word which cannot be found in a dictionary has at most one error, which might be a wrong, missing or extra letter or a single transposition. The unidentified input word is compared to the dictionary again, testing each time to see if the words match—assuming one of these errors occurred. During a test run on garbled text, correct identifications were made for over 95 percent of these error types.},
	pages = {171--176},
	number = {3},
	journaltitle = {Communications of the {ACM}},
	shortjournal = {Commun. {ACM}},
	author = {Damerau, Fred J.},
	urldate = {2022-04-15},
	date = {1964},
	file = {Full Text PDF:C\:\\Users\\ohund\\Zotero\\storage\\945RHSHC\\Damerau - 1964 - A technique for computer detection and correction .pdf:application/pdf}
}

@article{levenshtein_binary_1965,
	title = {Binary codes capable of correcting deletions, insertions, and reversals},
	url = {https://www.semanticscholar.org/paper/Binary-codes-capable-of-correcting-deletions%2C-and-Levenshtein/b2f8876482c97e804bb50a5e2433881ae31d0cdd},
	abstract = {Semantic Scholar extracted view of \&quot;Binary codes capable of correcting deletions, insertions, and reversals\&quot; by V. Levenshtein},
	journaltitle = {undefined},
	author = {Levenshtein, V.},
	urldate = {2022-04-15},
	date = {1965},
	langid = {english},
	file = {Levenshtein - 1965 - Binary codes capable of correcting deletions, inse.pdf:C\:\\Users\\ohund\\Zotero\\storage\\C7U66UK5\\Levenshtein - 1965 - Binary codes capable of correcting deletions, inse.pdf:application/pdf;Snapshot:C\:\\Users\\ohund\\Zotero\\storage\\UKNK3JBN\\b2f8876482c97e804bb50a5e2433881ae31d0cdd.html:text/html}
}