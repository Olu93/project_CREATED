\documentclass[./../../paper.tex]{subfiles}
\graphicspath{{\subfix{./../../figures/}}}

\begin{document}
\subsection{Generating Counterfactuals}
The topic of counterfactual generation as explanation method was introduced by \citeauthor{wachter_CounterfactualExplanationsOpening_2017} in \citeyear{wachter_CounterfactualExplanationsOpening_2017}\autocite{wachter_CounterfactualExplanationsOpening_2017}. The authors defined a loss function which incorporates the criteria to generate a counterfactual which maximizes the likelihood for a predefined outcome and minimizes the distance to the original instance. However, the solution of \citeauthor{wachter_CounterfactualExplanationsOpening_2017} did not account for the minimalisation of feature changes and does not penalize unrealistic features. Furthermore, their solution cannot incorporate categorical variables.

A newer approach by \citeauthor{dandl_MultiObjectiveCounterfactualExplanations_2020} incoporates four main criteria for counterfactuals (see \autoref{sec:counterfactuals}) by applying a genetic algorithm with a multi-objective fitness function\autocite{dandl_MultiObjectiveCounterfactualExplanations_2020}. This approach strongly differs from gradient-based methods, as it does not require a differentiable objective function. However, their solution was only tested on static data.

\subsection{Generating Counterfactual Sequences}
When it comes to sequential data most researchers work on ways to generate counterfactuals for natural language. This often entails generating univariate discrete counterfactuals with the use of \gls{DL} techniques. \citeauthor{martens_Explainingdatadrivendocument_2014} and later \citeauthor{krause_InteractingPredictionsVisual_2016} are early examples of counterfactual NLP research\autocites{martens_Explainingdatadrivendocument_2014,krause_InteractingPredictionsVisual_2016}. Their approach strongly focuses on the manipulation of sentences to achieve the desired outcome. However, as \citeauthor{robeer_GeneratingRealisticNatural_2021} puts it, their counterfactuals do not comply with \emph{realisticness}\autocite{robeer_GeneratingRealisticNatural_2021}.

Instead, \citeauthor{robeer_GeneratingRealisticNatural_2021} showed that it is possible to generate realistic counterfactuals with a \gls{GAN}\autocite{robeer_GeneratingRealisticNatural_2021}. They use the model to implicitly capture a latent state space and sample counterfactuals from it. Apart from implicitly modelling the latent space with \glspl{GAN}, it is possible to sample data from an explicit latent space. Examples of these approaches often use an encoder-decoder pattern in which the encoder encodes a data instance into a latent vector, which will be peturbed and then decoded into a a similar instance\autocites{melnyk_ImprovedNeuralText_2017,wang_ControllableUnsupervisedText_2019}. By modelling the latent space, we can simply sample from a distribution conditioned on the original instance. \citeauthor{bond-taylor_DeepGenerativeModelling_2021} provides an overview of the strengths and weaknesses of common generative models.

Eventhough, a single latent vector model can theoretically produce multivariate sequences, it may still be too restrictive to capture the combinatorial space of multivariate sequences. Hence, most of the models within \gls{NLP} were not used to produce a sequence of vectors, but a sequence of discrete symbols. For process instances, we can assume a causal relation between state vectors in a sequential latent space. We call models that capture a sequential latent state-space which has causal relations \emph{dynamic}\autocite{leglaive_RecurrentVariationalAutoencoder_2020}. Early models of this type of dynamic latent state-space models are the well-known \emph{Kalman-Filter} for continous states and \gls{HMM} for discrete states. In recent literature, many techniques use \gls{DL} to model complex state-spaces. The first models of this type were developed by \citeauthor{krishnan_StructuredInferenceNetworks_2017}\autocite{krause_InteractingPredictionsVisual_2016, krishnan_StructuredInferenceNetworks_2017}. Their \gls{DKF} and subsequent \gls{DMM} approximate the dynamic latent state-space by modeling the latent space given the data sequence and all previous latent vectors in the sequence. There are many variations\autocites{chung_RecurrentLatentVariable_2016,fraccaro_Sequentialneuralmodels_2016,leglaive_RecurrentVariationalAutoencoder_2020} of \citeauthor{krishnan_StructuredInferenceNetworks_2017}'s model, but most use \gls{ELBO} of the posterior for the current $Z_{t}$ given all previous $\{Z_{t-1},\ldots,Z_{1}\}$ and $X_{t}$\autocite{girin_DynamicalVariationalAutoencoders_2021}.

\subsection{Generating Counterfactual Time-Series}
Within the \emph{multivariate time-series} literature two recent approaches yield ideas worth discussing.

First, \citeauthor{delaney_InstanceBasedCounterfactualExplanations_2021} introduces a case-based reasoning to generate counterfactuals\autocite{delaney_InstanceBasedCounterfactualExplanations_2021}. Their method uses existing counterfactual instances, or \emph{prototypes}, in the dataset. Therefore, it ensures, that the proposed counterfactuals are \emph{realistic}. However, case-based approaches strongly depend on the \emph{representativeness} of the prototypes\autocite[p. 192]{molnar2019}. In other words, if the model displays behaviour, which is not capture within the set of prototypical instances, most case-based techniques will fail to provide viable counterfactuals. The likelihood of such a break-down increases due to the combinatorial explosion of possible behaviours if the \emph{true} process model has cycles or continuous event attributes. Cycles may cause infinite possible sequences and continous attributes can take values on a domain within infinite negative and positive bounds. These issues have not been explored in the paper of \citeauthor{delaney_InstanceBasedCounterfactualExplanations_2021}, as it mainly deals with time series classification\autocite{delaney_InstanceBasedCounterfactualExplanations_2021}. However, despite these shortcomings, case-based approaches may act as a valuable baseline against other sophisticated approaches.

The second paper within the multivariate time series field by \citeauthor{ates_CounterfactualExplanationsMultivariate_2021} also uses a case-based approach\autocite{ates_CounterfactualExplanationsMultivariate_2021}. However, it contrasts from other approaches, as it does not specify a particular model but proposes a general framework instead. Hence, within this framework, individual components could be substituted by better performing components. Describing a framework, rather than specifying a particular model, allows to adapt the framework, due to the heterogeneous process dataset landscape. In this paper, we will also introduce a framework that allows for flexibility depending on the dataset. 
% \optional{The framework will be evaluated in two steps. The first step aims to compare various model types against eachother based on the countefactual viability. The second step scrutinizes the best framework configurations from step one, by presenting its results to a domain expert.}

\subsection{Generating Counterfactuals for Business Processes}
So far, none of the models have been applied to process data.

Within \gls{PM}, \gls{causalinference} has long been used to analyse and model business processes. Mainly, due to the causal relationships underlying each process. However, early work has often attempted to incorporate domain-knowledge about the causality of processes in order to improve the process model itself\autocites{shook_assessmentusestructural_2004,baker_ClosingLoopEmpirical_2017,hompes_DiscoveringCausalFactors_2017,wang_CounterfactualDataAugmentedSequential_2021}.
Among these, \citeauthor{narendra_CounterfactualReasoningProcess_2019} approach is one of the first to include counterfactual reasoning for process optimization\autocite{narendra_CounterfactualReasoningProcess_2019}.
\citeauthor{oberst_CounterfactualOffPolicyEvaluation_2019} use counterfactuals to generate alternative solutions to treatments, which lead to a desired outcome\autocite{oberst_CounterfactualOffPolicyEvaluation_2019}.
Again, the authors do not attempt to provide an explanation of the models outcome and therefore, disregard multiple viability criterions for counterfactuals in \gls{XAI}. \citeauthor{qafari_CaseLevelCounterfactual_2021} published the most recent paper on the counterfactual generation of explanations\autocite{qafari_CaseLevelCounterfactual_2021}. The authors, use a known \gls{SCM}, to guide the generation of their counterfactuals. However, this approach requires a process model which is as close as possible to the \emph{true} process model. For our approach, we assume that no knowledge about the dependencies are known.

Within the \gls{XAI} context, \citeauthor{tsirtsis_CounterfactualExplanationsSequential_2021} develop the first explanation method for process data\autocite{tsirtsis_CounterfactualExplanationsSequential_2021}. However, their work closely resembles the work of \citeauthor{oberst_CounterfactualOffPolicyEvaluation_2019} and treat the task as \gls{MDP}\autocite{oberst_CounterfactualOffPolicyEvaluation_2019}. This extension of a regular \gls{MP} assumes that an actor influences the outcome of a process given the state. This formalisation allows the use of \gls{RL} methods like Q-learning or SARSA. However, this often requires additional assumptions such as a given reward function and an action-space. For counterfactual sequence generation, there is no obvious choice for the reward function or the action-space. Nonetheless, both \citeauthor{tsirtsis_CounterfactualExplanationsSequential_2021} and \citeauthor{oberst_CounterfactualOffPolicyEvaluation_2019} contribute an important idea. The idea of incrementally generating the counterfactual instead of the full sequence. \citeauthor{hsieh_DiCE4ELInterpretingProcess_2021} build on this concept by proposing a system that generates counterfactuals milestone-wise\autocite{hsieh_DiCE4ELInterpretingProcess_2021}. Their work is the closest to our approach. The authors recognised that some processes have critical events, which govern the overall outcome. Hence, by simply avoiding the undesired outcome from milestone to milestone, it is possible to limit the search space and compute viable counterfactuals with counterfactual generation methods, such as the DiCE alogorithm. However, their approach presupposes that the critical event points are known and the outcome is binary. Especially, the first condition makes their approach heavily dependent on the data which is used.

\end{document}