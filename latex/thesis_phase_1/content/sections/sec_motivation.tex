\documentclass[./../../paper.tex]{subfiles}
\graphicspath{{\subfix{./../../figures/}}}

\begin{document}
Many processes, often medical, economical, or administrative in nature, are governed by sequential events and their contextual environment. Many of these events and their order of appearance play a crucial part in the determination of every possible outcome. With the rise of AI and the increased abundance of data in recent years several techqniques emerged that help to predict the outcomes of complex processes in the real world. 

Research in the Process Mining discipline has shown that is possible to predict the outcome of a particular process fairly well\needscite{}. 
% THIS IS A TEST FOR\needscite{}. ANOTHER WAY WOULD BE\needscite{}. BUT ULTIMATELY WE WANT \needscite{THAT}.
For instance, in the medical domain, models have been shown to predict the outcome or trajectory of a patient's condition\needscite{}. In the private sector, process models can be used to detect faults or outliers. \gls{dl}, in particular, has shown promising results within domains that have been considered difficult for decades\needscite{Moravec Paradox}. However, while many prediction models can easily predict certain outcomes, it remains a difficult challenge to understand their reasoning. This difficulty arises from models, like neural networks, that are so-called \glspl{bbm}. Meaning, that their inference is imcomprehensible, due to the vast amount of parameters involved. This lack of comprehension is undesirable for many fields like IT or finance. Not knowing why a loan was given, makes it impossible to rule out possible biases. Knowing what will lead to a system failure, will help us knowing how to avoid it. In critical domains like medicine, the reasoning behind decisions become crucial. For instance, if we know that a treatment process of a patient reduces the chances for survival, we want to know which treatment step is the critical factor we ought to avoid. To summarise, knowing the outcome of a process often leads us to questions on how to change it. Formally, we want to change the outcome of a process, by making it maximally likely, with as little interventions as possible\needscite{}. \autoref{fig:desired_outcome} is a visual representation of the desired goal.

\needsfigure{fig:desired_outcome}{This figure illustrates a model, that predicts a certain trajectory of the process. However, we want to change the process steps in such a way, that it changes the outcome.}

One-way to better understand the \gls{ml} models lies within the \gls{XAI} discipline. \gls{XAI} dedicates its research to the research and development of \glspl{bbm}. Most of the discipline's techniques produce explanations that guide our understanding. Explanations can come in various forms, such as \attention{examples}\needscite{}. but some are more comprehensible for humans than others. 

A prominent and human-friendly approach are counterfactuals\needscite{}. Counterfactuals within the AI framework help us to answer hypothetical "what-if" questions. Basically, if we know \emph{what} would happen \emph{if} we changed the process, we could change it for the better. In this thesis, we will raise the question, how we can use counterfactuals to change the trajectory of a process models' prediction towards a desired outcome. Knowing the answers will not only increase the understanding of \glspl{bbm}, but also help us avoid or enforce certain outcomes. 
\end{document}