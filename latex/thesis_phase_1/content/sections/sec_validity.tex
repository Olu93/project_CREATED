\documentclass[./../../paper.tex]{subfiles}
\graphicspath{{\subfix{./../../figures/}}}

\begin{document}
In this section we define the components of the metric to measure the validity of a sequence.

\subsection{Precision}
We can evaluate the precision of a counterfactual trace by determining whether a counterfactual leads to the desired outcome. For this purpose, we use the predictive model which will out put a prediction base on the counterfactual sequence. However, it is often difficult to force a deterministic model to produce a different result. We can relax the condition by maximising the likelihood of the counterfactual outcome. If we compare the likelihood of the desired outcome under the factual sequence with the counterfactual sequence, we can determine an increase or decrease. Ideally, we want to increase the likelihood. We can compute a odds or the difference between the two likelihoods. We choose to use the odds\attention{Needs some thought or testing. Odds may  be too aggressive.}. In \autoref{eq:precision}, we define the function.

\begin{align}
    \label{eq:precision}
    odds = \frac{p(o^*|e^*)}{p(o^*|e)}
\end{align}

Here, $p(o|e)$ describes the probability of an outcome, given a sequence of events.

\subsection{Feasibility}
To determine the feasibility of a counterfactual trace, it is important to recognise two components. First, we have to compute the probability of the sequence of events themselves. This is a difficult task, given the \emph{open world assumption}. In theory, we cannot know whether any event \emph{can} follow after a nother event or not. However, we can assume that the data is representative of the process dynamics. Hence, we can simply compute the first-order transition probability by counting each transition. However, the issue remains that longer sequences tend to have a zero probability if they have never been seen in the data. We use the Kneser-Ney Smoothing\needscite{} approach to ensure that unseen sequences are accounted for. Second, we have compute the feasibility of the individual feature values given the sequence. We can relax the computation of this probability using the \emph{markov assumption}. In other words, we assume that each event determines its feature values. Meaning, we can model density estimators for every event and use them to determine the likelihood of a set of features. In \autoref{eq:feasibility}, we define both parts of the function.

\begin{align}
    \label{eq:feasibility}
    feasibility_e & =p(a_n|a_1\ldots e_{n-1}) \approx  p(a_n|a_{n-1}) \\
    feasibility_f & =p(f|a_n)
\end{align}

\noindent Here, $a \text{ and } f$ are the activity and features of a particular event. The first equation shows the approximation based on the markov assumption.

\subsection{Similarity}
We use a function to compute the distance between the factual sequence and the counterfactual candidates. Here, a low distance corresponds to a small change. We will use a modified version of the Damerau-Levenshtein distance. This distance computes the costs associated with aligning two sequences by taking changes, inserts, deletes and transpositions of elements into account. Each of these alignment operations is typically associated with a cost of 1. This allows us to directly compute the structural difference between two sequences regardless of their lenghts. Additionally, instead of computing a cost of 1 for every operation, we compute a cost of a distance between the feature vectors of each step. This allows us to take not only the sequential differences into account but the feature differences, too. The similarity edit function is defined in \autoref{eq:similarity}.

\begin{align}
    \label{eq:similarity}
    d_{a, b}(i, j) & =\min
    \begin{cases}
        0                                 & \text { if } i=j=0                                          \\
        \editDistance{i-1}{j  }+\editCost & \text { if } i>0                                            \\
        \editDistance{i  }{j-1}+\editCost & \text { if } j>0                                            \\
        \editDistance{i-1}{j-1}+\editCost & \text { if } i, j>0                                         \\
        \editDistance{i-2}{j-2}+\editCost & \text { if } i, j>1 \land a_{i}=b_{j-1} \land a_{i-1}=b_{j}
    \end{cases}         \\
    \editCost      & = dist(a_i, b_j) \\
    a_i,b_j        & \in \mathbb{R}^d
\end{align}

\noindent Here, $dist(x,y)$ is an arbitrary distance metric. $i \text{ and } j$ are the indidices of the sequence elements $a \text{ and } b$, respectively.

\subsection{Sparcity}
Sparsity refers to the number of changes between the factual and counterfactual sequence. We typically want to minimize the number of changes. However, sparcity is hard to measure, as we cannot easily count the changes. There are two reasons, why this is the case: First, the sequences that are compared can have varying lengths. Second, even if they were the same length, the events might not line up in such a way, that we can simply count the changes to a feature. Hence, to solve this issue we use another modified version of the Damerau-Levenshstein edit distance. The sparcity edit function is defined in \autoref{eq:sparcity}.

\begin{align}
    \label{eq:sparcity}
    d_{a, b}(i, j) & =\min
    \begin{cases}
        0                                 & \text { if } i=j=0                                          \\
        \editDistance{i-1}{j  }+\editCost & \text { if } i>0                                            \\
        \editDistance{i  }{j-1}+\editCost & \text { if } j>0                                            \\
        \editDistance{i-1}{j-1}+\editCost & \text { if } i, j>0                                         \\
        \editDistance{i-2}{j-2}+\editCost & \text { if } i, j>1 \land a_{i}=b_{j-1} \land a_{i-1}=b_{j}
    \end{cases}                             \\
    \editCost      & = \sum_d \mathbb{I}(a_{id} = b_{jd}) \\
    a_i,b_j        & \in \mathbb{R}^d
\end{align}

\noindent Here, $\sum_d \mathbb{I}(a_{id} = b_{jd})$ is an indicator function, that is used to count the number of changes in a vector.


% \subsection{Diversity}
% To measure the diversity of the generated counterfactual candidates, we are interested in the differences between all counterfactual candidates. Diversity can be understood as the inverse of the average similarity of all counterfactual candidate pairs. However, as we have to incoporate the sequential aspect of the counterfactuals, we need to compute the similarity across the whole sequence. We could use the Sparsity measure but the Damerau-Levenshtein measure is expensive to compute. Therefore, we simply align the candidates on their last step and pad the sequence with zeros. Hence, all sequences will have the length of the longest sequence. Hence, we compute the diversity as follows:

% \begin{align}
%     diversity  & = \frac{1}{\frac{1}{nt} \sum_{n}\sum_{t} sim(a_t^n, b_t^n)  }  
% \end{align}

% Here, \attention{explain variables}.

\end{document}