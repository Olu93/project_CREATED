\documentclass[./../../paper.tex]{subfiles}
\graphicspath{{\subfix{./../../figures/}}}

\begin{document}
In this section we define the components of the metric to measure the validity of a sequence.

\subsection{Precision}
We can evaluate the precision of a counterfactual trace by determining whether a counterfactual leads to the desired outcome. For this purpose, we use the predictive model which will out put a prediction base on the counterfactual sequence. However, it is often difficult to force a deterministc model to produce a different result. We can relax the condition by maximising the likelihood of the counterfactual outcome. If we compare the likelihood of the desired outcome under the factual sequence with the counterfactual sequence, we can determine an increase or decrease. Ideally, we want to increase the likelihood. We can compute a odds or the difference between the two likelihoods. We choose to use the odds. In \autoref{eq:precision}, we define the function.

\begin{align}
    \label{eq:precision}
    odds = \frac{p(o^*|x^*_t)}{p(o^*|x_t)}
\end{align}

Here, \attention{explain variables}.

\subsection{Feasibility}
To determine the feasibility of a counterfactual trace, it is important to recognise two components. First, we have to compute the probability of the sequence of events themselves. This is a difficult task, given the \emph{open world assumption}. In theory, we cannot know whether any event \emph{can} follow after a nother event or not. However, we can assume that the data is representative of the process dynamics. Hence, we can simply compute the first-order transition probability by counting each transition. However, the issue remains that longer sequences tend to have a zero probability if they have never been seen in the data. We use the Kneser-Ney Smoothing\needscite{} approach to ensure that unseen sequences are accounted for. Second, we have compute the feasibility of the individual feature values given the sequence. We can relax the computation of this probability using the \emph{markov assumption}. In other words, we assume that each event determines its feature values. Meaning, we can model density estimators for every event and use them to determine the likelihood of a set of features.In \autoref{eq:precision}, we define both parts of the function.

\begin{align}
    \label{eq:feasibility}
    feasibility_e  & =p(e_n|e_1\ldots e_{n-1}) \approx  p(e_n|e_{n-1}) \nonumber \\
    feasibility\_f & =p(f|e_n)                                                   \\
\end{align}

Here, \attention{explain variables}.

\subsection{Sparsity}
Sparsity refers to the number of changes between the factual and counterfactual sequence. We typically want to minimise the number of changes. We use a function to compute the distance between the factual sequence and the counterfactual candidates. Here, a low distance corresponds to a small change. We will use a modified version of the Damerau-Levenshtein distance. This distance computes the costs associated with aligning two sequences by taking changes, inserts, deletes and transpositions of elements into account. Each of these alignment operations is typically associated with a cost of 1. This allows us to directly compute the structural difference between two sequences regardless of their lenghts. Additionally, instead of computing a cost of 1 for every operation, we compute a cost of a distance between the feature vectors of each step. This allows us to take not only the sequential differences into account but the feature differences, too.

\begin{align}
    d_{a, b}(i, j)  & =\min
    \begin{cases}
        cost_{a_i, b_j}                    & \text { if } i=j=0                                          \\
        d_{a, b}(i-1, j)+cost_{a_i, b_j}   & \text { if } i>0                                            \\
        d_{a, b}(i, j-1)+cost_{a_i, b_j}   & \text { if } j>0                                            \\
        d_{a, b}(i-1, j-1)+cost_{a_i, b_j} & \text { if } i, j>0                                         \\
        d_{a, b}(i-2, j-2)+cost_{a_i, b_j} & \text { if } i, j>1 \land a_{i}=b_{j-1} \land a_{i-1}=b_{j}
    \end{cases}          \\
    cost_{a_i, b_j} & = dist(a_i, b_j)
\end{align}

Here, \attention{explain variables}.

\subsection{Diversity}
To measure the diversity of the generated counterfactual candidates, we are interested in the differences between all counterfactual candidates. Diversity can be understood as the inverse of the average similarity of all counterfactual candidate pairs. However, as we have to incoporate the sequential aspect of the counterfactuals, we need to compute the similarity across the whole sequence. We could use the Sparsity measure but the Damerau-Levenshtein measure is expensive to compute. Therefore, we simply align the candidates on their last step and pad the sequence with zeros. Hence, all sequences will have the length of the longest sequence. Hence, we compute the diversity as follows:

\begin{align}
    diversity  & = \frac{1}{\frac{1}{nt} \sum_{n}\sum_{t} sim(a_t^n, b_t^n)  }  
\end{align}

Here, \attention{explain variables}.

\end{document}