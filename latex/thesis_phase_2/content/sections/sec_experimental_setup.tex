\documentclass[./../../paper.tex]{subfiles}
\graphicspath{{\subfix{./../../figures/}}}

\begin{document}

As mentioned in \autoref{sec:counterfactuals}, counterfactual generation is notorious for their lack of a standardised evaluation procedure. Nonetheless, we attempt to address our research questions in three experiments.

\subsubsection{Evaluation 1: Model Comparison}
First, we assess the viability of \attention{a number of} models. For this purpose, we sample \attention{10} factuals and use the models to generate \attention{50} counterfactuals. We determine the \attention{mean} viability across the counterfactuals. With this experiment, we show that a model which optimizes quality criteria of counterfactuals produces better results than models, which do not. Hence, we expect the evolutionary algorithm to perform best, as it can directly optimize multiple viability criterions. In the following we list all models, we are going to compare:

\begin{enumerate}
    \item[RNG] A \ModelRNG, which generates random values and acts as a baseline. 
    \item[CBG] A \ModelCBG, which samples from process instances within the training set
    % \item[VAE] A \ModelVAE model, which samples counterfactuals from a latent space
    \item[EVO] A \ModelEVO, which optimizes viability using principles of evolution.
\end{enumerate}

\subsubsection{Evaluation 2: Comparing with alternative Literature}
The model comparison is not enough to establish the validity of our solution, as defined proposed the viability measure ourselves. Therefore, we also assess each model based on the evaluation criterions of an alternative work. More precisely, we quantify the viability of our models using the metrics employed by \citeauthor{hsieh_DiCE4ELInterpretingProcess_2021}. Hence, we measure the sparsity by computing the average Levenshtein difference and proximity using the L2-Norm. Furthermore, we compute the average diversity, plausibility and the rate of changing the prediction to a desired one. The goal is to show that models, which optimise viability criterions, remain to perform better, even if viability is assessed differently. 

\subsubsection{Evaluation 3: Qualitative Assessment}
For the last assessment, we follow \citeauthor{hsieh_DiCE4ELInterpretingProcess_2021}'s procedure of assessing the models qualitatively. We use the same examples as the authors do. However, as we focus on outcome prediction, we attempt to answer one of two questions: 

\begin{enumerate}
    \item \emph{what would I have had to change to prevent the cancellation/rejection of the loan application process}
    \item \emph{what would I have had to change to get cancelled/rejected of the loan application process}
\end{enumerate}

The goal is to show, that the results are viable despite not having a standardized protocol to measure their viability.

\end{document}