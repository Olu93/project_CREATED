\documentclass[./../../paper.tex]{subfiles}
\graphicspath{{\subfix{./../../figures/}}}

\begin{document}

To conclude this section, we have to stress again, that there are many ways to define feasibility. We chose a probablistic approach. There is an issue with this approach. Shorter sequences naturally have higher probabilities. Hence, we introduce a bias into our viability measure towards short sequences. This bias can be beneficial or detrimental depending on the use case. For instance, a medical process model might favor shorter counterfactual explanations. Mainly, because we want to understand how we can effectively reduce the time of illness. However, if we want to explain a highly standardised manufacturing process that went wrong in one instance; then, we would rather keep the counterfactual as close as possible to the factual.    


% TODO: Discuss how this formulation favors shorter sequences. Might be solved by using an exact sequence prob computation. Deep-Normalizing-Flows with VAE for instance.


% \subsubsection{Practical Matters}
% The general computation of these products is trivial. However, we need to probabilities for $\prob{e_0}$, $\cprob{f_t}{e_t}$ and $\cprob{e_t}{e_{t-1}}$ as the true distributions are unobservable.

% Starting with the transition dynamics part of the equation $\cprob{e_t}{e_{t-1}}$, we can estimate the model parameters by counting the transitions from one event state \optional{(acitvity)} to another \optional{(acitvity)}.\attention{Define the difference between event und activity in a better way and earlier.} $\prob{e_0}$ is a special case, as it does not have a preceding event.

% The emission probabilities are more complicated for three reasons: First, the event distribution does not necessarily belong to the same family as the feature distribution. Hence, we cannot use any simple method to estimate these conditionals. We need to estimate the probability for each event seperately. 
% The second issue directly follows from the first. If we estimate each event distribution by partioning the data by events, we naturally have less data to estimate each model's parameters. Although, event partitioning, are not an issue for common events states \optional{(activities)}, they can make emission probabilities of less frequent event states exceptionally hard to estimate.
% One can turn to Bayesian Methods, which hand these situations better by specifying a prior.

% However, the third issue exacerbates the main issue of using bayesian methods. Namely, because features do not necessarily have to be from the same distributional family, we have to model each conditional distribution with a mixture of distributions. Hence, simple bayesian updates are not possible either \optional{and require more time expensive methods such as Markov-Chain-Monte-Carlo methods or similar}.\attention{Check if this is true. Maybe we \emph{can} use MCSC}.

% From these issues, we can conclude there are multiple viable ways to model these conditional distributions and we have to choose an fitting method\footnotemark. \footnotetext{Note, that we did not mention modelling $\cprob{f_0}{e_0}$ as it is practically the same distribution as $\cprob{f_t}{e_t}$.}

% \subsubsection{Model Description}
% Knowing, there are many ways to model the sequence distribution, we choose to implement a number of different menthods. However, we evaluate them based on how well they fit the data distribution and choose the most promising method.

% \noindent\textbf{Transition Dynamics:} For the transition dynamics we count the individual transitions as mentioned by using a \emph{Transition-Count-Matrix}. Then, we compute the probabilities of each transition by dividing occurence count of the preceding event-state. We apply the same method on the $\prob{e_0}$. The only difference is that we count the starts and divide by thenumber of available cases.

% \noindent\textbf{Emission Probability:} For $\cprob{f_t}{e_t}$, we employ \attention{3} tactics:
% \begin{enumerate}
%     \item[Independent:] Here, we assume all feature columns are independent variables with no covariations. Hence, for discrete variables, we use Categorical distributions. In other words, if the variable is binary, we count the ones to estimate the parameters of a Bernoulli distribution. Similar holds for categorical distributions. In contrast, we use independent Gaussian distributions for continuous variables.
%     \item[Grouped:] Here, we group variables from the same distributional family and estimate their parameters. Meaning, we take all discrete distributions and compute the parameters of one categorical distribution. Likewise, we group all continuous variables and compute the parameters, mean and covariance, for one multivariate Gaussian distribution.
%     \item[Grouped with $\chi^2$:] This is similar to the grouped approach. However, a multivariate Gaussian is also a continuous distribution. The likelihood of a continuous density distribution is not limited to a range between 0 and 1. Only the area under the density funtion has to follow this restriction. Meaning, if we compute the likelihood of a specific data point we might end up values of 30 or event 300000. Therefore, we rather interprete a data point as the mean of a another Gaussian. With this assumption, we can use the $\chi^2$ distribution to compute the probability of that distribution belonging to the distribution at hand. Or rather, how likely it is to find another datapoint which is more likely to belong to the distribution. Here, we say $Q=(Y-\mu)^{T} \Sigma^{-1}(Y-\mu)$  and assume $Q \sim \chi^{2}(k)$. If Q is bigger than $(x-\mu)^{T} \Sigma^{-1}(x-\mu)$, then $\mathbb{P}\left[(y-\mu)^{T} \Sigma^{-1}(y-\mu) \geq(x-\mu)^{T} \Sigma^{-1}(x-\mu)\right]=1-\mathbb{P}\left[Q \leq(x-\mu)^{T} \Sigma^{-1}(x-\mu)\right]$
% \end{enumerate}

% \noindent\textbf{Gaussian Distribution under Event Partitions}
% \xixi{This section requires an intuitive understanding of linear algebra and the formula of simple gaussians and multivariate gaussians. Shall I put this in the discussion section instead?}
% Gaussian distributions are particularly vulnerable to data shortages due to event partionings.

% For instance, if we use independent variables, we often face issues of columns without any variation. Those lead to a covariance matrices' determinant being 0. In these cases the covariance is no longer a definite covariance matrix. A determinant with a value of 0 means intuitively, that at least one of the Gaussian distributions has an undefined probability. This undefinable probability occurs, because the variance parameter of a regular Gaussian appears in the denominator of the whole expression. By dividing by 0 and subsequently computing the joint probability we receive an undefined joint probability. Therefore, the covariance often needs to be definite. This issue is the reason, why  we often employ numerical solutions like \emph{Singular-Value-Composition} to get an approximation.

% However, if the covariance matrix has a higher rank than the number of data points used to estimate if, the issue remains. Meaning, if we have a 20x20 covariance matrix and just 3 datapoints to estimate it, we will likely face numerical issues of computing the determinant again.

% To mitigate these issues, we fallback to methods like adding a small constant to the diagonal. If this approach does not work either, we add a small constant to the full matrix. Alternatively, we can compute the Gaussian in a Bayesian way by adding a prior. However, using a fallbacks was deemed as the simpler solution.






\end{document}