\documentclass[./../../paper.tex]{subfiles}
\graphicspath{{\subfix{./../../figures/}}}

\begin{document}

\section{Determine the best Generator Algorithm}

\subsection{Experimental Setup}
\label{sec:exp4}
Knowing the , we compare the evolutionary algorithm with other algorithms. 

In this comparison, we employ the other models mentioned in \autoref{sec:models}. Namely, the \emph{Case-Based Generator} and the \emph{Random Generator}. 

For the evolutionary algorithm, we choose the model-configuration from \autoref{sec:exp1}, the rate-configuration determined in \autoref{sec:exp2} and the termination point from \autoref{sec:exp3}. Furthermore, we randomly sample \attention{20} factuals from the test set and use the same factuals for every generator. We ensure, that the outcomes are evenly divided. The remaining procedure follows the established procedure of previous experiments.

\subsection{Results}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/generated/exp4_winner_overall.png}
    \caption{This figure shows boxplots of the viability of each models' generated counterfactual.}
    \label{fig:exp4-winner}
\end{figure}

\noindent the results shown in \autoref{fig:exp4-winner} show that the evolutionary algorithm \attention{model-specifier} returns better results on average. The worst model is the random generated model.  

\autoref{tbl:exp4-winner} shows the detailed results.

\input{./../../tables/generated/exp4_winner_overall.tex}


\subsection{Discussion}
These results show that the model \attention{SBI-ES-OPC-FSR} is clearly superior to the other models. This result is unsurprising, as the baselines do not actively search for an optimal solution. However, knowing these results, a couple of questions remain. Namely, whether the results remain consistent for longer sequences and for other datasets? \optional{Furthermore, how does this procedure compare to other methods in the literature?} The remaining experiments will address these questions. 


\end{document}