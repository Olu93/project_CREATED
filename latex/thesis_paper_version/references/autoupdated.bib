
@misc{_10PythonLibrary_,
  title = {10 {{Python}} Library for Evolutionary and Genetic Algorithm | {{Data Science}} and {{Machine Learning}}},
  abstract = {10 Python library for evolutionary and genetic algorithm.},
  howpublished = {https://www.kaggle.com/getting-started/a},
  langid = {english},
  file = {/home/bestname/Zotero/storage/MB8RDTWK/112297.html}
}

@misc{_AllenaiAllennlpmodels_2021,
  title = {Allenai/Allennlp-Models},
  year = {2021},
  month = oct,
  abstract = {Officially supported AllenNLP models},
  copyright = {Apache-2.0},
  howpublished = {AI2}
}

@misc{_AllenNLP_,
  title = {{{AllenNLP}}},
  abstract = {AllenNLP is a free, open-source natural language processing platform for building state of the art models.},
  howpublished = {https://allennlp.org/\%PUBLIC\_URL\%},
  langid = {american},
  file = {/home/bestname/Zotero/storage/SIDXVJZR/allennlp.org.html}
}

@misc{_AllennlpmodelsAllennlpModels_,
  title = {Allennlp-Models/Allennlp\_models/Lm/Dataset\_readers at Main {$\cdot$} Allenai/Allennlp-Models},
  journal = {GitHub},
  abstract = {Officially supported AllenNLP models. Contribute to allenai/allennlp-models development by creating an account on GitHub.},
  howpublished = {https://github.com/allenai/allennlp-models},
  langid = {english},
  file = {/home/bestname/Zotero/storage/KGPX58WC/dataset_readers.html}
}

@misc{_AutoencoderBetaVAE_2018,
  title = {From {{Autoencoder}} to {{Beta-VAE}}},
  year = {2018},
  month = aug,
  journal = {Lil'Log},
  abstract = {Autocoders are a family of neural network models aiming to learn compressed latent variables of high-dimensional data. Starting from the basic autocoder model, this post reviews several variations, including denoising, sparse, and contractive autoencoders, and then Variational Autoencoder (VAE) and its modification beta-VAE.},
  howpublished = {https://lilianweng.github.io/2018/08/12/from-autoencoder-to-beta-vae.html},
  langid = {english},
  file = {/home/bestname/Zotero/storage/QU2EY475/from-autoencoder-to-beta-vae.html}
}

@misc{_CategoricalVariationalAutoEncoders_2018,
  title = {Categorical {{Variational Auto-Encoders}} and the {{Gumbel Trick}} \textbullet{} {{David Stutz}}},
  year = {2018},
  month = aug,
  journal = {David Stutz},
  abstract = {This article introduces categorical variational auto-encoders which allow to learn a latent space of discrete variables through the Gumbel reparameterization trick.},
  langid = {american},
  file = {/home/bestname/Zotero/storage/NQTQEDQU/categorical-variational-auto-encoders-and-the-gumbel-trick.html}
}

@misc{_CelonisAcademicAlliance_,
  title = {Celonis {{Academic Alliance}} - {{Process Mining Experts Program}}},
  journal = {Celonis},
  abstract = {Read how students use Process Mining to learn and research.},
  howpublished = {https://www.celonis.com/acal-thesis-support/},
  langid = {english},
  file = {/home/bestname/Zotero/storage/N8MGID9R/acal-thesis-support.html}
}

@misc{_ConvolutionsAutoregressiveNeural_,
  title = {Convolutions in {{Autoregressive Neural Networks}}},
  journal = {The Blog},
  abstract = {This post explains how to use one-dimensional causal and dilated convolutions in autoregressive neural networks such as WaveNet.},
  howpublished = {https://theblog.github.io},
  langid = {english},
  file = {/home/bestname/Zotero/storage/9Z9QBB87/convolution-in-autoregressive-neural-networks.html}
}

@misc{_Counterfactual_,
  title = {Counterfactual},
  journal = {Oxford Reference},
  doi = {10.1093/oi/authority.20110803095642948},
  abstract = {"counterfactual" published on  by null.},
  howpublished = {https://www-oxfordreference-com.proxy.library.uu.nl/view/10.1093/oi/authority.20110803095642948},
  langid = {english},
  file = {/home/bestname/Zotero/storage/ZRB6ALUS/authority.html}
}

@misc{_CounterfactualAdjectiveDefinition_,
  title = {Counterfactual Adjective - {{Definition}}, Pictures, Pronunciation and Usage Notes | {{Oxford Advanced American Dictionary}} at {{OxfordLearnersDictionaries}}.Com},
  howpublished = {https://www.oxfordlearnersdictionaries.com/definition/american\_english/counterfactual},
  file = {/home/bestname/Zotero/storage/G8PXBWPY/counterfactual.html}
}

@misc{_DEAP_2022,
  title = {{{DEAP}}},
  year = {2022},
  month = feb,
  abstract = {Distributed Evolutionary Algorithms in Python},
  copyright = {LGPL-3.0},
  howpublished = {Distributed Evolutionary Algorithms in Python}
}

@misc{_DefinitionPROCESS_,
  title = {Definition of {{PROCESS}}},
  abstract = {progress, advance; something going on : proceeding; a natural phenomenon marked by gradual changes that lead toward a particular result\ldots{} See the full definition},
  howpublished = {https://www.merriam-webster.com/dictionary/process},
  langid = {english},
  file = {/home/bestname/Zotero/storage/R3HXYZ8T/process.html}
}

@misc{_GeneticAlgorithmsEvolutionary_2011,
  title = {Genetic {{Algorithms}} and {{Evolutionary Algorithms}} - {{Introduction}}},
  year = {2011},
  month = jan,
  journal = {solver},
  abstract = {Welcome to our tutorial on genetic and evolutionary algorithms -- from Frontline Systems, developers of the Solver in Microsoft Excel. You can use genetic algorithms in Excel to solve optimization problems, using our advanced Evolutionary Solver, by downloading a free trial version of our Premium Solver Platform.},
  howpublished = {https://www.solver.com/genetic-evolutionary-introduction},
  langid = {english},
  file = {/home/bestname/Zotero/storage/IAAWL933/genetic-evolutionary-introduction.html}
}

@article{_GrangerCausality_2022,
  title = {Granger Causality},
  year = {2022},
  month = jan,
  journal = {Wikipedia},
  abstract = {The Granger causality test is a statistical hypothesis test for determining whether one time series is useful in forecasting another, first proposed in 1969. Ordinarily, regressions reflect "mere" correlations, but Clive Granger argued that causality in economics could be tested for by measuring the ability to predict the future values of a time series using prior values of another time series. Since the question of "true causality" is deeply philosophical, and because of the post hoc ergo propter hoc fallacy of assuming that one thing preceding another can be used as a proof of causation, econometricians assert that the Granger test finds only "predictive causality". Using the term "causality" alone is a misnomer, as Granger-causality is better described as  "precedence", or, as Granger himself later claimed in 1977, "temporally related". Rather than testing whether X causes Y, the Granger causality tests whether X forecasts Y.A time series X is said to Granger-cause Y if it can be shown, usually through a series of t-tests and F-tests on lagged values of X (and with lagged values of Y also included), that those X values provide statistically significant information about future values of Y. Granger also stressed that some studies using "Granger causality" testing in areas outside economics reached "ridiculous" conclusions. "Of course, many ridiculous papers appeared", he said in his Nobel lecture. However, it remains a popular method for causality analysis in time series due to its computational simplicity. The original definition of Granger causality does not account for latent confounding effects and does not capture instantaneous and non-linear causal relationships, though several extensions have been proposed to address these issues.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1064850191},
  file = {/home/bestname/Zotero/storage/P9BV74PH/Granger_causality.html}
}

@article{_IEEEStandardEXtensible_2016,
  title = {{{IEEE Standard}} for {{eXtensible Event Stream}} ({{XES}}) for {{Achieving Interoperability}} in {{Event Logs}} and {{Event Streams}}},
  year = {2016},
  month = nov,
  journal = {IEEE Std 1849-2016},
  pages = {1--50},
  doi = {10.1109/IEEESTD.2016.7740858},
  abstract = {A grammar for a tag-based language whose aim is to provide designers of information systems with a unified and extensible methodology for capturing systems behaviors by means of event logs and event streams is defined in the XES standard. An XML Schema describing the structure of an XES event log/stream and a XML Schema describing the structure of an extension of such a log/stream are included in this standard. Moreover, a basic collection of so-called XES extension prototypes that provide semantics to certain attributes as recorded in the event log/stream is included in this standard.},
  keywords = {Behavioral sciences,event log,Event recognition,event stream,extensions,Grammar,IEEE 1849(TM),IEEE Standards,Information systems,Semantics,system behavior,XML},
  file = {/home/bestname/Zotero/storage/W45TP9JI/7740858.html}
}

@misc{_ImplementingLevenshteinDistance_2020,
  title = {Implementing {{The Levenshtein Distance}} in {{Python}}},
  year = {2020},
  month = mar,
  journal = {Paperspace Blog},
  abstract = {This tutorial works through a step-by-step example of how to implement the Levenshtein distance in Python for word autocorrection and autocompletion.},
  howpublished = {https://blog.paperspace.com/implementing-levenshtein-distance-word-autocomplete-autocorrect/},
  langid = {english},
  file = {/home/bestname/Zotero/storage/ZAI2RZDQ/implementing-levenshtein-distance-word-autocomplete-autocorrect.html}
}

@book{_IntelligentControlSystems_,
  title = {Intelligent {{Control Systems}}},
  langid = {english},
  file = {/home/bestname/Zotero/storage/USHYQEYW/b101833.html}
}

@misc{_ModelZooVRNN_,
  title = {Model {{Zoo}} - {{VRNN PyTorch Model}}},
  howpublished = {https://modelzoo.co/model/vrnn},
  file = {/home/bestname/Zotero/storage/3A3WMBP2/vrnn.html}
}

@misc{_MultivariateTimeSeries_,
  title = {6.4.5. {{Multivariate Time Series Models}}},
  howpublished = {https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc45.htm},
  file = {/home/bestname/Zotero/storage/Y73DAWBE/pmc45.html}
}

@misc{_MultivariateTimeSeries_2018,
  title = {Multivariate {{Time Series}} | {{Vector Auto Regression}} ({{VAR}})},
  year = {2018},
  month = sep,
  journal = {Analytics Vidhya},
  abstract = {Vector Auto Regression method for forecasting multivariate time series uses vectors to represent the relationship between variables and past values.},
  langid = {english},
  file = {/home/bestname/Zotero/storage/EKRS97CW/multivariate-time-series-guide-forecasting-modeling-python-codes.html}
}

@misc{_MustreadPapersTextual_2021,
  title = {Must-Read {{Papers}} on {{Textual Adversarial Attack}} and {{Defense}} ({{TAAD}})},
  year = {2021},
  month = sep,
  abstract = {Must-read Papers on Textual Adversarial Attack and Defense},
  howpublished = {THUNLP},
  keywords = {adversarial-attacks,adversarial-defense,adversarial-learning,natural-language-processing,nlp,paper-list}
}

@misc{_NextTokenLm_,
  title = {Next\_token\_lm - {{AllenNLP Models}} v2.7.0},
  howpublished = {https://docs.allennlp.org/models/main/models/lm/models/next\_token\_lm/},
  file = {/home/bestname/Zotero/storage/LEBQZ5BB/next_token_lm.html}
}

@misc{_PapersCodeBPI_,
  title = {Papers with {{Code}} - {{BPI}} Challenge '12 {{Benchmark}} ({{Multivariate Time Series Forecasting}})},
  abstract = {The current state-of-the-art on BPI challenge '12 is QuerySelector. See a full comparison of 2 papers with code.},
  howpublished = {https://paperswithcode.com/sota/multivariate-time-series-forecasting-on-bpi},
  langid = {english},
  file = {/home/bestname/Zotero/storage/CG7IAFHF/multivariate-time-series-forecasting-on-bpi.html}
}

@misc{_ProbabilityVariationalAutoencoder_,
  title = {Probability - {{Variational}} Autoencoder: {{Why}} Reconstruction Term Is Same to Square Loss?},
  shorttitle = {Probability - {{Variational}} Autoencoder},
  journal = {Cross Validated},
  howpublished = {https://stats.stackexchange.com/questions/347378/variational-autoencoder-why-reconstruction-term-is-same-to-square-loss},
  file = {/home/bestname/Zotero/storage/XXNMP9VU/variational-autoencoder-why-reconstruction-term-is-same-to-square-loss.html}
}

@misc{_PyTorchTemporalConvolutional_,
  title = {({{PyTorch}}) {{Temporal Convolutional Networks}}},
  abstract = {Explore and run machine learning code with Kaggle Notebooks | Using data from Don't call me turkey!},
  howpublished = {https://kaggle.com/ceshine/pytorch-temporal-convolutional-networks},
  langid = {english},
  keywords = {code},
  file = {/home/bestname/Zotero/storage/GF6U6KH2/pytorch-temporal-convolutional-networks.html}
}

@misc{_RepresentingTextFeatures_,
  title = {Representing Text as Features: {{Tokenizers}}, {{TextFields}}, and {{TextFieldEmbedders}} {$\cdot$} {{A Guide}} to {{Natural Language Processing With AllenNLP}}},
  shorttitle = {Representing Text as Features},
  journal = {A Guide to Natural Language Processing With AllenNLP},
  abstract = {A deep dive into AllenNLP's core abstraction: how exactly we represent textual inputs, both on the data side and the model side.},
  howpublished = {https://guide.allennlp.org/representing-text-as-features},
  langid = {english},
  file = {/home/bestname/Zotero/storage/RGPJKYZ3/representing-text-as-features.html}
}

@misc{_SequenceModelingBenchmarks_2021,
  title = {Sequence {{Modeling Benchmarks}} and {{Temporal Convolutional Networks}} ({{TCN}})},
  year = {2021},
  month = oct,
  abstract = {Sequence modeling benchmarks and temporal convolutional networks},
  copyright = {MIT},
  howpublished = {CMU Locus Lab},
  keywords = {code}
}

@misc{_StanfordCS236Deep_,
  title = {Stanford {{CS236}} - {{Deep Generative Modelling}}},
  howpublished = {https://deepgenerativemodels.github.io/notes/},
  file = {/home/bestname/Zotero/storage/KKQLI8NU/notes.html}
}

@article{_StatespaceRepresentation_2021,
  title = {State-Space Representation},
  year = {2021},
  month = nov,
  journal = {Wikipedia},
  abstract = {In control engineering, a state-space representation is a mathematical model of a physical system as a set of input, output and state variables related by first-order differential equations or difference equations. State variables are variables whose values evolve over time in a way that depends on the values they have at any given time and on the externally imposed values of input variables. Output variables' values depend on the values of the state variables. The "state space" is the Euclidean space in which the variables on the axes are the state variables. The state of the system can be represented as a state vector within that space. To abstract from the number of inputs, outputs and states, these variables are expressed as vectors.  If the dynamical system is linear, time-invariant, and finite-dimensional, then the differential and algebraic equations may be written in matrix form. The state-space method is characterized by significant algebraization of general system theory, which makes it possible to use Kronecker vector-matrix structures. The capacity of these structures can be efficiently applied to research systems with modulation or without it.  The state-space representation (also known as the "time-domain approach") provides a convenient and compact way to model and analyze systems with multiple inputs and outputs. With                         p                 \{\textbackslash displaystyle p\}    inputs and                         q                 \{\textbackslash displaystyle q\}    outputs, we would otherwise have to write down                         q         \texttimes{}         p                 \{\textbackslash displaystyle q\textbackslash times p\}    Laplace transforms to encode all the information about a system. Unlike the frequency domain approach, the use of the state-space representation is not limited to systems with linear components and zero initial conditions. The state-space model can be applied in subjects such as economics, statistics, computer science and electrical engineering, and neuroscience. In econometrics, for example, state-space models can be used to decompose a time series into trend and cycle, compose individual indicators into a composite index, identify turning points of the business cycle, and estimate GDP using latent and unobserved time series. Many applications rely on the Kalman Filter to produce estimates of the current unknown state variables using their previous observations.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1053783244},
  file = {/home/bestname/Zotero/storage/3U3BYU3C/State-space_representation.html}
}

@misc{_StructuredInf_2022,
  title = {{{structuredInf}}},
  year = {2022},
  month = jan,
  abstract = {Structured Inference Networks for Nonlinear State Space Models},
  copyright = {MIT},
  howpublished = {Sontag Lab}
}

@misc{_TCNTemporalConvolutional_,
  title = {{{TCN}} ({{Temporal Convolutional Network}}) | Tsai},
  howpublished = {https://timeseriesai.github.io/tsai/models.TCN.html},
  keywords = {code},
  file = {/home/bestname/Zotero/storage/3YQ7MKST/models.TCN.html}
}

@misc{_TutorialVariationalAutoencoders_,
  title = {Tutorial \#5: Variational Autoencoders},
  howpublished = {https://www.borealisai.com/en/blog/tutorial-5-variational-auto-encoders/},
  file = {/home/bestname/Zotero/storage/DISKEMHY/tutorial-5-variational-auto-encoders.html}
}

@misc{_WhatDifferenceGenetic_,
  title = {What Is the Difference between Genetic and Evolutionary Algorithms?},
  journal = {Stack Overflow},
  howpublished = {https://stackoverflow.com/questions/2890061/what-is-the-difference-between-genetic-and-evolutionary-algorithms},
  file = {/home/bestname/Zotero/storage/KUZ2YJIU/what-is-the-difference-between-genetic-and-evolutionary-algorithms.html}
}

@misc{_WhatElitismIGI_,
  title = {What Is {{Elitism}} | {{IGI Global}}},
  howpublished = {https://www-igi-global-com.proxy.library.uu.nl/dictionary/multi-objective-evolutionary-algorithms/9592},
  file = {/home/bestname/Zotero/storage/N2FNNT99/9592.html}
}

@article{2020NumPy-Array,
  title = {Array Programming with {{NumPy}}},
  author = {Harris, Charles R. and Millman, K. Jarrod and {van der Walt}, St{\'e}fan J and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J. and Kern, Robert and Picus, Matti and Hoyer, Stephan and {van Kerkwijk}, Marten H. and Brett, Matthew and Haldane, Allan and {Fern{\'a}ndez del R{\'i}o}, Jaime and Wiebe, Mark and Peterson, Pearu and {G{\'e}rard-Marchant}, Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant, Travis E.},
  year = {2020},
  journal = {Nature},
  volume = {585},
  pages = {357--362},
  doi = {10.1038/s41586-020-2649-2}
}

@inproceedings{abadi2016tensorflow,
  title = {Tensorflow: {{A}} System for Large-Scale Machine Learning},
  booktitle = {12th \{\vphantom\}{{USENIX}}\vphantom\{\} Symposium on Operating Systems Design and Implementation (\{\vphantom\}{{OSDI}}\vphantom\{\} 16)},
  author = {Abadi, Mart{\'i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  year = {2016},
  pages = {265--283}
}

@article{agrawal_StudyPhenomenonMoravec_2010,
  title = {To Study the Phenomenon of the {{Moravec}}'s {{Paradox}}},
  author = {Agrawal, Kush},
  year = {2010},
  journal = {ArXiv},
  abstract = {Hans Moravec's paradox is involved with the fact that it is the seemingly easier day to day problems that are harder to implement in a machine, than the seemingly complicated logic based problems of today. "Encoded in the large, highly evolved sensory and motor portions of the human brain is a billion years of experience about the nature of the world and how to survive in it. The deliberate process we call reasoning is, I believe, the thinnest veneer of human thought, effective only because it is supported by this much older and much powerful, though usually unconscious, sensor motor knowledge. We are all prodigious Olympians in perceptual and motor areas, so good that we make the difficult look easy. Abstract thought, though, is a new trick, perhaps less than 100 thousand years old. We have not yet mastered it. It is not all that intrinsically difficult; it just seems so when we do it."- Hans Moravec Moravec's paradox is involved with the fact that it is the seemingly easier day to day problems that are harder to implement in a machine, than the seemingly complicated logic based problems of today. The results prove that most artificially intelligent machines are as adept if not more than us at under-taking long calculations or even play chess, but their logic brings them nowhere when it comes to carrying out everyday tasks like walking, facial gesture recognition or speech recognition.},
  file = {/home/bestname/Zotero/storage/5BJUR5L3/Agrawal_2010_To study the phenomenon of the Moravec's Paradox.pdf}
}

@misc{aip-pursuingsotaaiforeveryone_NeurIPS2020Explanation_,
  title = {{{NeurIPS}} 2020 | {{An Explanation}} to {{What}} Is {{Counterfactuals}} in {{Interpretable AI}}? ({{Tutorial}})},
  shorttitle = {{{NeurIPS}} 2020 | {{An Explanation}} to {{What}} Is {{Counterfactuals}} in {{Interpretable AI}}?},
  author = {{AIP - Pursuing SoTA AI for everyone}},
  abstract = {Join the channel membership: https://www.youtube.com/c/AIPursuit/join Subscribe to the channel: https://www.youtube.com/c/AIPursuit?s... Support and Donation: Paypal {$\dashrightarrow$} https://paypal.me/tayhengee Patreon {$\dashrightarrow$} https://www.patreon.com/hengee BTC {$\dashrightarrow$} bc1q2r7eymlf20576alvcmryn28tgrvxqw5r30cmpu ETH {$\dashrightarrow$} 0x58c4bD4244686F3b4e636EfeBD159258A5513744 Doge {$\dashrightarrow$} DSGNbzuS1s6x81ZSbSHHV5uGDxJXePeyKy Wanted to own BTC, ETH, or even Dogecoin? Kickstart your crypto portfolio with the largest crypto market Binance with my affiliate link: https://accounts.binance.com/en/regis...  The video is reposted for educational purposes and encourages involvement in the field of research.},
  annotation = {ZSCC: NoCitationData[s0]}
}

@article{anastasiou_CausalityDistanceMeasures_2021,
  title = {Causality {{Distance Measures}} for {{Multivariate Time Series}} with {{Applications}}},
  author = {Anastasiou, Achilleas and Hatzopoulos, Peter and Karagrigoriou, Alex and Mavridoglou, George},
  year = {2021},
  month = oct,
  journal = {Mathematics},
  volume = {9},
  number = {21},
  pages = {2708},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2227-7390},
  doi = {10.3390/math9212708},
  abstract = {In this work, we focus on the development of new distance measure algorithms, namely, the Causality Within Groups (CAWG), the Generalized Causality Within Groups (GCAWG) and the Causality Between Groups (CABG), all of which are based on the well-known Granger causality. The proposed distances together with the associated algorithms are suitable for multivariate statistical data analysis including unsupervised classification (clustering) purposes for the analysis of multivariate time series data with emphasis on financial and economic data where causal relationships are frequently present. For exploring the appropriateness of the proposed methodology, we implement, for illustrative purposes, the proposed algorithms to hierarchical clustering for the classification of 19 EU countries based on seven variables related to health resources in healthcare systems.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {classification,clustering,distance,divergence,Granger causality,healthcare systems,hot,multivariate time series,pattern recognition,skimmed},
  annotation = {ZSCC: 0000000},
  file = {/home/bestname/Zotero/storage/WVVTRN35/Anastasiou et al_2021_Causality Distance Measures for Multivariate Time Series with Applications.pdf;/home/bestname/Zotero/storage/4F8FW5VE/2708.html}
}

@article{anderson_SpeciesProblemIris_1936,
  title = {The {{Species Problem}} in {{Iris}}},
  author = {Anderson, Edgar},
  year = {1936},
  journal = {Annals of the Missouri Botanical Garden},
  volume = {23},
  number = {3},
  pages = {457--509},
  publisher = {{Missouri Botanical Garden Press}},
  issn = {0026-6493},
  doi = {10.2307/2394164},
  file = {/home/bestname/Zotero/storage/Z22ZHEJT/Anderson_1936_The Species Problem in Iris.pdf}
}

@article{apostolico_SequenceAlignmentMolecular_1998,
  ids = {Apostolico1998173},
  title = {Sequence {{Alignment}} in {{Molecular Biology}}},
  author = {Apostolico, Alberto and Giancarlo, Raffaele},
  year = {1998},
  month = jan,
  journal = {Journal of Computational Biology},
  volume = {5},
  number = {2},
  pages = {173--196},
  publisher = {{Mary Ann Liebert, Inc., publishers}},
  doi = {10.1089/cmb.1998.5.173},
  abstract = {Molecular biology is becoming a computationally intense realm of contemporary science and faces some of the current grand scientific challenges. In its context, tools that identify, store, compare and analyze effectively large and growing numbers of bio-sequences are found of increasingly crucial importance. Biosequences are routinely compared or aligned, in a variety of ways, to infer common ancestry, to detect functional equivalence, or simply while searching for similar entries in a database. A considerable body of knowledge has accumulated on sequence alignment during the past few decades. Without pretending to be exhaustive, this paper attempts a survey of some criteria of wide use in sequence alignment and comparison problems, and of the corresponding solutions. The paper is based on presentations and literature given at the Workshop on Sequence Alignment held at Princeton, N.J., in November 1994, as part of the DIMACS Special Year on Mathematical Support for Molecular Biology.},
  document_type = {Review},
  source = {Scopus},
  file = {/home/bestname/Zotero/storage/8VIXYT22/Apostolico_Giancarlo_1998_Sequence Alignment in Molecular Biology.pdf}
}

@article{araujo_HowEvolutionaryAlgorithms_2007,
  title = {How Evolutionary Algorithms Are Applied to Statistical Natural Language Processing},
  author = {Araujo, Lourdes},
  year = {2007},
  month = dec,
  journal = {Artificial Intelligence Review},
  volume = {28},
  number = {4},
  pages = {275--303},
  issn = {0269-2821},
  doi = {10.1007/s10462-009-9104-y},
  abstract = {Statistical natural language processing (NLP) and evolutionary algorithms (EAs) are two very active areas of research which have been combined many times. In general, statistical models applied to deal with NLP tasks require designing specific algorithms to be trained and applied to process new texts. The development of such algorithms may be hard. This makes EAs attractive since they offer a general design, yet providing a high performance in particular conditions of application. In this article, we present a survey of many works which apply EAs to different NLP problems, including syntactic and semantic analysis, grammar induction, summaries and text generation, document clustering and machine translation. This review finishes extracting conclusions about which are the best suited problems or particular aspects within those problems to be solved with an evolutionary algorithm.},
  keywords = {document classification,document clustering,Evolutionary algorithms,grammar induction,hot,language generation,machine translation,review,semantic analysis,skimmed,Statistical natural language processing,syntax analysis,text summarisation},
  file = {/home/bestname/Zotero/storage/TS7V8Y26/Araujo_2007_How evolutionary algorithms are applied to statistical natural language.pdf}
}

@inproceedings{ates_CounterfactualExplanationsMultivariate_2021,
  title = {Counterfactual {{Explanations}} for {{Multivariate Time Series}}},
  booktitle = {2021 {{International Conference}} on {{Applied Artificial Intelligence}} ({{ICAPAI}})},
  author = {Ates, Emre and Aksar, Burak and Leung, Vitus J. and Coskun, Ayse K.},
  year = {2021},
  month = may,
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Halden, Norway}},
  doi = {10.1109/ICAPAI49758.2021.9462056},
  abstract = {Multivariate time series are used in many science and engineering domains, including health-care, astronomy, and high-performance computing. A recent trend is to use machine learning (ML) to process this complex data and these ML-based frameworks are starting to play a critical role for a variety of applications. However, barriers such as user distrust or difficulty of debugging need to be overcome to enable widespread adoption of such frameworks in production systems. To address this challenge, we propose a novel explainability technique, CoMTE, that provides counterfactual explanations for supervised machine learning frameworks on multivariate time series data. Using various machine learning frameworks and data sets, we compare CoMTE with several state-of-the-art explainability methods and show that we outperform existing methods in comprehensibility and robustness. We also show how CoMTE can be used to debug machine learning frameworks and gain a better understanding of the underlying multivariate time series data.},
  isbn = {978-1-72815-934-8},
  langid = {english},
  annotation = {ZSCC: 0000011},
  file = {/home/bestname/Zotero/storage/M4IJ8LNR/Ates et al. - 2021 - Counterfactual Explanations for Multivariate Time .pdf}
}

@inproceedings{auer_DBpediaNucleusWeb_2007,
  title = {{{DBpedia}}: A Nucleus for a Web of Open Data},
  shorttitle = {{{DBpedia}}},
  booktitle = {Proceedings of the 6th International {{The}} Semantic Web and 2nd {{Asian}} Conference on {{Asian}} Semantic Web Conference},
  author = {Auer, S{\"o}ren and Bizer, Christian and Kobilarov, Georgi and Lehmann, Jens and Cyganiak, Richard and Ives, Zachary},
  year = {2007},
  month = nov,
  series = {{{ISWC}}'07/{{ASWC}}'07},
  pages = {722--735},
  publisher = {{Springer-Verlag}},
  address = {{Berlin, Heidelberg}},
  abstract = {DBpedia is a community effort to extract structured information from Wikipedia and to make this information available on the Web. DBpedia allows you to ask sophisticated queries against datasets derived from Wikipedia and to link other datasets on the Web to Wikipedia data. We describe the extraction of the DBpedia datasets, and how the resulting information is published on the Web for human-andmachine-consumption. We describe some emerging applications from the DBpedia community and show how website authors can facilitate DBpedia content within their sites. Finally, we present the current status of interlinking DBpedia with other open datasets on the Web and outline how DBpedia could serve as a nucleus for an emerging Web of open data.},
  isbn = {978-3-540-76297-3}
}

@article{augusto_AutomatedDiscoveryProcess_2019,
  title = {Automated {{Discovery}} of {{Process Models}} from {{Event Logs}}: {{Review}} and {{Benchmark}}},
  shorttitle = {Automated {{Discovery}} of {{Process Models}} from {{Event Logs}}},
  author = {Augusto, Adriano and Conforti, Raffaele and Dumas, Marlon and Rosa, Marcello La and Maggi, Fabrizio Maria and Marrella, Andrea and Mecella, Massimo and Soo, Allar},
  year = {2019},
  month = apr,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {31},
  number = {4},
  pages = {686--705},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2018.2841877},
  abstract = {Process mining allows analysts to exploit logs of historical executions of business processes to extract insights regarding the actual performance of these processes. One of the most widely studied process mining operations is automated process discovery. An automated process discovery method takes as input an event log, and produces as output a business process model that captures the control-flow relations between tasks that are observed in or implied by the event log. Various automated process discovery methods have been proposed in the past two decades, striking different tradeoffs between scalability, accuracy, and complexity of the resulting models. However, these methods have been evaluated in an ad-hoc manner, employing different datasets, experimental setups, evaluation measures, and baselines, often leading to incomparable conclusions and sometimes unreproducible results due to the use of closed datasets. This article provides a systematic review and comparative evaluation of automated process discovery methods, using an open-source benchmark and covering 12 publicly-available real-life event logs, 12 proprietary real-life event logs, and nine quality metrics. The results highlight gaps and unexplored tradeoffs in the field, including the lack of scalability of some methods and a strong divergence in their performance with respect to the different quality metrics used.},
  keywords = {automated process discovery,benchmark,Benchmark testing,Data mining,Data models,done,hot,Process control,Process mining,survey,Systematics,Task analysis},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/ISDGHY99/Augusto et al_2019_Automated Discovery of Process Models from Event Logs.pdf;/home/bestname/Zotero/storage/EUJJ46HH/8368306.html}
}

@article{augusto_AutomatedDiscoveryProcess_2019a,
  title = {Automated {{Discovery}} of {{Process Models}} from {{Event Logs}}: {{Review}} and {{Benchmark}}},
  shorttitle = {Automated {{Discovery}} of {{Process Models}} from {{Event Logs}}},
  author = {Augusto, Adriano and Conforti, Raffaele and Dumas, Marlon and Rosa, Marcello La and Maggi, Fabrizio Maria and Marrella, Andrea and Mecella, Massimo and Soo, Allar},
  year = {2019},
  month = apr,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {31},
  number = {4},
  pages = {686--705},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2018.2841877},
  abstract = {Process mining allows analysts to exploit logs of historical executions of business processes to extract insights regarding the actual performance of these processes. One of the most widely studied process mining operations is automated process discovery. An automated process discovery method takes as input an event log, and produces as output a business process model that captures the control-flow relations between tasks that are observed in or implied by the event log. Various automated process discovery methods have been proposed in the past two decades, striking different tradeoffs between scalability, accuracy, and complexity of the resulting models. However, these methods have been evaluated in an ad-hoc manner, employing different datasets, experimental setups, evaluation measures, and baselines, often leading to incomparable conclusions and sometimes unreproducible results due to the use of closed datasets. This article provides a systematic review and comparative evaluation of automated process discovery methods, using an open-source benchmark and covering 12 publicly-available real-life event logs, 12 proprietary real-life event logs, and nine quality metrics. The results highlight gaps and unexplored tradeoffs in the field, including the lack of scalability of some methods and a strong divergence in their performance with respect to the different quality metrics used.},
  keywords = {automated process discovery,benchmark,Benchmark testing,Data mining,Data models,Process control,Process mining,survey,Systematics,Task analysis},
  file = {/home/bestname/Zotero/storage/RANZ27ZU/08368306.pdf;/home/bestname/Zotero/storage/WLT84M6A/Augusto et al_2019_Automated Discovery of Process Models from Event Logs.pdf;/home/bestname/Zotero/storage/FMGW9C95/8368306.html}
}

@article{back_OverviewEvolutionaryAlgorithms_1993,
  title = {An {{Overview}} of {{Evolutionary Algorithms}} for {{Parameter Optimization}}},
  author = {B{\"a}ck, Thomas and Schwefel, Hans-Paul},
  year = {1993},
  month = mar,
  journal = {Evolutionary Computation},
  volume = {1},
  number = {1},
  pages = {1--23},
  issn = {1063-6560},
  doi = {10.1162/evco.1993.1.1.1},
  abstract = {Three main streams of evolutionary algorithms (EAs), probabilistic optimization algorithms based on the model of natural evolution, are compared in this article: evolution strategies (ESs), evolutionary programming (EP), and genetic algorithms (GAs). The comparison is performed with respect to certain characteristic components of EAs: the representation scheme of object variables, mutation, recombination, and the selection operator. Furthermore, each algorithm is formulated in a high-level notation as an instance of the general, unifying basic algorithm, and the fundamental theoretical results on the algorithms are presented. Finally, after presenting experimental results for three test functions representing a unimodal and a multimodal case as well as a step function with discontinuities, similarities and differences of the algorithms are elaborated, and some hints to open research questions are sketched.},
  keywords = {irrelevant},
  file = {/home/bestname/Zotero/storage/U3ZZ9T9Z/BÃ¤ck_Schwefel_1993_An Overview of Evolutionary Algorithms for Parameter Optimization.pdf;/home/bestname/Zotero/storage/73D3FFP6/An-Overview-of-Evolutionary-Algorithms-for.html}
}

@article{baker_ClosingLoopEmpirical_2017,
  title = {Closing the {{Loop}}: {{An Empirical Investigation}} of {{Causality}} in {{IT Business Value}}},
  shorttitle = {Closing the {{Loop}}},
  author = {Baker, J. and Song, Jaeki and Jones, Donald R.},
  year = {2017},
  journal = {undefined},
  abstract = {Researchers have established that information technology (IT) can improve firms' productivity. Whether improved productivity leads to additional investment in IT, however, remains largely uninvestigated. In this paper, we consider whether the relationship between productivity and subsequent IT investment might be positive, negative, or ad hoc, and hypothesize that this relationship is positive. We analyze seven years of panel data from 1,223 healthcare firms and present empirical evidence supporting our hypothesis. When our finding is combined with extant research, it becomes reasonable to propose that unidirectional causality does not fully describe the process of IT business value creation. Instead, we argue that existing static models of IT business value with unidirectional causality can be recast as dynamic models that explicitly incorporate multiple time periods and a positive feedback relationship to more accurately capture the complexity of this process. The creation of IT business value can thus be understood as a positive feedback model where productivity in a given time period leads to IT investment in a future time period, where IT investment builds the stock of IT inputs, and where those IT inputs then impact productivity, beginning the cycle anew.},
  langid = {english},
  annotation = {ZSCC: 0000000},
  file = {/home/bestname/Zotero/storage/5IR5FJ47/Baker et al_2017_Closing the Loop.pdf;/home/bestname/Zotero/storage/C32WXTBT/df210060211bdc598f2d3382c68c615319287f71.html}
}

@article{bao_NovelNondominatedSorting_2017,
  title = {A Novel Non-Dominated Sorting Algorithm for Evolutionary Multi-Objective Optimization},
  author = {Bao, Chunteng and Xu, Lihong and Goodman, Erik D. and Cao, Leilei},
  year = {2017},
  month = nov,
  journal = {Journal of Computational Science},
  volume = {23},
  pages = {31--43},
  issn = {1877-7503},
  doi = {10.1016/j.jocs.2017.09.015},
  abstract = {Evolutionary computation has shown great performance in solving many multi-objective optimization problems; in many such algorithms, non-dominated sorting plays an important role in determining the relative quality of solutions in a population. However, the implementation of non-dominated sorting can be computationally expensive, especially for populations with a large number of solutions and many objectives. The main reason is that most existing non-dominated sorting algorithms need to compare one solution with almost all others to determine its front, and many of these comparisons are redundant or unnecessary. Another reason is that as the number of objectives increases, more and more candidate solutions become non-dominated solutions, and most existing time-saving approaches cannot work effectively. In this paper, we present a novel non-dominated sorting strategy, called Hierarchical Non-Dominated Sorting (HNDS). HNDS first sorts all candidate solutions in ascending order by their first objective. Then it compares the first solution with all others one by one to make a rapid distinction between different quality solutions, thereby avoiding many unnecessary comparisons. Experiments on populations with different numbers of solutions, different numbers of objectives and different problems have been done. The results show that HNDS has better computational efficiency than fast non-dominated sort, Arena's principle and deductive sort.},
  langid = {english},
  keywords = {Computational complexity,Multi-objective evolutionary algorithm,Non-dominated sort,Pareto front},
  file = {/home/bestname/Zotero/storage/CT84LJAH/S1877750317310530.html}
}

@inproceedings{bao_ReviewCuttingEdgeTechniques_2009,
  title = {A {{Review}} on {{Cutting-Edge Techniques}} in {{Evolutionary Algorithms}}},
  booktitle = {2009 {{Fifth International Conference}} on {{Natural Computation}}},
  author = {Bao, Yun and Zhao, Erbo and Gan, Xiaocong and Luo, Dan and Han, Zhangang},
  year = {2009},
  month = aug,
  pages = {347--351},
  publisher = {{IEEE Computer Society}},
  doi = {10.1109/ICNC.2009.459},
  abstract = {There are vast amount researches in Evolutionary Algorithms (EA). We need an overview of the current state of EA research every few years. This paper reviews some of the interesting researches at the current state in both theory and application of EA. Works in EA performance improvements are introduced in the sense of balancing between convergence speed and diversity in the population. The combination of EA with other methods is highlighted as a prospective area that may give fertility results. Some smart applications are reviewed in this paper, for example, application in nuclear power plant. The authors point out some research highlights and drawbacks in the conclusion. Future research suggestions are also given.},
  isbn = {978-0-7695-3736-8},
  langid = {english},
  keywords = {done,hot},
  file = {/home/bestname/Zotero/storage/2K9CWG5F/Bao et al_2009_A Review on Cutting-Edge Techniques in Evolutionary Algorithms.pdf;/home/bestname/Zotero/storage/5KCZILAR/12OmNxI0KvG.html}
}

@misc{barla_HowCodeBERT_2021,
  title = {How to {{Code BERT Using PyTorch}} - {{Tutorial With Examples}}},
  author = {Barla, Nilesh},
  year = {2021},
  month = may,
  journal = {neptune.ai},
  abstract = {If you are an NLP enthusiast then you might have heard about BERT. In this article, we are going to explore BERT: what it is? and how it works?, and learn how to code it using PyTorch. In 2018, Google published a paper titled ``Pre-training of deep bidirectional transformers for language understanding''. In this paper, [\ldots ]},
  langid = {american},
  keywords = {blog,code},
  file = {/home/bestname/Zotero/storage/QAB77MC5/how-to-code-bert-using-pytorch-tutorial.html}
}

@inproceedings{bautista_ProcessMiningDrivenOptimization_2012,
  title = {Process {{Mining-Driven Optimization}} of a {{Consumer Loan Approvals Process}} - {{The BPIC}} 2012 {{Challenge Case Study}}},
  booktitle = {Business {{Process Management Workshops}}},
  author = {Bautista, Arjel D. and Wangikar, Lalit and Akbar, S.},
  year = {2012},
  doi = {10.1007/978-3-642-36285-9_24},
  abstract = {Findings are presented on how a deep understanding of the process was developed using the event log data, potential areas of efficiency improvement within the institution's operations and opportunities to use knowledge gathered during process execution to make predictions about likely eventual outcome of a loan application are identified. A real life event log of the loan and overdraft approvals process from a bank in the Netherlands is analyzed using process mining and other analytical techniques. The log consists of 262,200 events and 13,087 cases. Using a combination of traditional spreadsheet-based approaches, process-mining capabilities available in Disco and exploratory analytics using Classification and Regression Trees (CART). We examined the data in great detail and at multiple levels of granularity. In this report, we present our findings on how we developed a deep understanding of the process using the event log data, assessed potential areas of efficiency improvement within the institution's operations and identified opportunities to use knowledge gathered during process execution to make predictions about likely eventual outcome of a loan application. We also discuss unique challenges of working with such data, and opportunities for enhancing the impact of such analyses by incorporating additional data elements that should be available internally to the bank.}
}

@inproceedings{bautista_ProcessMiningDrivenOptimization_2013,
  title = {Process {{Mining-Driven Optimization}} of a {{Consumer Loan Approvals Process}}},
  booktitle = {Business {{Process Management Workshops}}},
  author = {Bautista, Arjel D. and Wangikar, Lalit and Akbar, Syed M. Kumail},
  editor = {La Rosa, Marcello and Soffer, Pnina},
  year = {2013},
  series = {Lecture {{Notes}} in {{Business Information Processing}}},
  pages = {219--220},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-36285-9_24},
  abstract = {A real life event log of the loan and overdraft approvals process from a bank in the Netherlands is analyzed using process mining and other analytical techniques. The log consists of 262,200 events and 13,087 cases. Using a combination of traditional spreadsheet-based approaches, process-mining capabilities available in Disco and exploratory analytics using Classification and Regression Trees (CART). We examined the data in great detail and at multiple levels of granularity. In this report, we present our findings on how we developed a deep understanding of the process using the event log data, assessed potential areas of efficiency improvement within the institution's operations and identified opportunities to use knowledge gathered during process execution to make predictions about likely eventual outcome of a loan application. We also discuss unique challenges of working with such data, and opportunities for enhancing the impact of such analyses by incorporating additional data elements that should be available internally to the bank.},
  isbn = {978-3-642-36285-9},
  langid = {english},
  file = {/home/bestname/Zotero/storage/72TYCL8I/Bautista et al_2013_Process Mining-Driven Optimization of a Consumer Loan Approvals Process.pdf}
}

@article{bayer_LearningStochasticRecurrent_2015,
  title = {Learning {{Stochastic Recurrent Networks}}},
  author = {Bayer, Justin and Osendorfer, Christian},
  year = {2015},
  month = mar,
  journal = {arXiv:1411.7610 [cs, stat]},
  eprint = {1411.7610},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Leveraging advances in variational inference, we propose to enhance recurrent neural networks with latent variables, resulting in Stochastic Recurrent Networks (STORNs). The model i) can be trained with stochastic gradient methods, ii) allows structured and multi-modal conditionals at each time step, iii) features a reliable estimator of the marginal likelihood and iv) is a generalisation of deterministic recurrent neural networks. We evaluate the method on four polyphonic musical data sets and motion capture data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,done,hot,M2M,named_model,Statistics - Machine Learning},
  annotation = {ZSCC: 0000002},
  file = {/home/bestname/Zotero/storage/CAI8L99Q/Bayer_Osendorfer_2015_Learning Stochastic Recurrent Networks.pdf;/home/bestname/Zotero/storage/EZEB95EW/1411.html}
}

@misc{berk_GeneticAlgorithmsNatural_2021,
  title = {Genetic {{Algorithms}} for {{Natural Language Processing}}},
  author = {Berk, Michael},
  year = {2021},
  month = jun,
  journal = {Medium},
  abstract = {Why GA's are effective for preprocessing NLP data},
  howpublished = {https://towardsdatascience.com/genetic-algorithms-for-natural-language-processing-b055aa7c14e9},
  langid = {english},
  file = {/home/bestname/Zotero/storage/FITYJCFX/genetic-algorithms-for-natural-language-processing-b055aa7c14e9.html}
}

@misc{bioinformatica_GlobalSequenceAlignment_2020,
  title = {Global {{Sequence Alignment}} \& {{Needleman-Wunsch}} || {{Algorithm}} and {{Example}}},
  author = {{Bioinformatica}},
  year = {2020},
  month = aug,
  abstract = {Global Sequence Alignment \& Needleman-Wunsch || Algorithm and Example In this video, we have discussed the types of common sequence alignment techniques used in Bioinformatics i.e Local alignment and  Global alignment. We have also discussed the working of the Needleman-Wunsch algorithm and discussed how to do global sequence alignment with examples. VIDEOS THAT CAN HELP YOU TO UNDERSTAND THE TOPIC BETTER Sequence similarity and identity https://youtu.be/wqsR8qOptto The concepts of Homology https://youtu.be/d9zprAGoCXY Sequence Comparison https://youtu.be/q1xXk9jnBQs LIKE      COMMENT       SHARE       SUBSCRIBE}
}

@article{bishnoi_SurveyDistanceMeasures_2020,
  title = {A Survey of Distance Measures for Mixed Variables},
  author = {Bishnoi, Sudha and Hooda, B. K.},
  year = {2020},
  journal = {International Journal of Chemical Studies},
  volume = {8},
  number = {4},
  pages = {338--343},
  publisher = {{AkiNik Publications}},
  issn = {2321-4902},
  doi = {10.22271/chemi.2020.v8.i4f.10087},
  abstract = {Distance measures are base for many statistical and data science methods with their applicability in various fields of science. Mixed variables data which is combination of continuous and categorical variables occurs frequently in fields such as medical, agriculture, remote sensing, biology, marketing, ecology etc., but a little work has been done for evaluating distance for such type of data. As there is not much literature available on distance measures for mixed data, therefore the fundamental sources that provide a comprehensive detail of a particular measure for mixed variables data were studied and reviewed in this paper.},
  langid = {english},
  keywords = {hot},
  annotation = {ZSCC: 0000000},
  file = {/home/bestname/Zotero/storage/XP3D98H5/Bishnoi_Hooda_2020_A survey of distance measures for mixed variables.pdf;/home/bestname/Zotero/storage/IJ459Q9D/special-issue.html}
}

@article{bond-taylor_DeepGenerativeModelling_2021,
  ids = {bond-taylor_DeepGenerativeModelling_2021a},
  title = {Deep {{Generative Modelling}}: {{A Comparative Review}} of {{VAEs}}, {{GANs}}, {{Normalizing Flows}}, {{Energy-Based}} and {{Autoregressive Models}}},
  shorttitle = {Deep {{Generative Modelling}}},
  author = {{Bond-Taylor}, Sam and Leach, Adam and Long, Yang and Willcocks, Chris G.},
  year = {2021},
  month = apr,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  eprint = {2103.04922},
  eprinttype = {arxiv},
  pages = {1--1},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2021.3116668},
  abstract = {Deep generative modelling is a class of techniques that train deep neural networks to model the distribution of training samples. Research has fragmented into various interconnected approaches, each of which making trade-offs including run-time, diversity, and architectural restrictions. In particular, this compendium covers energy-based models, variational autoencoders, generative adversarial networks, autoregressive models, normalizing flows, in addition to numerous hybrid approaches. These techniques are drawn under a single cohesive framework, comparing and contrasting to explain the premises behind each, while reviewing current state-of-the-art advances and implementations.},
  archiveprefix = {arXiv},
  keywords = {68T01 (Primary); 68T07 (Secondary),Analytical models,Autoregressive Models,Computational modeling,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Data models,Deep Learning,done,Energy-Based Models,G.3,Generative adversarial networks,Generative Adversarial Networks,Generative Models,hot,I.4.0,I.5.0,Neurons,Normalizing Flows,Predictive models,Statistics - Machine Learning,Training,Variational Autoencoders},
  annotation = {ZSCC: 0000010},
  file = {/home/bestname/Zotero/storage/TVCGFQZF/Bond-Taylor et al. - 2021 - Deep Generative Modelling A Comparative Review of.pdf;/home/bestname/Zotero/storage/WTB8AUFK/Bond-Taylor et al_2021_Deep Generative Modelling.pdf;/home/bestname/Zotero/storage/CWPND63K/9555209.html;/home/bestname/Zotero/storage/ZJTHZSZK/2103.html}
}

@article{bowman_GeneratingSentencesContinuous_2016,
  title = {Generating {{Sentences}} from a {{Continuous Space}}},
  author = {Bowman, Samuel R. and Vilnis, Luke and Vinyals, Oriol and Dai, Andrew M. and Jozefowicz, Rafal and Bengio, Samy},
  year = {2016},
  month = may,
  journal = {arXiv:1511.06349 [cs]},
  eprint = {1511.06349},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The standard recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an RNN-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {ZSCC: 0000003},
  file = {/home/bestname/Zotero/storage/JPZ5H6J3/Bowman et al_2016_Generating Sentences from a Continuous Space.pdf;/home/bestname/Zotero/storage/UZ6T7D78/1511.html}
}

@inproceedings{brown_DeepCounterfactualRegret_2019,
  title = {Deep {{Counterfactual Regret Minimization}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Brown, Noam and Lerer, Adam and Gross, Sam and Sandholm, Tuomas},
  year = {2019},
  month = may,
  pages = {793--802},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Counterfactual Regret Minimization (CFR) is the leading algorithm for solving large imperfect-information games. It converges to an equilibrium by iteratively traversing the game tree. In order to deal with extremely large games, abstraction is typically applied before running CFR. The abstracted game is solved with tabular CFR, and its solution is mapped back to the full game. This process can be problematic because aspects of abstraction are often manual and domain specific, abstraction algorithms may miss important strategic nuances of the game, and there is a chicken-and-egg problem because determining a good abstraction requires knowledge of the equilibrium of the game. This paper introduces Deep Counterfactual Regret Minimization, a form of CFR that obviates the need for abstraction by instead using deep neural networks to approximate the behavior of CFR in the full game. We show that Deep CFR is principled and achieves strong performance in large poker games. This is the first non-tabular variant of CFR to be successful in large games.},
  langid = {english},
  annotation = {ZSCC: 0000084},
  file = {/home/bestname/Zotero/storage/67DSSIX7/Brown et al_2019_Deep Counterfactual Regret Minimization.pdf;/home/bestname/Zotero/storage/VCINNVSB/Brown et al. - 2019 - Deep Counterfactual Regret Minimization.pdf}
}

@inproceedings{camargo_LearningAccurateLSTM_2019,
  title = {Learning {{Accurate LSTM Models}} of {{Business Processes}}},
  booktitle = {Business {{Process Management}}},
  author = {Camargo, Manuel and Dumas, Marlon and {Gonz{\'a}lez-Rojas}, Oscar},
  editor = {Hildebrandt, Thomas and {van Dongen}, Boudewijn F. and R{\"o}glinger, Maximilian and Mendling, Jan},
  year = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {286--302},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-26619-6_19},
  abstract = {Deep learning techniques have recently found applications in the field of predictive business process monitoring. These techniques allow us to predict, among other things, what will be the next events in a case, when will they occur, and which resources will trigger them. They also allow us to generate entire execution traces of a business process, or even entire event logs, which opens up the possibility of using such models for process simulation. This paper addresses the question of how to use deep learning techniques to train accurate models of business process behavior from event logs. The paper proposes an approach to train recurrent neural networks with Long-Short-Term Memory (LSTM) architecture in order to predict sequences of next events, their timestamp, and their associated resource pools. An experimental evaluation on real-life event logs shows that the proposed approach outperforms previously proposed LSTM architectures targeted at this problem.},
  isbn = {978-3-030-26619-6},
  langid = {english},
  keywords = {Deep learning,Long-Short-Term Memory,Process mining},
  file = {/home/bestname/Zotero/storage/3QWDSTQ5/Camargo et al_2019_Learning Accurate LSTM Models of Business Processes.pdf}
}

@article{carvalho_MachineLearningInterpretability_2019,
  title = {Machine {{Learning Interpretability}}: {{A Survey}} on {{Methods}} and {{Metrics}}},
  shorttitle = {Machine {{Learning Interpretability}}},
  author = {Carvalho, Diogo V. and Pereira, Eduardo M. and Cardoso, Jaime S.},
  year = {2019},
  month = aug,
  journal = {Electronics},
  volume = {8},
  number = {8},
  pages = {832},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/electronics8080832},
  abstract = {Machine learning systems are becoming increasingly ubiquitous. These systems\&rsquo;s adoption has been expanding, accelerating the shift towards a more algorithmic society, meaning that algorithmically informed decisions have greater potential for significant social impact. However, most of these accurate decision support systems remain complex black boxes, meaning their internal logic and inner workings are hidden to the user and even experts cannot fully understand the rationale behind their predictions. Moreover, new regulations and highly regulated domains have made the audit and verifiability of decisions mandatory, increasing the demand for the ability to question, understand, and trust machine learning systems, for which interpretability is indispensable. The research community has recognized this interpretability problem and focused on developing both interpretable models and explanation methods over the past few years. However, the emergence of these methods shows there is no consensus on how to assess the explanation quality. Which are the most suitable metrics to assess the quality of an explanation? The aim of this article is to provide a review of the current state of the research field on machine learning interpretability while focusing on the societal impact and on the developed methods and metrics. Furthermore, a complete literature review is presented in order to identify future directions of work on this field.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {explainability,interpretability,irrelevant,machine learning,XAI},
  annotation = {ZSCC: 0000348},
  file = {/home/bestname/Zotero/storage/I8J98D29/Carvalho et al. - 2019 - Machine Learning Interpretability A Survey on Met.pdf;/home/bestname/Zotero/storage/EVQWHZHQ/832.html}
}

@article{carvalho_MachineLearningInterpretability_2019a,
  title = {Machine {{Learning Interpretability}}: {{A Survey}} on {{Methods}} and {{Metrics}}},
  shorttitle = {Machine {{Learning Interpretability}}},
  author = {Carvalho, Diogo V. and Pereira, Eduardo M. and Cardoso, Jaime S.},
  year = {2019},
  month = aug,
  journal = {Electronics},
  volume = {8},
  number = {8},
  pages = {832},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2079-9292},
  doi = {10.3390/electronics8080832},
  abstract = {Machine learning systems are becoming increasingly ubiquitous. These systems's adoption has been expanding, accelerating the shift towards a more algorithmic society, meaning that algorithmically informed decisions have greater potential for significant social impact. However, most of these accurate decision support systems remain complex black boxes, meaning their internal logic and inner workings are hidden to the user and even experts cannot fully understand the rationale behind their predictions. Moreover, new regulations and highly regulated domains have made the audit and verifiability of decisions mandatory, increasing the demand for the ability to question, understand, and trust machine learning systems, for which interpretability is indispensable. The research community has recognized this interpretability problem and focused on developing both interpretable models and explanation methods over the past few years. However, the emergence of these methods shows there is no consensus on how to assess the explanation quality. Which are the most suitable metrics to assess the quality of an explanation? The aim of this article is to provide a review of the current state of the research field on machine learning interpretability while focusing on the societal impact and on the developed methods and metrics. Furthermore, a complete literature review is presented in order to identify future directions of work on this field.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {explainability,interpretability,machine learning,XAI},
  file = {/home/bestname/Zotero/storage/9BQHC6SI/Carvalho et al_2019_Machine Learning Interpretability.pdf;/home/bestname/Zotero/storage/9CDGALRA/832.html}
}

@misc{cerliani_MEDIUMNoteBook_2022,
  title = {{{MEDIUM}}\_{{NoteBook}}},
  author = {Cerliani, Marco},
  year = {2022},
  month = feb,
  abstract = {Repository containing notebooks of my posts on Medium},
  annotation = {ZSCC: NoCitationData[s0]}
}

@misc{cerliani_TimeSeriesGeneration_2020,
  title = {Time {{Series}} Generation with {{VAE LSTM}}},
  author = {Cerliani, Marco},
  year = {2020},
  month = dec,
  journal = {Medium},
  abstract = {Filling Time Series with Generative Deep Learning Models},
  howpublished = {https://towardsdatascience.com/time-series-generation-with-vae-lstm-5a6426365a1c},
  langid = {english},
  keywords = {done,irrelevant,VAE},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/TECNLPCW/time-series-generation-with-vae-lstm-5a6426365a1c.html}
}

@article{chen_EmpiricalStudySmoothing_,
  title = {An {{Empirical Study}} of {{Smoothing Techniques}} for {{Language Modeling}}},
  author = {Chen, Stanley F},
  pages = {64},
  abstract = {We present a tutorial introduction to n-gram models for language modeling and survey the most widely-used smoothing algorithms for such models. We then present an extensive empirical comparison of several of these smoothing techniques, including those described by Jelinek and Mercer (1980), Katz (1987), Bell, Cleary, and Witten (1990), Ney, Essen, and Kneser (1994), and Kneser and Ney (1995). We investigate how factors such as training data size, training corpus ( , e.g. Brown versus Wall Street Journal), count cuto s, and n-gram order (bigram versus trigram) a ect the relative performance of these methods, which is measured through the cross-entropy of test data. Our results show that previous comparisons have not been complete enough to fully characterize smoothing algorithm performance. We introduce methodologies for analyzing smoothing algorithm e cacy in detail, and using these techniques we motivate a novel variation of Kneser-Ney smoothing that consistently outperforms all other algorithms evaluated. Finally, results showing that improved language model smoothing leads to improved speech recognition performance are presented.},
  langid = {english},
  keywords = {hot},
  annotation = {ZSCC: 0004046},
  file = {/home/bestname/Zotero/storage/YKGI5QQ2/Chen - An Empirical Study of Smoothing Techniques for Lan.pdf}
}

@article{chen_EmpiricalStudySmoothing_1999,
  title = {An Empirical Study of Smoothing Techniques for Language Modeling},
  author = {Chen, Stanley F. and Goodman, Joshua},
  year = {1999},
  month = oct,
  journal = {Computer Speech \& Language},
  volume = {13},
  number = {4},
  pages = {359--394},
  issn = {0885-2308},
  doi = {10.1006/csla.1999.0128},
  abstract = {We survey the most widely-used algorithms for smoothing models for language n -gram modeling. We then present an extensive empirical comparison of several of these smoothing techniques, including those described by Jelinek and Mercer (1980); Katz (1987); Bell, Cleary and Witten (1990); Ney, Essen and Kneser (1994), and Kneser and Ney (1995). We investigate how factors such as training data size, training corpus (e.g. Brown vs. Wall Street Journal), count cutoffs, and n -gram order (bigram vs. trigram) affect the relative performance of these methods, which is measured through the cross-entropy of test data. We find that these factors can significantly affect the relative performance of models, with the most significant factor being training data size. Since no previous comparisons have examined these factors systematically, this is the first thorough characterization of the relative performance of various algorithms. In addition, we introduce methodologies for analyzing smoothing algorithm efficacy in detail, and using these techniques we motivate a novel variation of Kneser\textendash Ney smoothing that consistently outperforms all other algorithms evaluated. Finally, results showing that improved language model smoothing leads to improved speech recognition performance are presented.},
  langid = {english},
  file = {/home/bestname/Zotero/storage/7CZZ4NQU/Chen_Goodman_1999_An empirical study of smoothing techniques for language modeling.pdf;/home/bestname/Zotero/storage/6M7E4LPS/S0885230899901286.html}
}

@article{chen_GeneticAlgorithmsExtractive_2021,
  title = {Genetic {{Algorithms For Extractive Summarization}}},
  author = {Chen, William and Ramos, Kensal and Mullaguri, Kalyan Naidu and Wu, Annie S.},
  year = {2021},
  month = may,
  doi = {10.48550/arXiv.2105.02365},
  abstract = {Most current work in NLP utilizes deep learning, which requires a lot of training data and computational power. This paper investigates the strengths of Genetic Algorithms (GAs) for extractive summarization, as we hypothesized that GAs could construct more efficient solutions for the summarization task due to their relative customizability relative to deep learning models. This is done by building a vocabulary set, the words of which are represented as an array of weights, and optimizing those set of weights with the GA. These weights can be used to build an overall weighting of a sentence, which can then be passed to some threshold for extraction. Our results showed that the GA was able to learn a weight representation that could filter out excessive vocabulary and thus dictate sentence importance based on common English words.},
  langid = {english},
  keywords = {irrelevant},
  file = {/home/bestname/Zotero/storage/X3NZKNUN/Chen et al_2021_Genetic Algorithms For Extractive Summarization.pdf}
}

@article{chen_ProbabilisticForecastingTemporal_2020,
  ids = {chen_ProbabilisticForecastingTemporal_2020a},
  title = {Probabilistic Forecasting with Temporal Convolutional Neural Network},
  author = {Chen, Yitian and Kang, Yanfei and Chen, Yixiong and Wang, Zizhuo},
  year = {2020},
  month = jul,
  journal = {Neurocomputing},
  volume = {399},
  pages = {491--501},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2020.03.011},
  abstract = {We present a probabilistic forecasting framework based on convolutional neural network (CNN) for multiple related time series forecasting. The framework can be applied to estimate probability density under both parametric and non-parametric settings. More specifically, stacked residual blocks based on dilated causal convolutional nets are constructed to capture the temporal dependencies of the series. Combined with representation learning, our approach is able to learn complex patterns such as seasonality, holiday effects within and across series, and to leverage those patterns for more accurate forecasts, especially when historical data is sparse or unavailable. Extensive empirical studies are performed on several real-world datasets, including datasets from JD.com, China's largest online retailer. The results show that our framework compares favorably to the state-of-the-art in both point and probabilistic forecasting.},
  langid = {english},
  keywords = {Convolutional neural network,Demand forecasting,Dilated causal convolution,High-dimensional time series,Probabilistic forecasting},
  annotation = {ZSCC: 0000050},
  file = {/home/bestname/Zotero/storage/6BFV9DDJ/Chen et al_2020_Probabilistic forecasting with temporal convolutional neural network.pdf;/home/bestname/Zotero/storage/KY7BWC9F/Chen et al. - 2020 - Probabilistic forecasting with temporal convolutio.pdf}
}

@inproceedings{chung_RecurrentLatentVariable_2016,
  ids = {chung_RecurrentLatentVariable_2015},
  title = {A {{Recurrent Latent Variable Model}} for {{Sequential Data}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Neural Information Processing Systems}} - {{Volume}} 2},
  author = {Chung, Junyoung and Kastner, Kyle and Dinh, Laurent and Goel, Kratarth and Courville, Aaron and Bengio, Yoshua},
  year = {2016},
  month = apr,
  series = {{{NIPS}}'15},
  pages = {2980--2988},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA, USA}},
  abstract = {It is argued that through the use of high-level latent random variables, the variational RNN (VRNN)1 can model the kind of variability observed in highly structured sequential data such as natural speech. In this paper, we explore the inclusion of latent random variables into the hidden state of a recurrent neural network (RNN) by combining the elements of the variational autoencoder. We argue that through the use of high-level latent random variables, the variational RNN (VRNN)1 can model the kind of variability observed in highly structured sequential data such as natural speech. We empirically evaluate the proposed model against other related sequential models on four speech datasets and one handwriting dataset. Our results show the important roles that latent random variables can play in the RNN dynamics.},
  keywords = {Computer Science - Machine Learning,done,hot,M2M,most-complete-causally-dependent-DVAE,named_model,VAE},
  annotation = {VRNN \& VRNN-I  ZSCC: 0000909},
  file = {/home/bestname/Zotero/storage/6IE3AW2Y/Chung et al_2015_A Recurrent Latent Variable Model for Sequential Data.pdf;/home/bestname/Zotero/storage/6LMCUZNS/1506.html}
}

@book{commandeur_IntroductionStateSpace_2007,
  title = {An Introduction to State Space Time Series Analysis},
  author = {Commandeur, Jacques J. F. and Koopman, S. J.},
  year = {2007},
  series = {Practical Econometrics},
  publisher = {{Oxford University Press}},
  address = {{Oxford ; New York}},
  isbn = {978-0-19-922887-4},
  langid = {english},
  lccn = {QA280 .C65 2007},
  keywords = {State-space methods,Time-series analysis},
  annotation = {ZSCC: 0000496  OCLC: ocn123374304},
  file = {/home/bestname/Zotero/storage/SLB48B98/Commandeur und Koopman - 2007 - An introduction to state space time series analysi.pdf}
}

@article{damerau_TechniqueComputerDetection_1964,
  title = {A Technique for Computer Detection and Correction of Spelling Errors},
  author = {Damerau, Fred J.},
  year = {1964},
  month = mar,
  journal = {Communications of the ACM},
  volume = {7},
  number = {3},
  pages = {171--176},
  issn = {0001-0782},
  doi = {10.1145/363958.363994},
  abstract = {The method described assumes that a word which cannot be found in a dictionary has at most one error, which might be a wrong, missing or extra letter or a single transposition. The unidentified input word is compared to the dictionary again, testing each time to see if the words match\textemdash assuming one of these errors occurred. During a test run on garbled text, correct identifications were made for over 95 percent of these error types.},
  file = {/home/bestname/Zotero/storage/945RHSHC/Damerau - 1964 - A technique for computer detection and correction .pdf}
}

@inproceedings{dandl_MultiObjectiveCounterfactualExplanations_2020,
  title = {Multi-{{Objective Counterfactual Explanations}}},
  booktitle = {Parallel {{Problem Solving}} from {{Nature}} \textendash{} {{PPSN XVI}}},
  author = {Dandl, Susanne and Molnar, Christoph and Binder, Martin and Bischl, Bernd},
  editor = {B{\"a}ck, Thomas and Preuss, Mike and Deutz, Andr{\'e} and Wang, Hao and Doerr, Carola and Emmerich, Michael and Trautmann, Heike},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {448--469},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-58112-1_31},
  abstract = {Counterfactual explanations are one of the most popular methods to make predictions of black box machine learning models interpretable by providing explanations in the form of `what-if scenarios'. Most current approaches optimize a collapsed, weighted sum of multiple objectives, which are naturally difficult to balance a-priori. We propose the Multi-Objective Counterfactuals (MOC) method, which translates the counterfactual search into a multi-objective optimization problem. Our approach not only returns a diverse set of counterfactuals with different trade-offs between the proposed objectives, but also maintains diversity in feature space. This enables a more detailed post-hoc analysis to facilitate better understanding and also more options for actionable user responses to change the predicted outcome. Our approach is also model-agnostic and works for numerical and categorical input features. We show the usefulness of MOC in concrete cases and compare our approach with state-of-the-art methods for counterfactual explanations.},
  isbn = {978-3-030-58112-1},
  langid = {english},
  keywords = {Counterfactual explanations,Interpretability,Interpretable machine learning,Multi-objective optimization,NSGA-II},
  annotation = {ZSCC: 0000055},
  file = {/home/bestname/Zotero/storage/RL4FDYQH/Dandl et al_2020_Multi-Objective Counterfactual Explanations.pdf}
}

@article{danilevsky_SurveyStateExplainable_2020,
  title = {A {{Survey}} of the {{State}} of {{Explainable AI}} for {{Natural Language Processing}}},
  author = {Danilevsky, Marina and Qian, Kun and Aharonov, Ranit and Katsis, Yannis and Kawas, Ban and Sen, Prithviraj},
  year = {2020},
  month = oct,
  journal = {arXiv:2010.00711 [cs]},
  eprint = {2010.00711},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recent years have seen important advances in the quality of state-of-the-art models, but this has come at the expense of models becoming less interpretable. This survey presents an overview of the current state of Explainable AI (XAI), considered within the domain of Natural Language Processing (NLP). We discuss the main categorization of explanations, as well as the various ways explanations can be arrived at and visualized. We detail the operations and explainability techniques currently available for generating explanations for NLP model predictions, to serve as a resource for model developers in the community. Finally, we point out the current gaps and encourage directions for future work in this important research area.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,done,I.2.7,XAI},
  annotation = {ZSCC: 0000023},
  file = {/home/bestname/Zotero/storage/947IF9L2/Danilevsky et al_2020_A Survey of the State of Explainable AI for Natural Language Processing.pdf;/home/bestname/Zotero/storage/263WG3LE/2010.html}
}

@article{davahli_OptimizingCOVID19Vaccine_2021,
  title = {Optimizing {{COVID-19}} Vaccine Distribution across the {{United States}} Using Deterministic and Stochastic Recurrent Neural Networks},
  author = {Davahli, Mohammad Reza and Karwowski, Waldemar and Fiok, Krzysztof},
  year = {2021},
  month = jun,
  journal = {PLOS ONE},
  volume = {16},
  number = {7},
  pages = {e0253925},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0253925},
  abstract = {Optimizing COVID-19 vaccine distribution can help plan around the limited production and distribution of vaccination, particularly in early stages. One of the main criteria for equitable vaccine distribution is predicting the geographic distribution of active virus at the time of vaccination. This research developed sequence-learning models to predict the behavior of the COVID-19 pandemic across the US, based on previously reported information. For this objective, we used two time-series datasets of confirmed COVID-19 cases and COVID-19 effective reproduction numbers from January 22, 2020 to November 26, 2020 for all states in the US. The datasets have 310 time-steps (days) and 50 features (US states). To avoid training the models for all states, we categorized US states on the basis of their similarity to previously reported COVID-19 behavior. For this purpose, we used an unsupervised self-organizing map to categorize all states of the US into four groups on the basis of the similarity of their effective reproduction numbers. After selecting a leading state (the state with earliest outbreaks) in each group, we developed deterministic and stochastic Long Short Term Memory (LSTM) and Mixture Density Network (MDN) models. We trained the models with data from each leading state to make predictions, then compared the models with a baseline linear regression model. We also remove seasonality and trends from a dataset of non-stationary COVID-19 cases to determine the effects on prediction. We showed that the deterministic LSTM model trained on the COVID-19 effective reproduction numbers outperforms other prediction methods.},
  langid = {english},
  keywords = {COVID 19,Forecasting,Linear regression analysis,Pandemics,Recurrent neural networks,United States,Vaccines,Virus testing},
  file = {/home/bestname/Zotero/storage/CNFU666I/Davahli et al_2021_Optimizing COVID-19 vaccine distribution across the United States using.pdf;/home/bestname/Zotero/storage/G7AZ3VRU/article.html}
}

@misc{davahli_StochasticRecurrentNeural_2021,
  title = {Stochastic\_recurrent\_neural\_networks},
  author = {Davahli, Mohammad Reza},
  year = {2021},
  month = apr
}

@misc{davahli_StochasticRecurrentNeural_2021a,
  title = {Stochastic\_recurrent\_neural\_networks ({{CODE}})},
  author = {Davahli, Mohammad Reza},
  year = {2021},
  month = apr
}

@inproceedings{dekoninck_Act2vecTrace2vecLog2vec_2018,
  ids = {dekoninck_Act2vecTrace2vecLog2vec_2018a},
  title = {Act2vec, Trace2vec, Log2vec, and~Model2vec: {{Representation Learning}} for {{Business Processes}}},
  shorttitle = {Act2vec, Trace2vec, Log2vec, and~Model2vec},
  booktitle = {Business {{Process Management}}},
  author = {De Koninck, Pieter and {vanden Broucke}, Seppe and De Weerdt, Jochen},
  editor = {Weske, Mathias and Montali, Marco and Weber, Ingo and {vom Brocke}, Jan},
  year = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {305--321},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-98648-7_18},
  abstract = {In process mining, the challenge is typically to turn raw event data into meaningful models, insights, or actions. One of the key problems of a data-driven analysis of processes, is the high dimensionality of the data. In this paper, we address this problem by developing representation learning techniques for business processes. More specifically, the representation learning paradigm is applied to activities, traces, logs, and models in order to learn highly informative but low-dimensional vectors, often referred to as embeddings, based on a neural network architecture. Subsequently, these vectors can be used for automated inference tasks such as trace clustering, process comparison, predictive process monitoring, anomaly detection, etc. Accordingly, the main contribution of this paper is the proposal of representation learning architectures at the level of activities, traces, logs, and models that can produce a distributed representation of these objects and a thorough analysis of potential applications. In an experimental evaluation, we show the power of such derived representations in the context of trace clustering and process model comparison.},
  isbn = {978-3-319-98648-7},
  langid = {english},
  keywords = {done,Embeddings,Process mining,Representation learning,Word embedding},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/645Y5CWQ/De Koninck et al_2018_act2vec, trace2vec, log2vec, and model2vec.pdf}
}

@inproceedings{delaney_InstanceBasedCounterfactualExplanations_2021,
  title = {Instance-{{Based Counterfactual Explanations}} for {{Time Series Classification}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Delaney, Eoin and Greene, Derek and Keane, Mark T.},
  editor = {{S{\'a}nchez-Ruiz}, Antonio A. and Floyd, Michael W.},
  year = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {32--47},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-86957-1_3},
  abstract = {In recent years, there has been a rapidly expanding focus on explaining the predictions made by black-box AI systems that handle image and tabular data. However, considerably less attention has been paid to explaining the predictions of opaque AI systems handling time series data. In this paper, we advance a novel model-agnostic, case-based technique \textendash{} Native Guide \textendash{} that generates counterfactual explanations for time series classifiers. Given a query time series, TqTqT\_\{q\}, for which a black-box classification system predicts class, c, a counterfactual time series explanation shows how TqTqT\_\{q\} could change, such that the system predicts an alternative class, c{${'}$}c{${'}$}c'. The proposed instance-based technique adapts existing counterfactual instances in the case-base by highlighting and modifying discriminative areas of the time series that underlie the classification. Quantitative and qualitative results from two comparative experiments indicate that Native Guide generates plausible, proximal, sparse and diverse explanations that are better than those produced by key benchmark counterfactual methods.},
  isbn = {978-3-030-86957-1},
  langid = {english},
  keywords = {Counterfactual explanation,Time series,XCBR},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/SP9YN7AL/Delaney et al_2021_Instance-Based Counterfactual Explanations for Time Series Classification.pdf}
}

@article{deng_MNISTDatabaseHandwritten_2012,
  title = {The {{MNIST Database}} of {{Handwritten Digit Images}} for {{Machine Learning Research}} [{{Best}} of the {{Web}}]},
  author = {Deng, Li},
  year = {2012},
  month = nov,
  journal = {IEEE Signal Processing Magazine},
  volume = {29},
  number = {6},
  pages = {141--142},
  issn = {1558-0792},
  doi = {10.1109/MSP.2012.2211477},
  abstract = {In this issue, ``Best of the Web'' presents the modified National Institute of Standards and Technology (MNIST) resources, consisting of a collection of handwritten digit images used extensively in optical character recognition and machine learning research.},
  keywords = {Machine learning},
  file = {/home/bestname/Zotero/storage/7TU42I7X/Deng_2012_The MNIST Database of Handwritten Digit Images for Machine Learning Research.pdf;/home/bestname/Zotero/storage/F8XN8M6T/6296535.html}
}

@inproceedings{dohare_CombinationSimilarityMeasures_2011,
  title = {Combination of Similarity Measures for Time Series Classification Using Genetic Algorithms},
  booktitle = {2011 {{IEEE Congress}} of {{Evolutionary Computation}} ({{CEC}})},
  author = {Dohare, Deepti and Devi, V. Susheela},
  year = {2011},
  month = jun,
  pages = {401--408},
  issn = {1941-0026},
  doi = {10.1109/CEC.2011.5949646},
  abstract = {Time series classification deals with the problem of classification of data that is multivariate in nature. This means that one or more of the attributes is in the form of a sequence. The notion of similarity or distance, used in time series data, is significant and affects the accuracy, time, and space complexity of the classification algorithm. There exist numerous similarity measures for time series data, but each of them has its own disadvantages. Instead of relying upon a single similarity measure, our aim is to find the near optimal solution to the classification problem by combining different similarity measures. In this work, we use genetic algorithms to combine the similarity measures so as to get the best performance. The weightage given to different similarity measures evolves over a number of generations so as to get the best combination. We test our approach on a number of benchmark time series datasets and present promising results.},
  keywords = {Accuracy,Genetic algorithms,Genetics,Time measurement,Time series analysis,Training,Weight measurement},
  file = {/home/bestname/Zotero/storage/RTXHAEI4/Dohare_Devi_2011_Combination of similarity measures for time series classification using genetic.pdf;/home/bestname/Zotero/storage/7KC7GXAR/5949646.html}
}

@misc{doshi_FoundationsNLPExplained_2021,
  title = {Foundations of {{NLP Explained Visually}}: {{Beam Search}}, {{How}} It {{Works}}},
  shorttitle = {Foundations of {{NLP Explained Visually}}},
  author = {Doshi, Ketan},
  year = {2021},
  month = may,
  journal = {Medium},
  abstract = {A Gentle Guide to how Beam Search enhances predictions, in Plain English},
  howpublished = {https://towardsdatascience.com/foundations-of-nlp-explained-visually-beam-search-how-it-works-1586b9849a24},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/5PVKI3V5/foundations-of-nlp-explained-visually-beam-search-how-it-works-1586b9849a24.html}
}

@misc{emmanuel_VariationalRecurrentNeuralNetwork_2022,
  title = {{{VariationalRecurrentNeuralNetwork}}},
  author = {{emmanuel}},
  year = {2022},
  month = mar,
  abstract = {Pytorch implementation of the Variational Recurrent Neural Network (VRNN).}
}

@article{fabius_VariationalRecurrentAutoEncoders_2015,
  title = {Variational {{Recurrent Auto-Encoders}}},
  author = {Fabius, Otto and {van Amersfoort}, Joost R.},
  year = {2015},
  month = jun,
  journal = {arXiv:1412.6581 [cs, stat]},
  eprint = {1412.6581},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {In this paper we propose a model that combines the strengths of RNNs and SGVB: the Variational Recurrent Auto-Encoder (VRAE). Such a model can be used for efficient, large scale unsupervised learning on time series data, mapping the time series data to a latent vector representation. The model is generative, such that data can be generated from samples of the latent space. An important contribution of this work is that the model can make use of unlabeled data in order to facilitate supervised training of RNNs by initialising the weights and network state.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,done,irrelevant,Statistics - Machine Learning},
  annotation = {ZSCC: 0000005},
  file = {/home/bestname/Zotero/storage/8FU4IGI2/Fabius_van Amersfoort_2015_Variational Recurrent Auto-Encoders.pdf;/home/bestname/Zotero/storage/KMM5AJKI/1412.html}
}

@misc{falcon_VariationalAutoencoderDemystified_2020,
  title = {Variational {{Autoencoder Demystified With PyTorch Implementation}}.},
  author = {Falcon, William},
  year = {2020},
  month = dec,
  journal = {Medium},
  abstract = {This tutorial implements a variational autoencoder for non-black and white images using PyTorch.},
  howpublished = {https://towardsdatascience.com/variational-autoencoder-demystified-with-pytorch-implementation-3a06bee395ed},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/75AJ6QYY/variational-autoencoder-demystified-with-pytorch-implementation-3a06bee395ed.html}
}

@misc{fdeloche_EnglishDiagramOneunit_2017,
  title = {English:  {{A}} Diagram for a One-Unit Recurrent Neural Network ({{RNN}}). {{From}} Bottom to Top~: Input State, Hidden State, Output State. {{U}}, {{V}}, {{W}} Are the Weights of the Network. {{Compressed}} Diagram on the Left and the Unfold Version of It on the Right.},
  shorttitle = {English},
  author = {{fdeloche}},
  year = {2017},
  month = jun,
  file = {/home/bestname/Zotero/storage/HKIFTFYC/FileRecurrent_neural_network_unfold.html}
}

@article{feder_CausalInferenceNatural_2021,
  title = {Causal {{Inference}} in {{Natural Language Processing}}: {{Estimation}}, {{Prediction}}, {{Interpretation}} and {{Beyond}}},
  shorttitle = {Causal {{Inference}} in {{Natural Language Processing}}},
  author = {Feder, Amir and Keith, Katherine A. and Manzoor, Emaad and Pryzant, Reid and Sridhar, Dhanya and {Wood-Doughty}, Zach and Eisenstein, Jacob and Grimmer, Justin and Reichart, Roi and Roberts, Margaret E. and Stewart, Brandon M. and Veitch, Victor and Yang, Diyi},
  year = {2021},
  month = sep,
  journal = {arXiv:2109.00725 [cs]},
  eprint = {2109.00725},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {A fundamental goal of scientific research is to learn about causal relationships. However, despite its critical role in the life and social sciences, causality has not had the same importance in Natural Language Processing (NLP), which has traditionally placed more emphasis on predictive tasks. This distinction is beginning to fade, with an emerging area of interdisciplinary research at the convergence of causal inference and language processing. Still, research on causality in NLP remains scattered across domains without unified definitions, benchmark datasets and clear articulations of the remaining challenges. In this survey, we consolidate research across academic areas and situate it in the broader NLP landscape. We introduce the statistical challenge of estimating causal effects, encompassing settings where text is used as an outcome, treatment, or as a means to address confounding. In addition, we explore potential uses of causal inference to improve the performance, robustness, fairness, and interpretability of NLP models. We thus provide a unified overview of causal inference for the computational linguistics community.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/Y99EVZ2K/Feder et al_2021_Causal Inference in Natural Language Processing.pdf;/home/bestname/Zotero/storage/IY8GNV8N/2109.html}
}

@inproceedings{fern_TextCounterfactualsLatent_2021,
  title = {Text {{Counterfactuals}} via {{Latent Optimization}} and {{Shapley-Guided Search}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Fern, Xiaoli and Pope, Quintin},
  year = {2021},
  month = nov,
  pages = {5578--5593},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.452},
  abstract = {We study the problem of generating counterfactual text for a classifier as a means for understanding and debugging classification. Given a textual input and a classification model, we aim to minimally alter the text to change the model's prediction. White-box approaches have been successfully applied to similar problems in vision where one can directly optimize the continuous input. Optimization-based approaches become difficult in the language domain due to the discrete nature of text. We bypass this issue by directly optimizing in the latent space and leveraging a language model to generate candidate modifications from optimized latent representations. We additionally use Shapley values to estimate the combinatoric effect of multiple changes. We then use these estimates to guide a beam search for the final counterfactual text. We achieve favorable performance compared to recent white-box and black-box baselines using human and automatic evaluations. Ablation studies show that both latent optimization and the use of Shapley values improve success rate and the quality of the generated counterfactuals.},
  annotation = {ZSCC: 0000001},
  file = {/home/bestname/Zotero/storage/T9H47JW3/Fern_Pope_2021_Text Counterfactuals via Latent Optimization and Shapley-Guided Search.pdf}
}

@article{fisher_UseMultipleMeasurements_1936,
  title = {The {{Use}} of {{Multiple Measurements}} in {{Taxonomic Problems}}},
  author = {Fisher, R. A.},
  year = {1936},
  journal = {Annals of Eugenics},
  volume = {7},
  number = {2},
  pages = {179--188},
  issn = {2050-1439},
  doi = {10.1111/j.1469-1809.1936.tb02137.x},
  abstract = {The articles published by the Annals of Eugenics (1925\textendash 1954) have been made available online as an historical archive intended for scholarly use. The work of eugenicists was often pervaded by prejudice against racial, ethnic and disabled groups. The online publication of this material for scholarly research purposes is not an endorsement of those views nor a promotion of eugenics in any way.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-1809.1936.tb02137.x},
  file = {/home/bestname/Zotero/storage/9234AERN/Fisher_1936_The Use of Multiple Measurements in Taxonomic Problems.pdf}
}

@inproceedings{fraccaro_SequentialNeuralModels_2016,
  title = {Sequential Neural Models with Stochastic Layers},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Fraccaro, Marco and S{\o}nderby, S{\o}ren Kaae and Paquet, Ulrich and Winther, Ole},
  year = {2016},
  month = dec,
  series = {{{NIPS}}'16},
  pages = {2207--2215},
  publisher = {{Curran Associates Inc.}},
  address = {{Red Hook, NY, USA}},
  abstract = {How can we efficiently propagate uncertainty in a latent state representation with recurrent neural networks? This paper introduces stochastic recurrent neural networks which glue a deterministic recurrent neural network and a state space model together to form a stochastic and sequential neural generative model. The clear separation of deterministic and stochastic layers allows a structured variational inference network to track the factorization of the model's posterior distribution. By retaining both the nonlinear recursive structure of a recurrent neural network and averaging over the uncertainty in a latent path, like a state space model, we improve the state of the art results on the Blizzard and TTMIT speech modeling data sets by a large margin, while achieving comparable performances to competing methods on polyphonic music modeling.},
  isbn = {978-1-5108-3881-9},
  keywords = {hot,M2M,named_model,skimmed},
  annotation = {ZSCC: 0000286},
  file = {/home/bestname/Zotero/storage/WADVPS34/Fraccaro et al_2016_Sequential neural models with stochastic layers.pdf}
}

@techreport{francis79browncorpus,
  title = {Brown Corpus Manual},
  author = {Francis, W. N. and Kucera, H.},
  year = {1979},
  institution = {{Department of Linguistics, Brown University, Providence, Rhode Island, US}},
  added-at = {2008-02-29T17:14:20.000+0100},
  biburl = {https://www.bibsonomy.org/bibtex/260bb0c74c2ecced0632393e47eb64f48/sb3000},
  interhash = {119c367841941ad1a8f0db35d9f1c0b9},
  intrahash = {60bb0c74c2ecced0632393e47eb64f48},
  keywords = {corpus linguistics text-mining},
  timestamp = {2008-02-29T17:14:20.000+0100}
}

@article{frank_HierarchicalSequentialProcessing_2018,
  title = {Hierarchical and Sequential Processing of Language},
  author = {Frank, Stefan L. and Christiansen, Morten H.},
  year = {2018},
  month = oct,
  journal = {Language, Cognition and Neuroscience},
  volume = {33},
  number = {9},
  pages = {1213--1218},
  publisher = {{Routledge}},
  issn = {2327-3798},
  doi = {10.1080/23273798.2018.1424347},
  abstract = {Ding et al. (2017) contrast their view that language processing is based on hierarchical syntactic structures, to a view that relies on word-level input statistics. In this response to their paper, we clarify how, exactly, the two views differ (and how they do not), and make a case for the importance of sequential, as opposed to hierarchical, structure for language processing.},
  keywords = {abstraction and generalisation,Hierarchical structure,language statistics,sequential structure},
  annotation = {\_eprint: https://doi.org/10.1080/23273798.2018.1424347},
  file = {/home/bestname/Zotero/storage/ZXJK5XAJ/Frank_Christiansen_2018_Hierarchical and sequential processing of language.pdf;/home/bestname/Zotero/storage/SN2C6PP7/23273798.2018.html}
}

@article{gamback_EvolutionaryAlgorithmsNatural_2010,
  title = {Evolutionary {{Algorithms}} in {{Natural Language Processing}}},
  author = {Gamb{\"a}ck, Bj{\"o}rn},
  year = {2010},
  month = nov,
  journal = {Norwegian Artificial Intelligens Symposium (NAIS)},
  abstract = {(Full-text PDF) Evolutionary Algorithms in Natural Language Processing},
  langid = {english},
  keywords = {ambiguity resolution,done,grammar induction,hot,machine translation,review,text summarisation},
  file = {/home/bestname/Zotero/storage/T5UHQSA2/GambÃ¤ck_2010_Evolutionary Algorithms in Natural Language Processing.pdf;/home/bestname/Zotero/storage/SVBC5EMV/Evolutionary_Algorithms_in_Natural_Language_Processing.html}
}

@article{gardner_EvaluatingModelsLocal_2020,
  title = {Evaluating {{Models}}' {{Local Decision Boundaries}} via {{Contrast Sets}}},
  author = {Gardner, Matt and Artzi, Yoav and Basmova, Victoria and Berant, Jonathan and Bogin, Ben and Chen, Sihao and Dasigi, Pradeep and Dua, Dheeru and Elazar, Yanai and Gottumukkala, Ananth and Gupta, Nitish and Hajishirzi, Hanna and Ilharco, Gabriel and Khashabi, Daniel and Lin, Kevin and Liu, Jiangming and Liu, Nelson F. and Mulcaire, Phoebe and Ning, Qiang and Singh, Sameer and Smith, Noah A. and Subramanian, Sanjay and Tsarfaty, Reut and Wallace, Eric and Zhang, Ally and Zhou, Ben},
  year = {2020},
  month = oct,
  journal = {arXiv:2004.02709 [cs]},
  eprint = {2004.02709},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Standard test sets for supervised learning evaluate in-distribution generalization. Unfortunately, when a dataset has systematic gaps (e.g., annotation artifacts), these evaluations are misleading: a model can learn simple decision rules that perform well on the test set but do not capture a dataset's intended capabilities. We propose a new annotation paradigm for NLP that helps to close systematic gaps in the test data. In particular, after a dataset is constructed, we recommend that the dataset authors manually perturb the test instances in small but meaningful ways that (typically) change the gold label, creating contrast sets. Contrast sets provide a local view of a model's decision boundary, which can be used to more accurately evaluate a model's true linguistic capabilities. We demonstrate the efficacy of contrast sets by creating them for 10 diverse NLP datasets (e.g., DROP reading comprehension, UD parsing, IMDb sentiment analysis). Although our contrast sets are not explicitly adversarial, model performance is significantly lower on them than on the original test sets---up to 25\textbackslash\% in some cases. We release our contrast sets as new evaluation benchmarks and encourage future dataset construction efforts to follow similar annotation processes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Evaluation,irrelevant},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/2PHLBZLN/Gardner et al_2020_Evaluating Models' Local Decision Boundaries via Contrast Sets.pdf;/home/bestname/Zotero/storage/9BVDYYVT/2004.html}
}

@techreport{gartnerpm2019-interview,
  title = {Interview in the 2019 Gartner Market Guide for Process Mining, Research Note {{G00387812}} by m. {{Kerremans}}},
  author = {van der Aalst, W.M.P.},
  year = {2019},
  added-at = {2020-01-07T14:01:33.000+0100},
  biburl = {https://www.bibsonomy.org/bibtex/210c445ad11af96f07ece965cc1b66567/wvdaalst},
  interhash = {e1a6d063d88e9a533df371ad2e2e90f6},
  intrahash = {10c445ad11af96f07ece965cc1b66567},
  keywords = {imported},
  annotation = {ZSCC: NoCitationData[s0]},
  timestamp = {2020-01-07T14:02:12.000+0100}
}

@inproceedings{garza-fabre_RankingMethodsManyObjective_2009,
  title = {Ranking {{Methods}} for {{Many-Objective Optimization}}},
  booktitle = {{{MICAI}} 2009: {{Advances}} in {{Artificial Intelligence}}},
  author = {{Garza-Fabre}, Mario and Pulido, Gregorio Toscano and Coello, Carlos A. Coello},
  editor = {Aguirre, Arturo Hern{\'a}ndez and Borja, Ra{\'u}l Monroy and Garci{\'a}, Carlos Alberto Reyes},
  year = {2009},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {633--645},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-05258-3_56},
  abstract = {An important issue with Evolutionary Algorithms (EAs) is the way to identify the best solutions in order to guide the search process. Fitness comparisons among solutions in single-objective optimization is straightforward, but when dealing with multiple objectives, it becomes a non-trivial task. Pareto dominance has been the most commonly adopted relation to compare solutions in a multiobjective optimization context. However, it has been shown that as the number of objectives increases, the convergence ability of approaches based on Pareto dominance decreases. In this paper, we propose three novel fitness assignment methods for many-objective optimization. We also perform a comparative study in order to investigate how effective are the proposed approaches to guide the search in high-dimensional objective spaces. Results indicate that our approaches behave better than six state-of-the-art fitness assignment methods.},
  isbn = {978-3-642-05258-3},
  langid = {english},
  keywords = {Average Ranking,Multiobjective Optimization,Pareto Front,Ranking Method,Relative Entropy},
  file = {/home/bestname/Zotero/storage/64EXQA63/Garza-Fabre et al_2009_Ranking Methods for Many-Objective Optimization.pdf}
}

@article{gerzson_IntelligentControlSystems_2005,
  title = {Intelligent Control Systems: An Introduction with Examples},
  shorttitle = {Intelligent Control Systems},
  author = {Gerzson, Miklos},
  year = {2005},
  month = jan,
  journal = {Automatica},
  abstract = {Intelligent control systems: an introduction with examples},
  langid = {english},
  file = {/home/bestname/Zotero/storage/7LB37B4U/Intelligent_control_systems_an_introduction_with_examples.html}
}

@article{girin_DynamicalVariationalAutoencoders_2021,
  title = {Dynamical {{Variational Autoencoders}}: {{A Comprehensive Review}}},
  shorttitle = {Dynamical {{Variational Autoencoders}}},
  author = {Girin, Laurent and Leglaive, Simon and Bie, Xiaoyu and Diard, Julien and Hueber, Thomas and {Alameda-Pineda}, Xavier},
  year = {2021},
  month = dec,
  journal = {Foundations and Trends\textregistered{} in Machine Learning},
  volume = {15},
  number = {1-2},
  eprint = {2008.12595},
  eprinttype = {arxiv},
  pages = {1--175},
  publisher = {{Now Publishers, Inc.}},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000089},
  abstract = {The Variational Autoencoder (VAE) is a powerful deep generative model that is now extensively used to represent high-dimensional complex data via a low-dimensional latent space learned in an unsupervised manner. In the original VAE model, input data vectors are processed independently. In recent years, a series of papers have presented different extensions of the VAE to process sequential data, that not only model the latent space, but also model the temporal dependencies within a sequence of data vectors and corresponding latent vectors, relying on recurrent neural networks or state space models. In this paper we perform an extensive literature review of these models. Importantly, we introduce and discuss a general class of models called Dynamical Variational Autoencoders (DVAEs) that encompasses a large subset of these temporal VAE extensions. Then we present in detail seven different instances of DVAE that were recently proposed in the literature, with an effort to homogenize the notations and presentation lines, as well as to relate these models with existing classical temporal models. We reimplemented those seven DVAE models and we present the results of an experimental benchmark conducted on the speech analysis-resynthesis task (the PyTorch code is made publicly available). The paper is concluded with an extensive discussion on important issues concerning the DVAE class of models and future research guidelines.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,hot,skimmed,Statistics - Machine Learning},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/IGKHL2PE/Girin et al_2021_Dynamical Variational Autoencoders.pdf;/home/bestname/Zotero/storage/I4JBA56T/2008.html}
}

@article{graves_GeneratingSequencesRecurrent_2014,
  title = {Generating {{Sequences With Recurrent Neural Networks}}},
  author = {Graves, Alex},
  year = {2014},
  month = jun,
  journal = {arXiv:1308.0850 [cs]},
  eprint = {1308.0850},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing},
  annotation = {ZSCC: 0000001},
  file = {/home/bestname/Zotero/storage/NAZX7VIE/Graves_2014_Generating Sequences With Recurrent Neural Networks.pdf;/home/bestname/Zotero/storage/G3MA22UR/1308.html}
}

@article{gregor_TemporalDifferenceVariational_2019,
  title = {Temporal {{Difference Variational Auto-Encoder}}},
  author = {Gregor, Karol and Papamakarios, George and Besse, Frederic and Buesing, Lars and Weber, Theophane},
  year = {2019},
  month = jan,
  journal = {arXiv:1806.03107 [cs, stat]},
  eprint = {1806.03107},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {To act and plan in complex environments, we posit that agents should have a mental simulator of the world with three characteristics: (a) it should build an abstract state representing the condition of the world; (b) it should form a belief which represents uncertainty on the world; (c) it should go beyond simple step-by-step simulation, and exhibit temporal abstraction. Motivated by the absence of a model satisfying all these requirements, we propose TD-VAE, a generative sequence model that learns representations containing explicit beliefs about states several steps into the future, and that can be rolled out directly without single-step transitions. TD-VAE is trained on pairs of temporally separated time points, using an analogue of temporal difference learning used in reinforcement learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/8GYNCZT7/Gregor et al_2019_Temporal Difference Variational Auto-Encoder.pdf;/home/bestname/Zotero/storage/MM26JGGH/1806.html}
}

@misc{grigsby_MultivariateTimeSeries_2021,
  ids = {grigsby_MultivariateTimeSeries_2021a,grigsby_MultivariateTimeSeries_2021b},
  title = {Multivariate {{Time Series Forecasting}} with {{Transformers}}},
  author = {Grigsby, Jake},
  year = {2021},
  month = oct,
  journal = {Medium},
  abstract = {Spatiotemporal Learning Without a Graph},
  howpublished = {https://towardsdatascience.com/multivariate-time-series-forecasting-with-transformers-384dc6ce989b},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/2AZ7D8D8/multivariate-time-series-forecasting-with-transformers-384dc6ce989b.html;/home/bestname/Zotero/storage/A5DW2UEB/multivariate-time-series-forecasting-with-transformers-384dc6ce989b.html;/home/bestname/Zotero/storage/RW4PU6UK/multivariate-time-series-forecasting-with-transformers-384dc6ce989b.html}
}

@article{grisold_AdoptionUseManagement_2020,
  title = {Adoption, Use and Management of Process Mining in Practice},
  author = {Grisold, Thomas and Mendling, Jan and Otto, Markus and {vom Brocke}, Jan},
  year = {2020},
  month = jan,
  journal = {Business Process Management Journal},
  volume = {27},
  number = {2},
  pages = {369--387},
  publisher = {{Emerald Publishing Limited}},
  issn = {1463-7154},
  doi = {10.1108/BPMJ-03-2020-0112},
  abstract = {Purpose This study explores how process managers perceive the adoption, use and management of process mining in practice. While research in process mining predominantly focuses on the technical aspects, our work highlights organizational and managerial implications. Design/methodology/approach We report on a focus group study conducted with process managers from various industries in Central Europe. This setting allowed us to gain diverse and in-depth insights about the needs and expectations of practitioners in relation to the adoption, use and management of process mining. Findings We find that process managers face four central challenges. These challenges are largely related to four stages; (1) planning and business case calculation, (2) process selection, (3) implementation, and (4) process mining use. Research limitations/implications We point to research opportunities in relation to the adoption, use and management of process mining. We suggest that future research should apply interdisciplinary study designs to better understand the managerial and organizational implications of process mining. Practical implications The reported challenges have various practical implications at the organizational and managerial level. We explore how existing BPM frameworks can be extended to meet these challenges. Originality/value This study is among the first attempts to explore process mining from the perspective of process managers. It clarifies important challenges and points to avenues for future research.},
  keywords = {Data privacy,Focus group,Governance,Leadership,Organizational implications,Process mining},
  annotation = {ZSCC: 0000012},
  file = {/home/bestname/Zotero/storage/65Q2N8AN/Grisold et al_2020_Adoption, use and management of process mining in practice.pdf}
}

@article{gulrajani_PixelVAELatentVariable_2016,
  title = {{{PixelVAE}}: {{A Latent Variable Model}} for {{Natural Images}}},
  shorttitle = {{{PixelVAE}}},
  author = {Gulrajani, Ishaan and Kumar, Kundan and Ahmed, Faruk and Taiga, Adrien Ali and Visin, Francesco and Vazquez, David and Courville, Aaron},
  year = {2016},
  month = nov,
  journal = {arXiv:1611.05013 [cs]},
  eprint = {1611.05013},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Natural image modeling is a landmark challenge of unsupervised learning. Variational Autoencoders (VAEs) learn a useful latent representation and model global structure well but have difficulty capturing small details. PixelCNN models details very well, but lacks a latent code and is difficult to scale for capturing large structures. We present PixelVAE, a VAE model with an autoregressive decoder based on PixelCNN. Our model requires very few expensive autoregressive layers compared to PixelCNN and learns latent codes that are more compressed than a standard VAE while still capturing most non-trivial structure. Finally, we extend our model to a hierarchy of latent variables at different scales. Our model achieves state-of-the-art performance on binarized MNIST, competitive performance on 64x64 ImageNet, and high-quality samples on the LSUN bedrooms dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/bestname/Zotero/storage/IIIGPABC/Gulrajani et al_2016_PixelVAE.pdf;/home/bestname/Zotero/storage/RZ59SA3Q/1611.html}
}

@book{hangos_IntelligentControlSystems_2001,
  title = {Intelligent {{Control Systems}}: {{An Introduction}} with {{Examples}}},
  shorttitle = {Intelligent {{Control Systems}}},
  author = {Hangos, Katalin M. and Szederk{\'e}nyi, G{\'a}bor and Lakner, R. and Gerzson, M.},
  year = {2001},
  publisher = {{Springer Science \& Business Media}},
  abstract = {Intelligent control is a rapidly developing, complex and challenging field with great practical importance and potential. Because of the rapidly developing and interdisciplinary nature of the subject, there are only a few edited volumes consisting of research papers on intelligent control systems but little is known and published about the fundamentals and the general know-how in designing, implementing and operating intelligent control systems.  Intelligent control system emerged from artificial intelligence and computer controlled systems as an interdisciplinary field. Therefore the book summarizes the fundamentals of knowledge representation, reasoning, expert systems and real-time control systems and then discusses the design, implementation verification and operation of real-time expert systems using G2 as an example. Special tools and techniques applied in intelligent control are also described including qualitative modelling, Petri nets and fuzzy controllers. The material is illlustrated with simple examples taken from the field of intelligent process control.  Audience: The book is suitable for advanced undergraduate students and graduate engineering students. In addition, practicing engineers will find it appropriate for self-study.},
  isbn = {978-1-4020-0134-5},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / Expert Systems,Computers / Artificial Intelligence / General,Computers / Information Technology,Computers / Software Development \& Engineering / General,Language Arts \& Disciplines / Library \& Information Science / General,Mathematics / Linear \& Nonlinear Programming,Science / System Theory,Social Science / General,Technology \& Engineering / Automation,Technology \& Engineering / Electrical,Technology \& Engineering / Mechanical}
}

@incollection{hansen_CMAEvolutionStrategy_2006,
  title = {The {{CMA Evolution Strategy}}: {{A Comparing Review}}},
  shorttitle = {The {{CMA Evolution Strategy}}},
  booktitle = {Towards a {{New Evolutionary Computation}}: {{Advances}} in the {{Estimation}} of {{Distribution Algorithms}}},
  author = {Hansen, Nikolaus},
  editor = {Lozano, Jose A. and Larra{\~n}aga, Pedro and Inza, I{\~n}aki and Bengoetxea, Endika},
  year = {2006},
  series = {Studies in {{Fuzziness}} and {{Soft Computing}}},
  pages = {75--102},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-32494-1_4},
  abstract = {Derived from the concept of self-adaptation in evolution strategies, the CMA (Covariance Matrix Adaptation) adapts the covariance matrix of a multi-variate normal search distribution. The CMA was originally designed to perform well with small populations. In this review, the argument starts out with large population sizes, reflecting recent extensions of the CMA algorithm. Commonalities and differences to continuous Estimation of Distribution Algorithms are analyzed. The aspects of reliability of the estimation, overall step size control, and independence from the coordinate system (invariance) become particularly important in small populations sizes. Consequently, performing the adaptation task with small populations is more intricate.},
  isbn = {978-3-540-32494-2},
  langid = {english},
  keywords = {Covariance Matrix,Distribution Algorithm,Evolution Path,Search Point,Step Length}
}

@misc{harder_VRNNProject_2021,
  title = {{{VRNN Project}}},
  author = {Harder, Frederik},
  year = {2021},
  month = apr,
  abstract = {MSc AI project on variational recurrent networks by J. Chung et al.}
}

@article{hazem_ComparisonSmoothingTechniques_,
  title = {A {{Comparison}} of {{Smoothing Techniques}} for {{Bilingual Lexicon Extraction}} from {{Comparable Corpora}}},
  author = {Hazem, Amir and Morin, Emmanuel},
  pages = {10},
  abstract = {Smoothing is a central issue in language modeling and a prior step in different natural language processing (NLP) tasks. However, less attention has been given to it for bilingual lexicon extraction from comparable corpora. If a first work to improve the extraction of low frequency words showed significant improvement while using distance-based averaging (Pekar et al., 2006), no investigation of the many smoothing techniques has been carried out so far. In this paper, we present a study of some widelyused smoothing algorithms for language n-gram modeling (Laplace, Good-Turing, Kneser-Ney...). Our main contribution is to investigate how the different smoothing techniques affect the performance of the standard approach (Fung, 1998) traditionally used for bilingual lexicon extraction. We show that using smoothing as a preprocessing step of the standard approach increases its performance significantly.},
  langid = {english},
  annotation = {ZSCC: 0000012},
  file = {/home/bestname/Zotero/storage/A9I39PHM/Hazem und Morin - A Comparison of Smoothing Techniques for Bilingual.pdf}
}

@misc{herkert_MultivariateTimeSeries_2022,
  title = {Multivariate {{Time Series Forecasting}} with {{Deep Learning}}},
  author = {Herkert, Daniel},
  year = {2022},
  month = jan,
  journal = {Medium},
  abstract = {Using LSTM networks for time series prediction and interpreting the results},
  howpublished = {https://towardsdatascience.com/multivariate-time-series-forecasting-with-deep-learning-3e7b3e2d2bcf},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/DXNJIMYL/multivariate-time-series-forecasting-with-deep-learning-3e7b3e2d2bcf.html}
}

@incollection{hinton_PracticalGuideTraining_2012,
  title = {A {{Practical Guide}} to {{Training Restricted Boltzmann Machines}}},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}: {{Second Edition}}},
  author = {Hinton, Geoffrey E.},
  editor = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  year = {2012},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {599--619},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-35289-8_32},
  abstract = {Restricted Boltzmann machines (RBMs) have been used as generative models of many different types of data. RBMs are usually trained using the contrastive divergence learning procedure. This requires a certain amount of practical experience to decide how to set the values of numerical meta-parameters. Over the last few years, the machine learning group at the University of Toronto has acquired considerable expertise at training RBMs and this guide is an attempt to share this expertise with other machine learning researchers.},
  isbn = {978-3-642-35289-8},
  langid = {english},
  keywords = {done,Hide Unit,Learning Rate,Reconstruction Error,Restrict Boltzmann Machine,Training Case},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/9LCK32ZV/Hinton - 2012 - A Practical Guide to Training Restricted Boltzmann.pdf}
}

@incollection{hitchcock_CausalModels_2020,
  ids = {hitchcock_CausalModels_2020a},
  title = {Causal {{Models}}},
  booktitle = {The {{Stanford Encyclopedia}} of {{Philosophy}}},
  author = {Hitchcock, Christopher},
  editor = {Zalta, Edward N.},
  year = {2020},
  edition = {Summer 2020},
  publisher = {{Metaphysics Research Lab, Stanford University}},
  abstract = {Causal models are mathematical models representing causalrelationships within an individual system or population. Theyfacilitate inferences about causal relationships from statisticaldata. They can teach us a good deal about the epistemology ofcausation, and about the relationship between causation andprobability. They have also been applied to topics of interest tophilosophers, such as the logic of counterfactuals, decision theory,and the analysis of actual causation.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/66XSDPF9/causal-models.html;/home/bestname/Zotero/storage/GFVZ4I3T/causal-models.html}
}

@article{hochreiter_LongShortTermMemory_1997,
  title = {Long {{Short-Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  month = nov,
  journal = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.8.1735},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}
}

@inproceedings{hompes_DiscoveringCausalFactors_2017,
  title = {Discovering {{Causal Factors Explaining Business Process Performance Variation}}},
  booktitle = {Advanced {{Information Systems Engineering}}},
  author = {Hompes, Bart F. A. and Maaradji, Abderrahmane and La Rosa, Marcello and Dumas, Marlon and Buijs, Joos C. A. M. and {van der Aalst}, Wil M. P.},
  editor = {Dubois, Eric and Pohl, Klaus},
  year = {2017},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {177--192},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-59536-8_12},
  abstract = {Business process performance may be affected by a range of factors, such as the volume and characteristics of ongoing cases or the performance and availability of individual resources. Event logs collected by modern information systems provide a wealth of data about the execution of business processes. However, extracting root causes for performance issues from these event logs is a major challenge. Processes may change continuously due to internal and external factors. Moreover, there may be many resources and case attributes influencing performance. This paper introduces a novel approach based on time series analysis to detect cause-effect relations between a range of business process characteristics and process performance indicators. The scalability and practical relevance of the approach has been validated by a case study involving a real-life insurance claims handling process.},
  isbn = {978-3-319-59536-8},
  langid = {english},
  keywords = {Performance analysis,Process mining,Root cause analysis},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/DFSF34T8/Hompes et al_2017_Discovering Causal Factors Explaining Business Process Performance Variation.pdf}
}

@misc{hr_MrhazVRNN_2019,
  title = {Mrhaz/{{VRNN}}},
  author = {HR},
  year = {2019},
  month = jun,
  abstract = {Variational recurrent neural network models}
}

@inproceedings{hsieh_DiCE4ELInterpretingProcess_2021,
  ids = {hsieh_DiCE4ELInterpretingProcess_2021a},
  title = {{{DiCE4EL}}: {{Interpreting Process Predictions}} Using a {{Milestone-Aware Counterfactual Approach}}},
  shorttitle = {{{DiCE4EL}}},
  booktitle = {2021 3rd {{International Conference}} on {{Process Mining}} ({{ICPM}})},
  author = {Hsieh, Chihcheng and Moreira, Catarina and Ouyang, Chun},
  year = {2021},
  month = oct,
  pages = {88--95},
  publisher = {{IEEE}},
  address = {{Eindhoven, Netherlands}},
  doi = {10.1109/ICPM53251.2021.9576881},
  abstract = {Predictive process analytics often apply machine learning to predict the future states of a running business process. However, the internal mechanisms of many existing predictive algorithms are opaque and a human decision-maker is unable to understand why a certain activity was predicted. Recently, counterfactuals have been proposed in the literature to derive human-understandable explanations from predictive models. Current counterfactual approaches consist of finding the minimum feature change that can make a certain prediction flip its outcome. Although many algorithms have been proposed, their application to multi-dimensional sequence data like event logs has not been explored in the literature.},
  isbn = {978-1-66543-514-7},
  langid = {english},
  keywords = {Counterfactual,done,explainable AI,hot,interpretability,Machine learning,Machine learning algorithms,Prediction algorithms,Predictive models,predictive process analytics,Protocols,Search problems,Training},
  annotation = {ZSCC: 0000001},
  file = {/home/bestname/Zotero/storage/HFUNGZXL/Hsieh et al_2021_DiCE4EL.pdf;/home/bestname/Zotero/storage/SKPNU5B3/Hsieh et al. - 2021 - DiCE4EL Interpreting Process Predictions using a .pdf;/home/bestname/Zotero/storage/R77U3DQB/9576881.html}
}

@misc{huls_WhyPythonSlow_2022,
  title = {Why {{Python}} Is so Slow and How to Speed It Up},
  author = {Huls, Mike},
  year = {2022},
  month = jul,
  journal = {Medium},
  abstract = {Take a look under the hood to see where Python's bottlenecks lie},
  howpublished = {https://towardsdatascience.com/why-is-python-so-slow-and-how-to-speed-it-up-485b5a84154e},
  langid = {english}
}

@misc{hundogan_ThesisProjectCode_2022,
  title = {Thesis {{Project}}: {{Code}} for {{CREATED}}},
  author = {Hundogan, Olusanmi},
  year = {2022},
  month = jan,
  copyright = {GPL-3.0}
}

@article{ismailfawaz_DeepLearningTime_2019,
  title = {Deep Learning for Time Series Classification: A Review},
  shorttitle = {Deep Learning for Time Series Classification},
  author = {Ismail Fawaz, Hassan and Forestier, Germain and Weber, Jonathan and Idoumghar, Lhassane and Muller, Pierre-Alain},
  year = {2019},
  month = jul,
  journal = {Data Mining and Knowledge Discovery},
  volume = {33},
  number = {4},
  pages = {917--963},
  issn = {1573-756X},
  doi = {10.1007/s10618-019-00619-1},
  abstract = {Time Series Classification (TSC) is an important and challenging problem in data mining. With the increase of time series data availability, hundreds of TSC algorithms have been proposed. Among these methods, only a few have considered Deep Neural Networks (DNNs) to perform this task. This is surprising as deep learning has seen very successful applications in the last years. DNNs have indeed revolutionized the field of computer vision especially with the advent of novel deeper architectures such as Residual and Convolutional Neural Networks. Apart from images, sequential data such as text and audio can also be processed with DNNs to reach state-of-the-art performance for document classification and speech recognition. In this article, we study the current state-of-the-art performance of deep learning algorithms for TSC by presenting an empirical study of the most recent DNN architectures for TSC. We give an overview of the most successful deep learning applications in various time series domains under a unified taxonomy of DNNs for TSC. We also provide an open source deep learning framework to the TSC community where we implemented each of the compared approaches and evaluated them on a univariate TSC benchmark (the UCR/UEA archive) and 12 multivariate time series datasets. By training 8730 deep learning models on 97 time series datasets, we propose the most exhaustive study of DNNs for TSC to date.},
  langid = {english},
  annotation = {ZSCC: 0001247},
  file = {/home/bestname/Zotero/storage/YARQQ2A6/Ismail Fawaz et al_2019_Deep learning for time series classification.pdf}
}

@article{jain_AttentionNotExplanation_2019,
  title = {Attention Is Not {{Explanation}}},
  author = {Jain, Sarthak and Wallace, Byron C.},
  year = {2019},
  month = may,
  journal = {arXiv:1902.10186 [cs]},
  eprint = {1902.10186},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work, we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful `explanations' for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do. Code for all experiments is available at https://github.com/successar/AttentionExplanation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {ZSCC: 0000467},
  file = {/home/bestname/Zotero/storage/MN9JMMLD/Jain_Wallace_2019_Attention is not Explanation.pdf;/home/bestname/Zotero/storage/N3TUYE9A/1902.html}
}

@article{jiang_CombiningEmbeddingbasedSymbolbased_2022,
  ids = {jiang_Combiningembeddingbasedsymbolbased_2022},
  title = {Combining Embedding-Based and Symbol-Based Methods for Entity Alignment},
  author = {Jiang, Tingting and Bu, Chenyang and Zhu, Yi and Wu, Xindong},
  year = {2022},
  month = apr,
  journal = {Pattern Recognition},
  volume = {124},
  pages = {108433},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2021.108433},
  abstract = {The objective of entity alignment is to judge whether entities refer to the same object in the real world. Methods for entity alignment can be grossly divided into two groups: conventional symbol-based entity alignment methods and embedding-based entity alignment methods. Both groups of methods have advantages and disadvantages (which are detailed in Section 1). Therefore, combining the advantages of both methods might be a promising strategy. However, to the best of our knowledge, only the RTEA algorithm that was proposed in our previous conference paper (Proceeding of Pacific Rim International Conference on Artificial Intelligence, pp. 162\textendash 175, 2019) utilizes this strategy for entity alignment. This manuscript is an extended version of that conference paper, in which an improved algorithm, namely, ESEA (combining embedding-based and symbol-based methods for entity alignment), is proposed based on the following steps. First, a novel method for combining embedding models with symbol-based models is proposed. Entities with high vector similarities are obtained through a hybrid embedding model, and the final aligned entity pairs are calculated via symbol-based methods. Second, a series of symbol-based methods, instead of only the edit distance method in the original version, are combined with embedding-based methods for relation alignment. Third, we combine symbol-based and embedding-based methods in a more complicated framework with the objective of better exploiting the advantages of both methods. The experimental results on real-world datasets demonstrate that the proposed method outperformed several state-of-the-art embedding-based entity alignment approaches and outperformed our previous RTEA method.},
  langid = {english},
  keywords = {Entity alignment,Knowledge graph embedding,String Similarity},
  file = {/home/bestname/Zotero/storage/4C3QHCP2/Jiang et al_2022_Combining embedding-based and symbol-based methods for entity alignment.pdf;/home/bestname/Zotero/storage/DKYX2TY5/S0031320321006099.html;/home/bestname/Zotero/storage/P52K6BKG/display.html}
}

@misc{jing_MusthaveTrainingTrick_2021,
  title = {A Must-Have Training Trick for {{VAE}}(Variational Autoencoder)},
  author = {Jing, Cheng},
  year = {2021},
  month = nov,
  journal = {MLearning.ai},
  abstract = {How Cyclical KL Annealing Schedule save your VAE model in a second.},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/SDM73SAY/a-must-have-training-trick-for-vae-variational-autoencoder-d28ff53b0023.html}
}

@article{Jokinen19961439,
  title = {A Comparison of Approximate String Matching Algorithms},
  author = {Jokinen, P. and Tarhio, J. and Ukkonen, E.},
  year = {1996},
  journal = {Software - Practice and Experience},
  volume = {26},
  number = {12},
  pages = {1439--1458},
  doi = {10.1002/(SICI)1097-024X(199612)26:12<1439::AID-SPE71>3.0.CO;2-1},
  document_type = {Review},
  source = {Scopus}
}

@article{jozefowicz_ExploringLimitsLanguage_2016,
  title = {Exploring the {{Limits}} of {{Language Modeling}}},
  author = {Jozefowicz, Rafal and Vinyals, Oriol and Schuster, Mike and Shazeer, Noam and Wu, Yonghui},
  year = {2016},
  month = feb,
  journal = {arXiv:1602.02410 [cs]},
  eprint = {1602.02410},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7. We also release these models for the NLP and ML community to study and improve upon.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,done},
  annotation = {ZSCC: 0000982},
  file = {/home/bestname/Zotero/storage/3WTT45A2/Jozefowicz et al. - 2016 - Exploring the Limits of Language Modeling.pdf;/home/bestname/Zotero/storage/DIKLRQHR/1602.html}
}

@article{jurafsky_SpeechLanguageProcessing_,
  title = {Speech and {{Language Processing}}},
  author = {Jurafsky, Daniel and Martin, James H and Norvig, Peter and Russell, Stuart},
  pages = {561},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s1]},
  file = {/home/bestname/Zotero/storage/QJE89FD3/Jurafsky et al. - Speech and Language Processing.pdf}
}

@article{kalman_NewApproachLinear_1960,
  title = {A {{New Approach}} to {{Linear Filtering}} and {{Prediction Problems}}},
  author = {Kalman, Rudolph Emil},
  year = {1960},
  journal = {Transactions of the ASME\textendash Journal of Basic Engineering},
  volume = {82},
  number = {Series D},
  pages = {35--45},
  keywords = {hot},
  annotation = {ZSCC: 0038460},
  file = {/home/bestname/Zotero/storage/72UBAQSU/Kalman1960.pdf}
}

@inproceedings{karimi_ModelAgnosticCounterfactualExplanations_2020,
  title = {Model-{{Agnostic Counterfactual Explanations}} for {{Consequential Decisions}}},
  booktitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Karimi, Amir-Hossein and Barthe, Gilles and Balle, Borja and Valera, Isabel},
  year = {2020},
  month = jun,
  pages = {895--905},
  publisher = {{PMLR}},
  issn = {2640-3498},
  langid = {english},
  keywords = {irrelevant},
  annotation = {ZSCC: 0000071},
  file = {/home/bestname/Zotero/storage/3JJDW3JU/Karimi et al_2020_Model-Agnostic Counterfactual Explanations for Consequential Decisions.pdf;/home/bestname/Zotero/storage/6SQ5DBI9/Karimi et al. - 2020 - Model-Agnostic Counterfactual Explanations for Con.pdf}
}

@article{kerremans_MarketGuideProcess_2019,
  title = {Market Guide for Process Mining},
  author = {Kerremans, Marc},
  year = {2019},
  journal = {Gartner Inc},
  abstract = {New forms of automation (e.g., robotic process automation) and knowledge of the underlying processes/interactions are key to digital transformation. Process mining helps enterprise architecture and technology innovation leaders assess operations and performance, increasing these initiatives' value.},
  lccn = {ID G00387812},
  annotation = {ZSCC: 0000017},
  file = {/home/bestname/Zotero/storage/RGB8X77N/market_guide_for_pm_gartner.pdf}
}

@article{khaleghi_ConsistentAlgorithmsClustering_2016,
  title = {Consistent {{Algorithms}} for {{Clustering Time Series}}},
  author = {Khaleghi, Azadeh and Ryabko, Daniil and Mary, J{\'e}r{\'e}mie and Preux, Philippe},
  year = {2016},
  journal = {Journal of Machine Learning Research},
  volume = {17},
  number = {3},
  pages = {1--32},
  issn = {1533-7928},
  abstract = {The problem of clustering is considered for the case where every point is a time series. The time series are either given in one batch (offline setting), or they are allowed to grow with time and new time series can be added along the way (online setting). We propose a natural notion of consistency for this problem, and show that there are simple, computationally efficient algorithms that are asymptotically consistent under extremely weak assumptions on the distributions that generate the data. The notion of consistency is as follows. A clustering algorithm is called consistent if it places two time series into the same cluster if and only if the distribution that generates them is the same. In the considered framework the time series are allowed to be highly dependent, and the dependence can have arbitrary form. If the number of clusters is known, the only assumption we make is that the (marginal) distribution of each time series is stationary ergodic. No parametric, memory or mixing assumptions are made. When the number of clusters is unknown, stronger assumptions are provably necessary, but it is still possible to devise nonparametric algorithms that are consistent under very general conditions. The theoretical findings of this work are illustrated with experiments on both synthetic and real data.},
  keywords = {skimmed},
  annotation = {ZSCC: 0000049},
  file = {/home/bestname/Zotero/storage/4Z69RFVJ/Khaleghi et al_2016_Consistent Algorithms for Clustering Time Series.pdf}
}

@article{kingma_AutoEncodingVariationalBayes_2014,
  ids = {kingma_AutoEncodingVariationalBayes_2014a},
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2014},
  month = may,
  journal = {arXiv:1312.6114 [cs, stat]},
  eprint = {1312.6114},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{Ithaca, NYarXiv.org}},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {ZSCC: 0018901},
  file = {/home/bestname/Zotero/storage/79P5F6VI/Kingma_Welling_2014_Auto-Encoding Variational Bayes.pdf;/home/bestname/Zotero/storage/PVC4GYJH/search.html}
}

@article{kingma_IntroductionVariationalAutoencoders_2019,
  ids = {kingma_IntroductionVariationalAutoencoders_2019a},
  title = {An {{Introduction}} to {{Variational Autoencoders}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2019},
  month = nov,
  journal = {Foundations and Trends\textregistered{} in Machine Learning},
  volume = {12},
  number = {4},
  pages = {307--392},
  publisher = {{Now Publishers, Inc.}},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000056},
  abstract = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,tutorial},
  annotation = {ZSCC: 0000697},
  file = {/home/bestname/Zotero/storage/J3XPIN78/Kingma_Welling_2019_An Introduction to Variational Autoencoders.pdf;/home/bestname/Zotero/storage/5QJNNIFW/MAL-056.html;/home/bestname/Zotero/storage/SVHSVARW/1906.html}
}

@article{klimek_LongtermSeriesForecasting_2021,
  ids = {klimek_LongtermSeriesForecasting_2021a},
  title = {Long-Term Series Forecasting with {{Query Selector}} -- Efficient Model of Sparse Attention},
  author = {Klimek, Jacek and Klimek, Jakub and Kraskiewicz, Witold and Topolewski, Mateusz},
  year = {2021},
  month = aug,
  journal = {arXiv:2107.08687 [cs]},
  eprint = {2107.08687},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Various modifications of TRANSFORMER were recently used to solve time-series forecasting problem. We propose Query Selector - an efficient, deterministic algorithm for sparse attention matrix. Experiments show it achieves state-of-the art results on ETT, Helpdesk and BPI'12 datasets.},
  archiveprefix = {arXiv},
  keywords = {Code Available,Computer Science - Machine Learning},
  annotation = {ZSCC: 0000001},
  file = {/home/bestname/Zotero/storage/5RGPVPYH/Klimek et al_2021_Long-term series forecasting with Query Selector -- efficient model of sparse.pdf;/home/bestname/Zotero/storage/8H8NYGFM/Klimek et al. - 2021 - Long-term series forecasting with Query Selector -.pdf;/home/bestname/Zotero/storage/CMJM2V4Q/2107.html;/home/bestname/Zotero/storage/KJ77LFCD/2107.html}
}

@inproceedings{koorn_LookingMeaningDiscovering_2020,
  title = {Looking for {{Meaning}}: {{Discovering Action-Response-Effect Patterns}} in~{{Business Processes}}},
  shorttitle = {Looking for {{Meaning}}},
  booktitle = {Business {{Process Management}}},
  author = {Koorn, Jelmer J. and Lu, Xixi and Leopold, Henrik and Reijers, Hajo A.},
  editor = {Fahland, Dirk and Ghidini, Chiara and Becker, J{\"o}rg and Dumas, Marlon},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {167--183},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-58666-9_10},
  abstract = {Process mining enables organizations to capture and improve their processes based on fact-based process execution data. A key question in the context of process improvement is how response s to an event (action) result in desired or undesired outcomes (effects). From a process perspective, this requires understanding the action-response patterns that occur. Current discovery techniques do not allow organizations to gain such insights. In this paper we present a novel approach to tackle this problem. We propose and formalize a technique to discover action-response-effect patterns. In this technique we use well-established statistical tests to uncover potential dependency relations between each response and its effect s on the cases. The goal of this technique is to provide organizations with processes that are: (1) appropriately represented, and (2) effectively filtered to show meaningful relations. The approach is evaluated on a real-world data set from a Dutch healthcare facility in the context of aggressive behavior of clients and the response s of caretakers.},
  isbn = {978-3-030-58666-9},
  langid = {english},
  keywords = {Effect measurement,Healthcare,Patterns,Process discovery},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/PHEFW86J/Koorn et al_2020_Looking for Meaning.pdf}
}

@misc{kostadinov_UnderstandingEncoderDecoderSequence_2019,
  title = {Understanding {{Encoder-Decoder Sequence}} to {{Sequence Model}}},
  author = {Kostadinov, Simeon},
  year = {2019},
  month = nov,
  journal = {Medium},
  abstract = {In this article, I will try to give a short and concise explanation of the sequence to sequence model which have recently achieved\ldots},
  langid = {english},
  keywords = {blog},
  annotation = {ZSCC: 0000012},
  file = {/home/bestname/Zotero/storage/XCC9SQEA/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346.html}
}

@inproceedings{krause_InteractingPredictionsVisual_2016,
  title = {Interacting with {{Predictions}}: {{Visual Inspection}} of {{Black-box Machine Learning Models}}},
  shorttitle = {Interacting with {{Predictions}}},
  booktitle = {Proceedings of the 2016 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Krause, Josua and Perer, Adam and Ng, Kenney},
  year = {2016},
  month = may,
  series = {{{CHI}} '16},
  pages = {5686--5697},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2858036.2858529},
  abstract = {Understanding predictive models, in terms of interpreting and identifying actionable insights, is a challenging task. Often the importance of a feature in a model is only a rough estimate condensed into one number. However, our research goes beyond these na\"ive estimates through the design and implementation of an interactive visual analytics system, Prospector. By providing interactive partial dependence diagnostics, data scientists can understand how features affect the prediction overall. In addition, our support for localized inspection allows data scientists to understand how and why specific datapoints are predicted as they are, as well as support for tweaking feature values and seeing how the prediction responds. Our system is then evaluated using a case study involving a team of data scientists improving predictive models for detecting the onset of diabetes from electronic medical records.},
  isbn = {978-1-4503-3362-7},
  keywords = {interactive machine learning,partial dependence,predictive modeling},
  annotation = {ZSCC: 0000326},
  file = {/home/bestname/Zotero/storage/RYD5E2M8/Krause et al_2016_Interacting with Predictions.pdf}
}

@article{krishnan_StructuredInferenceNetworks_2016,
  title = {Structured {{Inference Networks}} for {{Nonlinear State Space Models}}},
  author = {Krishnan, Rahul G. and Shalit, Uri and Sontag, David},
  year = {2016},
  month = dec,
  journal = {arXiv:1609.09869 [cs, stat]},
  eprint = {1609.09869},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Gaussian state space models have been used for decades as generative models of sequential data. They admit an intuitive probabilistic interpretation, have a simple functional form, and enjoy widespread adoption. We introduce a unified algorithm to efficiently learn a broad class of linear and non-linear state space models, including variants where the emission and transition distributions are modeled by deep neural networks. Our learning algorithm simultaneously learns a compiled inference network and the generative model, leveraging a structured variational approximation parameterized by recurrent neural networks to mimic the posterior distribution. We apply the learning algorithm to both synthetic and real-world datasets, demonstrating its scalability and versatility. We find that using the structured approximation to the posterior results in models with significantly higher held-out likelihood.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/6TP4HNA9/Krishnan et al_2016_Structured Inference Networks for Nonlinear State Space Models.pdf;/home/bestname/Zotero/storage/NJDSVZU5/1609.html}
}

@article{krishnan_StructuredInferenceNetworks_2017,
  title = {Structured {{Inference Networks}} for {{Nonlinear State Space Models}}},
  author = {Krishnan, Rahul and Shalit, Uri and Sontag, David},
  year = {2017},
  month = feb,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {31},
  number = {1},
  issn = {2374-3468},
  abstract = {Gaussian state space models have been used for decades as generative models of sequential data. They admit an intuitive probabilistic interpretation, have a simple functional form, and enjoy widespread adoption. We introduce a unified algorithm to efficiently learn a broad class of linear and non-linear state space models, including variants where the emission and transition distributions are modeled by deep neural networks. Our learning algorithm simultaneously learns a compiled inference network and the generative model, leveraging a structured variational approximation parameterized by recurrent neural networks to mimic the posterior distribution. We apply the learning algorithm to both synthetic and real-world datasets, demonstrating its scalability and versatility. We find that using the structured approximation to the posterior results in models with significantly higher held-out likelihood.},
  copyright = {Copyright (c)},
  langid = {english},
  keywords = {Hidden Markov Models,hot,M2M,named_model,skimmed},
  annotation = {ZSCC: 0000321},
  file = {/home/bestname/Zotero/storage/I47WD4QT/Krishnan et al_2017_Structured Inference Networks for Nonlinear State Space Models.pdf}
}

@misc{kubler_ExtensibleEvolutionaryAlgorithm_2021,
  title = {An Extensible {{Evolutionary Algorithm Example}} in {{Python}}},
  author = {K{\"u}bler, Dr Robert},
  year = {2021},
  month = aug,
  journal = {Medium},
  abstract = {Learning how to write an easy Evolutionary Algorithm from scratch in less than 50 lines of code that you can use for your projects.},
  howpublished = {https://towardsdatascience.com/an-extensible-evolutionary-algorithm-example-in-python-7372c56a557b},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/3IP5S636/an-extensible-evolutionary-algorithm-example-in-python-7372c56a557b.html}
}

@inproceedings{lambora_GeneticAlgorithmLiterature_2019,
  title = {Genetic {{Algorithm- A Literature Review}}},
  booktitle = {2019 {{International Conference}} on {{Machine Learning}}, {{Big Data}}, {{Cloud}} and {{Parallel Computing}} ({{COMITCon}})},
  author = {Lambora, Annu and Gupta, Kunal and Chopra, Kriti},
  year = {2019},
  month = feb,
  pages = {380--384},
  doi = {10.1109/COMITCon.2019.8862255},
  abstract = {Genetic Algorithm (GA) may be attributed as method for optimizing the search tool for difficult problems based on genetics selection principle. In additions to Optimization it also serves the purpose of machine learning and for Research and development. It is analogous to biology for chromosome generation with variables such as selection, crossover and mutation together constituting genetic operations which would be applicable on a random population initially. GA aims to yield solutions for the consecutive generations. The extent of success in individual production is directly in proportion to fitness of solution which is represented by it, thereby ensuring that quality in successive generations will be better. The process is concluded once an GA is most suitable for the issues that need optimization associated with some computable system.. John Holland may be regarded as funding father of original genetic algorithm and is attributed to year 1970's as funding date. Additionally a random search method represented by Charles Darwin for a defined search space in order to effetely solve a problem. In this paper, what is genetic algorithm and its basic workflow is discussed how a genetic algorithm work and what are the process is included in this is also discussed. Further, the features and application of genetic algorithm are mentioned in the paper.},
  keywords = {Biological cells,Crossover,Encoding,Genetic Algorithm,Genetic algorithms,Genetics,Inheritance,Mutation,Optimization,Selection,Sociology,Statistics},
  file = {/home/bestname/Zotero/storage/EMLKX9GE/Lambora et al_2019_Genetic Algorithm- A Literature Review.pdf}
}

@article{lea_TemporalConvolutionalNetworks_2016,
  title = {Temporal {{Convolutional Networks}}: {{A Unified Approach}} to {{Action Segmentation}}},
  shorttitle = {Temporal {{Convolutional Networks}}},
  author = {Lea, Colin and Vidal, Rene and Reiter, Austin and Hager, Gregory D.},
  year = {2016},
  month = aug,
  journal = {arXiv:1608.08242 [cs]},
  eprint = {1608.08242},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The dominant paradigm for video-based action segmentation is composed of two steps: first, for each frame, compute low-level features using Dense Trajectories or a Convolutional Neural Network that encode spatiotemporal information locally, and second, input these features into a classifier that captures high-level temporal relationships, such as a Recurrent Neural Network (RNN). While often effective, this decoupling requires specifying two separate models, each with their own complexities, and prevents capturing more nuanced long-range spatiotemporal relationships. We propose a unified approach, as demonstrated by our Temporal Convolutional Network (TCN), that hierarchically captures relationships at low-, intermediate-, and high-level time-scales. Our model achieves superior or competitive performance using video or sensor data on three public action segmentation datasets and can be trained in a fraction of the time it takes to train an RNN.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {ZSCC: 0000276},
  file = {/home/bestname/Zotero/storage/G3S5IFE5/Lea et al. - 2016 - Temporal Convolutional Networks A Unified Approac.pdf;/home/bestname/Zotero/storage/Y4UYN6LD/1608.html}
}

@incollection{lee_CounterFactualReinforcementLearning_2013,
  title = {Counter-{{Factual Reinforcement Learning}}: {{How}} to {{Model Decision-Makers That Anticipate}} the {{Future}}},
  shorttitle = {Counter-{{Factual Reinforcement Learning}}},
  booktitle = {Decision {{Making}} and {{Imperfection}}},
  author = {Lee, Ritchie and Wolpert, David H. and Bono, James and Backhaus, Scott and Bent, Russell and Tracey, Brendan},
  editor = {Guy, Tatiana V. and Karny, Miroslav and Wolpert, David},
  year = {2013},
  series = {Studies in {{Computational Intelligence}}},
  pages = {101--128},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-36406-8_4},
  abstract = {This chapter introduces a novel framework for modeling interacting humans in a multi-stage game. This ''iterated semi network-form game'' framework has the following desirable characteristics: (1) Bounded rational players, (2) strategic players (i.e., players account for one another's reward functions when predicting one another's behavior), and (3) computational tractability even on real-world systems. We achieve these benefits by combining concepts from game theory and reinforcement learning. To be precise, we extend the bounded rational ''level-K reasoning'' model to apply to games over multiple stages. Our extension allows the decomposition of the overall modeling problem into a series of smaller ones, each of which can be solved by standard reinforcement learning algorithms. We call this hybrid approach ''level-K reinforcement learning''. We investigate these ideas in a cyber battle scenario over a smart power grid and discuss the relationship between the behavior predicted by our model and what one might expect of real human defenders and attackers.},
  isbn = {978-3-642-36406-8},
  langid = {english},
  keywords = {irrelevant,Power Grid,Reactive Power,Reinforcement Learning,Reward Function,Solution Concept},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/568AVM36/Lee et al_2013_Counter-Factual Reinforcement Learning.pdf}
}

@misc{lee_TensorflowImplementingTemporal_2019,
  title = {[{{Tensorflow}}] {{Implementing Temporal Convolutional Networks}}},
  author = {Lee, Ceshine},
  year = {2019},
  month = mar,
  journal = {Veritable},
  abstract = {Understanding Tensorflow Part 3},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/UK5L7ZSG/notes-understanding-tensorflow-part-3-7f6633fcc7c7.html}
}

@incollection{leemans_EarthMoversStochastic_2019,
  title = {Earth {{Movers}}' {{Stochastic Conformance Checking}}},
  booktitle = {Business {{Process Management Forum}}},
  author = {Leemans, Sander J. J. and Syring, Anja F. and {van der Aalst}, Wil M. P.},
  editor = {Hildebrandt, Thomas and {van Dongen}, Boudewijn F. and R{\"o}glinger, Maximilian and Mendling, Jan},
  year = {2019},
  volume = {360},
  pages = {127--143},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-26643-1_8},
  abstract = {Process Mining aims to support Business Process Management (BPM) by extracting information about processes from real-life process executions recorded in event logs. In particular, conformance checking aims to measure the quality of a process model by quantifying differences between the model and an event log or another model. Even though event logs provide insights into the likelihood of observed behaviour, most state-of-the-art conformance checking techniques ignore this point of view. In this paper, we propose a conformance measure that considers the stochastic characteristics of both the event log and the process model. It is based on the ``earth movers' distance'' and measures the effort to transform the distributions of traces of the event log into the distribution of traces of the model. We formalize this intuitive conformance metric and provide an approximation and a simplified variant. The latter two have been implemented in ProM and we evaluate them using several real-life examples.},
  isbn = {978-3-030-26642-4 978-3-030-26643-1},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/NDACZE2R/Leemans et al. - 2019 - Earth Moversâ Stochastic Conformance Checking.pdf}
}

@inproceedings{leglaive_RecurrentVariationalAutoencoder_2020,
  ids = {leglaive_RecurrentVariationalAutoencoder_2020a},
  title = {A {{Recurrent Variational Autoencoder}} for {{Speech Enhancement}}},
  booktitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Leglaive, Simon and {Alameda-Pineda}, Xavier and Girin, Laurent and Horaud, Radu},
  year = {2020},
  month = feb,
  eprint = {1910.10942},
  eprinttype = {arxiv},
  pages = {371--375},
  issn = {2379-190X},
  doi = {10.1109/ICASSP40776.2020.9053164},
  abstract = {This paper presents a generative approach to speech enhancement based on a recurrent variational autoencoder (RVAE). The deep generative speech model is trained using clean speech signals only, and it is combined with a nonnegative matrix factorization noise model for speech enhancement. We propose a variational expectation-maximization algorithm where the encoder of the RVAE is fine-tuned at test time, to approximate the distribution of the latent variables given the noisy speech observations. Compared with previous approaches based on feed-forward fully-connected architectures, the proposed recurrent deep generative speech model induces a posterior temporal dynamic over the latent variables, which is shown to improve the speech enhancement results.},
  archiveprefix = {arXiv},
  keywords = {Approximation algorithms,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Heuristic algorithms,hot,Markov processes,named_model,Noise measurement,nonnegative matrix factorization,recurrent variational autoencoders,Signal processing,Signal processing algorithms,Speech enhancement,variational inference},
  annotation = {ZSCC: 0000023},
  file = {/home/bestname/Zotero/storage/JB5QY92T/Leglaive et al_2020_A Recurrent Variational Autoencoder for Speech Enhancement.pdf;/home/bestname/Zotero/storage/JI28KZJH/1910.html;/home/bestname/Zotero/storage/TLMDA54A/9053164.html}
}

@article{lempa_SurveyUseGenetic_2016,
  ids = {lempa2010survey},
  title = {A {{Survey}} on the {{Use}} of {{Genetic Algorithms}} in {{Natural Language Processing}}},
  author = {Lempa, Pawel and Ptaszynski, Michal and Masui, Fumito},
  year = {2016},
  journal = {Advanced Robotics},
  volume = {24},
  number = {5-6},
  pages = {2},
  abstract = {In this paper we present our survey on application of genetic algorithms in Natural Language Processing (NLP). We focus on main issues of NLP and describe research presented in papers related with this topics.},
  langid = {english},
  keywords = {dialogue systems,done,irrelevant,language generation,machine translation,review},
  file = {/home/bestname/Zotero/storage/QEWXY5WP/Lempa et al. - 2016 - A Survey on the Use of Genetic Algorithms in Natur.pdf}
}

@misc{lendave_HowMultivariateTime_2021,
  title = {How {{To Do Multivariate Time Series Forecasting Using LSTM}}},
  author = {Lendave, Vijaysinh},
  year = {2021},
  month = jul,
  journal = {Analytics India Magazine},
  abstract = {Time series forecasting is also an important area in machine learning. However, it is neglected due to its complexity, and this complexity.},
  langid = {american},
  annotation = {ZSCC: NoCitationData[s0]}
}

@article{levenshtein_BinaryCodesCapable_1965,
  title = {Binary Codes Capable of Correcting Deletions, Insertions, and Reversals},
  author = {Levenshtein, V.},
  year = {1965},
  journal = {undefined},
  abstract = {Semantic Scholar extracted view of \&quot;Binary codes capable of correcting deletions, insertions, and reversals\&quot; by V. Levenshtein},
  langid = {english},
  file = {/home/bestname/Zotero/storage/C7U66UK5/Levenshtein - 1965 - Binary codes capable of correcting deletions, inse.pdf;/home/bestname/Zotero/storage/UKNK3JBN/b2f8876482c97e804bb50a5e2433881ae31d0cdd.html}
}

@article{li_DisentangledSequentialAutoencoder_2018,
  title = {Disentangled {{Sequential Autoencoder}}},
  author = {Li, Yingzhen and Mandt, Stephan},
  year = {2018},
  month = jun,
  journal = {arXiv:1803.02991 [cs]},
  eprint = {1803.02991},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We present a VAE architecture for encoding and generating high dimensional sequential data, such as video or audio. Our deep generative model learns a latent representation of the data which is split into a static and dynamic part, allowing us to approximately disentangle latent time-dependent features (dynamics) from features which are preserved over time (content). This architecture gives us partial control over generating content and dynamics by conditioning on either one of these sets of features. In our experiments on artificially generated cartoon video clips and voice recordings, we show that we can convert the content of a given sequence into another one by such content swapping. For audio, this allows us to convert a male speaker into a female speaker and vice versa, while for video we can separately manipulate shapes and dynamics. Furthermore, we give empirical evidence for the hypothesis that stochastic RNNs as latent state models are more efficient at compressing and generating long sequences than deterministic ones, which may be relevant for applications in video compression.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,hot,M2M,named_model},
  annotation = {ZSCC: 0000087},
  file = {/home/bestname/Zotero/storage/3A6IZNPM/Li_Mandt_2018_Disentangled Sequential Autoencoder.pdf;/home/bestname/Zotero/storage/3PFLNLJA/1803.html}
}

@article{liu_SurveyContextualEmbeddings_2020,
  title = {A {{Survey}} on {{Contextual Embeddings}}},
  author = {Liu, Qi and Kusner, Matt J. and Blunsom, Phil},
  year = {2020},
  month = apr,
  journal = {arXiv:2003.07278 [cs]},
  eprint = {2003.07278},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Contextual embeddings, such as ELMo and BERT, move beyond global word representations like Word2Vec and achieve ground-breaking performance on a wide range of natural language processing tasks. Contextual embeddings assign each word a representation based on its context, thereby capturing uses of words across varied contexts and encoding knowledge that transfers across languages. In this survey, we review existing contextual embedding models, cross-lingual polyglot pre-training, the application of contextual embeddings in downstream tasks, model compression, and model analyses.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {ZSCC: 0000036},
  file = {/home/bestname/Zotero/storage/3UPAYQAE/Liu et al. - 2020 - A Survey on Contextual Embeddings.pdf;/home/bestname/Zotero/storage/MXQ8UHI9/2003.html}
}

@inproceedings{m.k_SurveySimilarityMeasures_2016,
  title = {A {{Survey}} on {{Similarity Measures}} in {{Text Mining}}},
  author = {M.K, Vijaymeena and Kavitha, K.},
  year = {2016},
  doi = {10.5121/MLAIJ.2016.3103},
  abstract = {Semantic Scholar extracted view of "A Survey on Similarity Measures in Text Mining" by Vijaymeena M.K et al.},
  keywords = {irrelevant},
  annotation = {ZSCC: 0000183},
  file = {/home/bestname/Zotero/storage/TMN2PAZB/M.K_Kavitha_2016_A Survey on Similarity Measures in Text Mining.pdf}
}

@inproceedings{malik_AdvOLMGeneratingTextual_2021,
  title = {Adv-{{OLM}}: {{Generating Textual Adversaries}} via {{OLM}}},
  shorttitle = {Adv-{{OLM}}},
  booktitle = {Proceedings of the 16th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Main Volume}}},
  author = {Malik, Vijit and Bhat, Ashwani and Modi, Ashutosh},
  year = {2021},
  month = apr,
  pages = {841--849},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  abstract = {Deep learning models are susceptible to adversarial examples that have imperceptible perturbations in the original input, resulting in adversarial attacks against these models. Analysis of these attacks on the state of the art transformers in NLP can help improve the robustness of these models against such adversarial inputs. In this paper, we present Adv-OLM, a black-box attack method that adapts the idea of Occlusion and Language Models (OLM) to the current state of the art attack methods. OLM is used to rank words of a sentence, which are later substituted using word replacement strategies. We experimentally show that our approach outperforms other attack methods for several text classification tasks.},
  keywords = {Pertubations,Supervised},
  annotation = {ZSCC: 0000002},
  file = {/home/bestname/Zotero/storage/IJMRNIDE/Malik et al_2021_Adv-OLM.pdf}
}

@misc{malteos_AwesomeDocumentSimilarity_2021,
  title = {Awesome {{Document Similarity Measures}}},
  author = {{malteos}},
  year = {2021},
  month = dec,
  abstract = {A curated list of resources on document similarity measures (papers, tutorials, code, ...)},
  copyright = {MIT},
  annotation = {ZSCC: NoCitationData[s0]}
}

@article{mannhardt_AnalyzingTrajectoriesPatients_2017,
  title = {Analyzing the Trajectories of Patients with Sepsis Using Process Mining: {{RADAR}} + {{EMISA}} 2017},
  shorttitle = {Analyzing the Trajectories of Patients with Sepsis Using Process Mining},
  author = {Mannhardt, F. and Blinde, D.},
  year = {2017},
  journal = {RADAR+EMISA 2017, Essen, Germany, June 12-13, 2017},
  series = {{{CEUR Workshop Proceedings}}},
  pages = {72--80},
  publisher = {{CEUR-WS.org}},
  abstract = {Process mining techniques analyze processes based on event data. We analyzed the trajectories of patients in a Dutch hospital from their registration in the emergency room until their discharge. We considered a sample of 1050 patients with symptoms of a sepsis condition, which is a life-threatening condition. We extracted an event log that includes events on activities in the emergency room, admission to hospital wards, and discharge. The event log was enriched with data from laboratory tests and triage checklists.We try to automatically discover a process model of the patient trajectories, we check conformance to medical guidelines for sepsis patients, and visualize the flow of patients on a de-jure process model. The lessons-learned from this analysis are: (1) process mining can be used to clarify the patient flow in a hospital; (2) process mining can be used to check the daily clinical practice against medical guidelines; (3) process discovery methods  may return unsuitable models that are difficult to understand for stakeholders; and (4) process mining is an iterative process, e.g., data quality issues are often discovered and need to be addressed.},
  keywords = {Medical guidelines,Patient trajectories,Process mining}
}

@article{manzoni_EvolutionarybasedApproachNatural_2020,
  title = {Towards an Evolutionary-Based Approach for Natural Language Processing},
  author = {Manzoni, Luca and Jakobovic, Domagoj and Mariot, Luca and Picek, Stjepan and Castelli, Mauro},
  year = {2020},
  month = apr,
  doi = {10.48550/arXiv.2004.13832},
  abstract = {Tasks related to Natural Language Processing (NLP) have recently been the focus of a large research endeavor by the machine learning community. The increased interest in this area is mainly due to the success of deep learning methods. Genetic Programming (GP), however, was not under the spotlight with respect to NLP tasks. Here, we propose a first proof-of-concept that combines GP with the well established NLP tool word2vec for the next word prediction task. The main idea is that, once words have been moved into a vector space, traditional GP operators can successfully work on vectors, thus producing meaningful words as the output. To assess the suitability of this approach, we perform an experimental evaluation on a set of existing newspaper headlines. Individuals resulting from this (pre-)training phase can be employed as the initial population in other NLP tasks, like sentence generation, which will be the focus of future investigations, possibly employing adversarial co-evolutionary approaches.},
  langid = {english},
  keywords = {Genetic programming,irrelevant},
  file = {/home/bestname/Zotero/storage/Y79E57T8/Manzoni et al_2020_Towards an evolutionary-based approach for natural language processing.pdf;/home/bestname/Zotero/storage/MYH7PHF3/2004.html}
}

@article{marcus_BuildingLargeAnnotated_1993,
  title = {Building a Large Annotated Corpus of {{English}}: The Penn Treebank},
  shorttitle = {Building a Large Annotated Corpus of {{English}}},
  author = {Marcus, Mitchell P. and Marcinkiewicz, Mary Ann and Santorini, Beatrice},
  year = {1993},
  month = jun,
  journal = {Computational Linguistics},
  volume = {19},
  number = {2},
  pages = {313--330},
  issn = {0891-2017},
  file = {/home/bestname/Zotero/storage/SSGZMEQ5/Marcus et al_1993_Building a large annotated corpus of English.pdf}
}

@article{martens_ExplainingDatadrivenDocument_2014,
  title = {Explaining Data-Driven Document Classifications},
  author = {Martens, David and Provost, Foster},
  year = {2014},
  month = mar,
  journal = {MIS Quarterly},
  volume = {38},
  number = {1},
  pages = {73--100},
  issn = {0276-7783},
  doi = {10.25300/MISQ/2014/38.1.04},
  abstract = {Many document classification applications require human understanding of the reasons for data-driven classification decisions by managers, client-facing employees, and the technical team. Predictive models treat documents as data to be classified, and document data are characterized by very high dimensionality, often with tens of thousands to millions of variables (words). Unfortunately, due to the high dimensionality, understanding the decisions made by document classifiers is very difficult. This paper begins by extending the most relevant prior theoretical model of explanations for intelligent systems to account for some missing elements. The main theoretical contribution is the definition of a new sort of explanation as a minimal set of words (terms, generally), such that removing all words within this set from the document changes the predicted class from the class of interest. We present an algorithm to find such explanations, as well as a framework to assess such an algorithm's performance. We demonstrate the value of the new approach with a case study from a real-world document classification task: classifying web pages as containing objectionable content, with the goal of allowing advertisers to choose not to have their ads appear on those pages. A second empirical demonstration on news-story topic classification shows the explanations to be concise and document-specific, and to be capable of providing understanding of the exact reasons for the classification decisions, of the workings of the classification models, and of the business application itself. We also illustrate how explaining the classifications of documents can help to improve data quality and model performance.},
  keywords = {comprehensibility,document classification,instance level explanation,text mining},
  annotation = {ZSCC: 0000232}
}

@inproceedings{mattei_LeveragingExactLikelihood_2018,
  title = {Leveraging the {{Exact Likelihood}} of {{Deep Latent Variable Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Mattei, Pierre-Alexandre and Frellsen, Jes},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  annotation = {ZSCC: 0000035},
  file = {/home/bestname/Zotero/storage/TPJUSQ4E/Mattei_Frellsen_2018_Leveraging the Exact Likelihood of Deep Latent Variable Models.pdf}
}

@misc{mccolgan_Tensorflowvrnn_2021,
  title = {Tensorflow-Vrnn},
  author = {McColgan, Thomas},
  year = {2021},
  month = aug,
  abstract = {A variational recurrent neural network implementation in tensorflow},
  keywords = {done,hot}
}

@article{meena_EvolutionaryAlgorithmsExtractive_2015,
  title = {Evolutionary {{Algorithms}} for {{Extractive Automatic Text Summarization}}},
  author = {Meena, Yogesh Kumar and Gopalani, Dinesh},
  year = {2015},
  month = jan,
  journal = {Procedia Computer Science},
  series = {International {{Conference}} on {{Computer}}, {{Communication}} and {{Convergence}} ({{ICCC}} 2015)},
  volume = {48},
  pages = {244--249},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2015.04.177},
  abstract = {Due to the exponential growth of documents on internet, users want all the relevant data at one place without any hassle. This led to the growth of Automatic Text Summarization. For extractive text summarization in which representative sentences from the document itself are selected as summary, various statistical, knowledge based and discourse based methods are proposed by researchers. The goal of this paper is to give a survey on the important techniques and methodologies that are employed using Genetic Algorithms in Automatic Text Summarization. This paper gives a review of the growth and improvement in the techniques of Automatic Text Summarization on implementing Evolutionary Algorithms techniques. We propose a broad set of features that considers additional features in the fitness function.},
  langid = {english},
  keywords = {Evolutionary,Extractive,Features,Genetic,irrelevant,Summarization,Term Frequency,text summarisation,Weights},
  file = {/home/bestname/Zotero/storage/J7XFRNEL/Meena_Gopalani_2015_Evolutionary Algorithms for Extractive Automatic Text Summarization.pdf;/home/bestname/Zotero/storage/8LKV7W22/S1877050915006869.html}
}

@article{melnyk_ImprovedNeuralText_2017,
  title = {Improved {{Neural Text Attribute Transfer}} with {{Non-parallel Data}}},
  author = {Melnyk, Igor and dos Santos, Cicero Nogueira and Wadhawan, Kahini and Padhi, Inkit and Kumar, Abhishek},
  year = {2017},
  month = dec,
  journal = {arXiv:1711.09395 [cs]},
  eprint = {1711.09395},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Text attribute transfer using non-parallel data requires methods that can perform disentanglement of content and linguistic attributes. In this work, we propose multiple improvements over the existing approaches that enable the encoder-decoder framework to cope with the text attribute transfer from non-parallel data. We perform experiments on the sentiment transfer task using two datasets. For both datasets, our proposed method outperforms a strong baseline in two of the three employed evaluation metrics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {ZSCC: 0000019},
  file = {/home/bestname/Zotero/storage/6FD8A7BV/Melnyk et al_2017_Improved Neural Text Attribute Transfer with Non-parallel Data.pdf;/home/bestname/Zotero/storage/NH3SAE42/1711.html}
}

@inproceedings{michel_EvaluationAdversarialPerturbations_2019,
  title = {On {{Evaluation}} of {{Adversarial Perturbations}} for {{Sequence-to-Sequence Models}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Michel, Paul and Li, Xian and Neubig, Graham and Pino, Juan},
  year = {2019},
  month = jun,
  pages = {3103--3114},
  publisher = {{Association for Computational Linguistics}},
  address = {{Minneapolis, Minnesota}},
  doi = {10.18653/v1/N19-1314},
  abstract = {Adversarial examples \textemdash{} perturbations to the input of a model that elicit large changes in the output \textemdash{} have been shown to be an effective way of assessing the robustness of sequence-to-sequence (seq2seq) models. However, these perturbations only indicate weaknesses in the model if they do not change the input so significantly that it legitimately results in changes in the expected output. This fact has largely been ignored in the evaluations of the growing body of related literature. Using the example of untargeted attacks on machine translation (MT), we propose a new evaluation framework for adversarial attacks on seq2seq models that takes the semantic equivalence of the pre- and post-perturbation input into account. Using this framework, we demonstrate that existing methods may not preserve meaning in general, breaking the aforementioned assumption that source side perturbations should not result in changes in the expected output. We further use this framework to demonstrate that adding additional constraints on attacks allows for adversarial perturbations that are more meaning-preserving, but nonetheless largely change the output sequence. Finally, we show that performing untargeted adversarial training with meaning-preserving attacks is beneficial to the model in terms of adversarial robustness, without hurting test performance. A toolkit implementing our evaluation framework is released at https://github.com/pmichel31415/teapot-nlp.},
  keywords = {Evaluation,irrelevant},
  annotation = {ZSCC: 0000061},
  file = {/home/bestname/Zotero/storage/E9E9BQRK/Michel et al_2019_On Evaluation of Adversarial Perturbations for Sequence-to-Sequence Models.pdf}
}

@article{Mitton20101,
  title = {Fifty Years of Spellchecking},
  author = {Mitton, R.},
  year = {2010},
  journal = {Writing Systems Research},
  volume = {2},
  number = {1},
  pages = {1--7},
  doi = {10.1093/wsr/wsq004},
  document_type = {Review},
  source = {Scopus},
  file = {/home/bestname/Zotero/storage/HFVFSXH7/Mitton_2010_Fifty years of spellchecking.pdf}
}

@misc{mollaysa_SummaryRecurrentLatent_2020,
  title = {Summary of the Recurrent Latent Variable Model: {{VRNN}}},
  shorttitle = {Summary of the Recurrent Latent Variable Model},
  author = {Mollaysa, Amina},
  year = {2020},
  month = apr,
  journal = {Medium},
  abstract = {In this article we will be focusing on some insights of the paper presented by Junyoung et all:},
  langid = {english},
  file = {/home/bestname/Zotero/storage/B76MQIYE/summary-of-the-recurrent-latent-variable-model-vrnn-4096b52e731.html}
}

@book{molnar_AdversarialExamplesInterpretable_,
  title = {6.2 {{Adversarial Examples}} | {{Interpretable Machine Learning}}},
  author = {Molnar, Christoph},
  abstract = {Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/H8UE4MDH/adversarial.html}
}

@book{molnar_ChapterExampleBasedExplanations_,
  title = {Chapter 6 {{Example-Based Explanations}} | {{Interpretable Machine Learning}}},
  author = {Molnar, Christoph},
  abstract = {Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/ZJIVIZPL/example-based.html}
}

@incollection{molnar_CounterfactualExplanations_,
  title = {Counterfactual {{Explanations}}},
  booktitle = {Interpretable Machine Learning. {{A Guide}} for {{Making Black Box Models Explainable}}},
  author = {Molnar, Christoph and Dandl, Susanne},
  langid = {english}
}

@book{molnar_CounterfactualExplanationsInterpretable_,
  title = {6.1 {{Counterfactual Explanations}} | {{Interpretable Machine Learning}}},
  author = {Molnar, Christoph},
  abstract = {Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable.},
  keywords = {XAI},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/3858VE5Q/Molnar_6.pdf;/home/bestname/Zotero/storage/3GKZ4WVW/counterfactual.html}
}

@book{molnar_InfluentialInstancesInterpretable_,
  title = {6.4 {{Influential Instances}} | {{Interpretable Machine Learning}}},
  author = {Molnar, Christoph},
  abstract = {Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/8D78KEI3/influential.html}
}

@book{molnar_PrototypesCriticismsInterpretable_,
  title = {6.3 {{Prototypes}} and {{Criticisms}} | {{Interpretable Machine Learning}}},
  author = {Molnar, Christoph},
  abstract = {Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/G6ZRC6EU/proto.html}
}

@book{molnar2019,
  ids = {molnar_InterpretableMachineLearning_},
  title = {Interpretable Machine Learning. {{A Guide}} for {{Making Black Box Models Explainable}}},
  author = {Molnar, Christoph},
  year = {2019},
  langid = {english},
  annotation = {ZSCC: 0001930},
  file = {/home/bestname/Zotero/storage/42DZ6UIM/Molnar - Interpretable Machine Learning.pdf}
}

@inproceedings{mothilal_ExplainingMachineLearning_2020,
  title = {Explaining Machine Learning Classifiers through Diverse Counterfactual Explanations},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Mothilal, Ramaravind K. and Sharma, Amit and Tan, Chenhao},
  year = {2020},
  month = jan,
  series = {{{FAT}}* '20},
  pages = {607--617},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3351095.3372850},
  abstract = {Post-hoc explanations of machine learning models are crucial for people to understand and act on algorithmic predictions. An intriguing class of explanations is through counterfactuals, hypothetical examples that show people how to obtain a different prediction. We posit that effective counterfactual explanations should satisfy two properties: feasibility of the counterfactual actions given user context and constraints, and diversity among the counterfactuals presented. To this end, we propose a framework for generating and evaluating a diverse set of counterfactual explanations based on determinantal point processes. To evaluate the actionability of counterfactuals, we provide metrics that enable comparison of counterfactual-based methods to other local explanation methods. We further address necessary tradeoffs and point to causal implications in optimizing for counterfactuals. Our experiments on four real-world datasets show that our framework can generate a set of counterfactuals that are diverse and well approximate local decision boundaries, outperforming prior approaches to generating diverse counterfactuals. We provide an implementation of the framework at https://github.com/microsoft/DiCE.},
  isbn = {978-1-4503-6936-7},
  annotation = {ZSCC: 0000268},
  file = {/home/bestname/Zotero/storage/PZ5WCKIC/Mothilal et al_2020_Explaining machine learning classifiers through diverse counterfactual.pdf}
}

@inproceedings{mothilal_ExplainingMachineLearning_2020a,
  title = {Explaining {{Machine Learning Classifiers}} through {{Diverse Counterfactual Explanations}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Mothilal, Ramaravind Kommiya and Sharma, Amit and Tan, Chenhao},
  year = {2020},
  month = jan,
  eprint = {1905.07697},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {607--617},
  doi = {10.1145/3351095.3372850},
  abstract = {Post-hoc explanations of machine learning models are crucial for people to understand and act on algorithmic predictions. An intriguing class of explanations is through counterfactuals, hypothetical examples that show people how to obtain a different prediction. We posit that effective counterfactual explanations should satisfy two properties: feasibility of the counterfactual actions given user context and constraints, and diversity among the counterfactuals presented. To this end, we propose a framework for generating and evaluating a diverse set of counterfactual explanations based on determinantal point processes. To evaluate the actionability of counterfactuals, we provide metrics that enable comparison of counterfactual-based methods to other local explanation methods. We further address necessary tradeoffs and point to causal implications in optimizing for counterfactuals. Our experiments on four real-world datasets show that our framework can generate a set of counterfactuals that are diverse and well approximate local decision boundaries, outperforming prior approaches to generating diverse counterfactuals. We provide an implementation of the framework at https://github.com/microsoft/DiCE.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/bestname/Zotero/storage/FP9RCINX/Mothilal et al_2020_Explaining Machine Learning Classifiers through Diverse Counterfactual.pdf;/home/bestname/Zotero/storage/INWJ3EH6/1905.html}
}

@inproceedings{narendra_CounterfactualReasoningProcess_2019,
  title = {Counterfactual {{Reasoning}} for {{Process Optimization Using Structural Causal Models}}},
  booktitle = {Business {{Process Management Forum}}},
  author = {Narendra, Tanmayee and Agarwal, Prerna and Gupta, Monika and Dechu, Sampath},
  editor = {Hildebrandt, Thomas and {van Dongen}, Boudewijn F. and R{\"o}glinger, Maximilian and Mendling, Jan},
  year = {2019},
  series = {Lecture {{Notes}} in {{Business Information Processing}}},
  pages = {91--106},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-26643-1_6},
  abstract = {Business processes are complex and involve the execution of various steps using different resources that can be shared across various tasks. Processes require analysis and process owners need to constantly look for methods to improve process performance indicators. It is non-trivial to quantify the improvement of a proposed change, without implementing or conducting randomized controlled trials. In several cases, the cost and time for implementing and evaluating the benefits of these changes are high. To address this, we propose a principled framework using Structural Causal Models which formally codify existing cause-effect assumptions about the process, control confounding and answer ``what if'' questions with observational data. We formally define an end to end methodology which takes process execution logs and specified BPMN model as inputs for structural causal model discovery and for performing counterfactual reasoning. We show that exploiting the process specification for causal discovery automatically ensures the inclusion of subject matter expertise, and also provides an effective computational methodology. We illustrate the effectiveness of our approach by answering intervention and counterfactual questions on example process models.},
  isbn = {978-3-030-26643-1},
  langid = {english},
  keywords = {Counterfactual reasoning,Process optimization,Process redesign,Structural causal model,What-if analysis},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/LFE9B4XV/Narendra et al_2019_Counterfactual Reasoning for Process Optimization Using Structural Causal Models.pdf}
}

@article{naumann_ConsequenceawareSequentialCounterfactual_2021,
  title = {Consequence-Aware {{Sequential Counterfactual Generation}}},
  author = {Naumann, Philip and Ntoutsi, Eirini},
  year = {2021},
  journal = {arXiv:2104.05592 [cs]},
  volume = {12976},
  eprint = {2104.05592},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {682--698},
  doi = {10.1007/978-3-030-86520-7_42},
  abstract = {Counterfactuals have become a popular technique nowadays for interacting with black-box machine learning models and understanding how to change a particular instance to obtain a desired outcome from the model. However, most existing approaches assume instant materialization of these changes, ignoring that they may require effort and a specific order of application. Recently, methods have been proposed that also consider the order in which actions are applied, leading to the so-called sequential counterfactual generation problem. In this work, we propose a model-agnostic method for sequential counterfactual generation. We formulate the task as a multi-objective optimization problem and present a genetic algorithm approach to find optimal sequences of actions leading to the counterfactuals. Our cost model considers not only the direct effect of an action, but also its consequences. Experimental results show that compared to state-of-the-art, our approach generates less costly solutions, is more efficient and provides the user with a diverse set of solutions to choose from.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,hot},
  annotation = {ZSCC: 0000001},
  file = {/home/bestname/Zotero/storage/9MTEDR9K/Naumann_Ntoutsi_2021_Consequence-aware Sequential Counterfactual Generation.pdf;/home/bestname/Zotero/storage/UPIVJZ3J/2104.html}
}

@article{neuberg_CAUSALITYMODELSREASONING_2003,
  title = {{{CAUSALITY}}: {{MODELS}}, {{REASONING}}, {{AND INFERENCE}}, by {{Judea Pearl}}, {{Cambridge University Press}}, 2000},
  shorttitle = {{{CAUSALITY}}},
  author = {Neuberg, Leland Gerson},
  year = {2003},
  month = aug,
  journal = {Econometric Theory},
  volume = {19},
  number = {4},
  pages = {675--685},
  publisher = {{Cambridge University Press}},
  issn = {1469-4360, 0266-4666},
  doi = {10.1017/S0266466603004109},
  abstract = {This book seeks to integrate research on cause and effect inference  from cognitive science, econometrics, epidemiology, philosophy, and  statistics. It puts forward the work of its author, his collaborators,  and others over the past two decades as a new account of cause and  effect inference that can aid practical researchers in many fields,  including econometrics. Pearl adheres to several propositions on cause  and effect inference. Though cause and effect relations are  fundamentally deterministic (he explicitly excludes quantum mechanical  phenomena from his concept of cause and effect), cause and effect  analysis involves probability language. Probability language helps to  convey uncertainty about cause and effect relations but is insufficient  to fully express those relations. In addition to conditional  probabilities of events, cause and effect analysis requires graphs or  diagrams and a language that distinguishes intervention or manipulation  from observation. Cause and effect analysis also requires  counterfactual reasoning and causal assumptions in addition to  observations and statistical assumptions.},
  langid = {english},
  file = {/home/bestname/Zotero/storage/48ISWUIJ/Neuberg_2003_CAUSALITY.pdf;/home/bestname/Zotero/storage/M9IWHAXH/DA2D9ABB0AD3DAC95AE7B3081FCDF139.html}
}

@inproceedings{nolle_BINetMultivariateBusiness_2018,
  title = {{{BINet}}: {{Multivariate Business Process Anomaly Detection Using Deep Learning}}},
  shorttitle = {{{BINet}}},
  booktitle = {Business {{Process Management}}},
  author = {Nolle, Timo and Seeliger, Alexander and M{\"u}hlh{\"a}user, Max},
  editor = {Weske, Mathias and Montali, Marco and Weber, Ingo and {vom Brocke}, Jan},
  year = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {271--287},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-98648-7_16},
  abstract = {In this paper, we propose BINet, a neural network architecture for real-time multivariate anomaly detection in business process event logs. BINet has been designed to handle both the control flow and the data perspective of a business process. Additionally, we propose a heuristic for setting the threshold of an anomaly detection algorithm automatically. We demonstrate that BINet can be used to detect anomalies in event logs not only on a case level, but also on event attribute level. We compare BINet to 6 other state-of-the-art anomaly detection algorithms and evaluate their performance on an elaborate data corpus of 60 synthetic and 21 real life event logs using artificial anomalies. BINet reached an average F1F1F\_1 score over all detection levels of 0.83, whereas the next best approach, a denoising autoencoder, reached only 0.74. This F1F1F\_1 score is calculated over two different levels of detection, namely case and attribute level. BINet reached 0.84 on case and 0.82 on attribute level, whereas the next best approach reached 0.78 and 0.71 respectively.},
  isbn = {978-3-319-98648-7},
  langid = {english},
  keywords = {Anomaly detection,Artificial process intelligence,Business process management,Deep learning,Recurrent neural networks},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/ES8IV56F/Nolle et al_2018_BINet.pdf}
}

@misc{nptel-nociitm_DeepLearningPart_2019,
  title = {Deep {{Learning Part}} - {{II}} ({{CS7015}}): {{Lec}} 21.2 {{Masked Autoencoder Density Estimator}} ({{MADE}})},
  shorttitle = {Deep {{Learning Part}} - {{II}} ({{CS7015}})},
  author = {{NPTEL-NOC IITM}},
  year = {2019},
  month = apr,
  abstract = {Deep Learning Part - II (CS7015): Lec 21.2 Masked Autoencoder Density Estimator (MADE)},
  annotation = {ZSCC: NoCitationData[s0]}
}

@article{oberst_CounterfactualOffPolicyEvaluation_2019,
  ids = {oberst_CounterfactualOffPolicyEvaluation_2019a},
  title = {Counterfactual {{Off-Policy Evaluation}} with {{Gumbel-Max Structural Causal Models}}},
  author = {Oberst, Michael and Sontag, David},
  year = {2019},
  month = jun,
  journal = {Proceedings of the 36th International Conference on Machine Learning},
  eprint = {1905.05824},
  eprinttype = {arxiv},
  issn = {4881-4890},
  doi = {10.48550/arXiv.1905.05824},
  abstract = {We introduce an off-policy evaluation procedure for highlighting episodes where applying a reinforcement learned (RL) policy is likely to have produced a substantially different outcome than the observed policy. In particular, we introduce a class of structural causal models (SCMs) for generating counterfactual trajectories in finite partially observable Markov Decision Processes (POMDPs). We see this as a useful procedure for off-policy "debugging" in high-risk settings (e.g., healthcare); by decomposing the expected difference in reward between the RL and observed policy into specific episodes, we can identify episodes where the counterfactual difference in reward is most dramatic. This in turn can be used to facilitate review of specific episodes by domain experts. We demonstrate the utility of this procedure with a synthetic environment of sepsis management.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,hot,skimmed,Statistics - Machine Learning},
  annotation = {ZSCC: 0000061},
  file = {/home/bestname/Zotero/storage/PTY3DHVM/Oberst and Sontag - 2019 - Counterfactual Off-Policy Evaluation with Gumbel-M.pdf;/home/bestname/Zotero/storage/XTWV66VS/1905.html}
}

@article{olszewski_GeneralizedFeatureExtraction_2001,
  title = {Generalized Feature Extraction for Structural Pattern Recognition in Time-Series Data},
  author = {Olszewski, R. and Maxion, R. and Siewiorek, D.},
  year = {2001},
  journal = {undefined},
  abstract = {The ability of the suite of structure detectors to generate features useful for structural pattern recognition is evaluated by comparing the classification accuracies achieved when using the structure detectors versus commonly-used statistical feature extractors, thus demonstrating that the suiteOf structure detectors effectively performs generalized feature extraction forStructural pattern recognition in time-series data. Pattern recognition encompasses two fundamental tasks: description and classification. Given an object to analyze, a pattern recognition system first generates a description of it (i.e., the pattern) and then classifies the object based on that description (i.e., the recognition). Two general approaches for implementing pattern recognition systems, statistical and structural, employ different techniques for description and classification. Statistical approaches to pattern recognition use decision-theoretic concepts to discriminate among objects belonging to different groups based upon their quantitative features. Structural approaches to pattern recognition use syntactic grammars to discriminate among objects belonging to different groups based upon the arrangement of their morphological (i.e., shape-based or structural) features. Hybrid approaches to pattern recognition combine aspects of both statistical and structural pattern recognition.  Structural pattern recognition systems are difficult to apply to new domains because implementation of both the description and classification tasks requires domain knowledge. Knowledge acquisition techniques necessary to obtain domain knowledge from experts are tedious and often fail to produce a complete and accurate knowledge base. Consequently, applications of structural pattern recognition have been primarily restricted to domains in which the set of useful morphological features has been established in the literature (e.g., speech recognition and character recognition) and the syntactic grammars can be composed by hand (e.g., electrocardiogram diagnosis). To overcome this limitation, a domain-independent approach to structural pattern recognition is needed that is capable of extracting morphological features and performing classification without relying on domain knowledge. A hybrid system that employs a statistical classification technique to perform discrimination based on structural features is a natural solution. While a statistical classifier is inherently domain independent, the domain knowledge necessary to support the description task can be eliminated with a set of generally-useful morphological features. Such a set of morphological features is suggested as the foundation for the development of a suite of structure detectors to perform generalized feature extraction for structural pattern recognition in time-series data.  The ability of the suite of structure detectors to generate features useful for structural pattern recognition is evaluated by comparing the classification accuracies achieved when using the structure detectors versus commonly-used statistical feature extractors. Two real-world databases with markedly different characteristics and established ground truth serve as sources of data for the evaluation. The classification accuracies achieved using the features extracted by the structure detectors were consistently as good as or better than the classification accuracies achieved when using the features generated by the statistical feature extractors, thus demonstrating that the suite of structure detectors effectively performs generalized feature extraction for structural pattern recognition in time-series data.},
  langid = {english},
  keywords = {skimmed},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/NGUGFC92/Olszewski et al_2001_Generalized feature extraction for structural pattern recognition in.pdf;/home/bestname/Zotero/storage/NC8ZTTTH/d74fa52a7ddd41b175378dbe0604e635dab8a708.html}
}

@misc{omray_IntroductionNormalizingFlows_2021,
  title = {Introduction to {{Normalizing Flows}}},
  author = {Omray, Aryansh},
  year = {2021},
  month = jul,
  journal = {Medium},
  abstract = {Why and how to implement normalizing flows over GANs and VAEs},
  howpublished = {https://towardsdatascience.com/introduction-to-normalizing-flows-d002af262a4b},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/MU5LM2HL/introduction-to-normalizing-flows-d002af262a4b.html}
}

@article{oord_PixelRecurrentNeural_2016,
  title = {Pixel {{Recurrent Neural Networks}}},
  author = {van den Oord, Aaron and Kalchbrenner, Nal and Kavukcuoglu, Koray},
  year = {2016},
  month = aug,
  journal = {arXiv:1601.06759 [cs]},
  eprint = {1601.06759},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  annotation = {ZSCC: 0001610},
  file = {/home/bestname/Zotero/storage/5IVW9HK6/Oord et al_2016_Pixel Recurrent Neural Networks.pdf;/home/bestname/Zotero/storage/CCPGPVSG/1601.html}
}

@article{oord_WaveNetGenerativeModel_2016,
  title = {{{WaveNet}}: {{A Generative Model}} for {{Raw Audio}}},
  shorttitle = {{{WaveNet}}},
  author = {van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  year = {2016},
  month = sep,
  journal = {arXiv:1609.03499 [cs]},
  eprint = {1609.03499},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound},
  annotation = {ZSCC: 0003297},
  file = {/home/bestname/Zotero/storage/TD83MDLF/Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf;/home/bestname/Zotero/storage/NDXFNTH7/1609.html}
}

@article{otten_EventGenerationStatistical_2021,
  title = {Event Generation and Statistical Sampling for Physics with Deep Generative Models and a Density Information Buffer},
  author = {Otten, Sydney and Caron, Sascha and {de Swart}, Wieske and {van Beekveld}, Melissa and Hendriks, Luc and {van Leeuwen}, Caspar and Podareanu, Damian and {Ruiz de Austri}, Roberto and Verheyen, Rob},
  year = {2021},
  month = may,
  journal = {Nature Communications},
  volume = {12},
  number = {1},
  pages = {2985},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-22616-z},
  abstract = {Simulating nature and in particular processes in particle physics require expensive computations and sometimes would take much longer than scientists can afford. Here, we explore ways to a solution for this problem by investigating recent advances in generative modeling and present a study for the generation of events from a physical process with deep generative models. The simulation of physical processes requires not only the production of physical events, but to also ensure that these events occur with the correct frequencies. We investigate the feasibility of learning the event generation and the frequency of occurrence with several generative machine learning models to produce events like Monte Carlo generators. We study three processes: a simple two-body decay, the processes e+e-\,\textrightarrow\,Z\,\textrightarrow\,l+l- and \$\$pp\textbackslash to t\textbackslash bar\{t\}\$\$including the decay of the top quarks and a simulation of the detector response. By buffering density information of encoded Monte Carlo events given the encoder of a Variational Autoencoder we are able to construct a prior for the sampling of new events from the decoder that yields distributions that are in very good agreement with real Monte Carlo events and are generated several orders of magnitude faster. Applications of this work include generic density estimation and sampling, targeted event generation via a principal component analysis of encoded ground truth data, anomaly detection and more efficient importance sampling, e.g., for the phase space integration of matrix elements in quantum field theories.},
  copyright = {2021 The Author(s)},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]  Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Characterization and analytical techniques;Information theory and computation;Statistics Subject\_term\_id: characterization-and-analytical-techniques;information-theory-and-computation;statistics},
  file = {/home/bestname/Zotero/storage/RICY2KYN/Otten et al. - 2021 - Event generation and statistical sampling for phys.pdf;/home/bestname/Zotero/storage/49CST9A6/s41467-021-22616-z.html}
}

@inproceedings{otto_EfficientEvolutionaryDecoding_2004,
  title = {Towards an {{Efficient Evolutionary Decoding Algorithm}} for {{Statistical Machine Translation}}},
  booktitle = {{{MICAI}} 2004: {{Advances}} in {{Artificial Intelligence}}},
  author = {Otto, Eridan and Riff, Mar{\'i}a Cristina},
  editor = {Monroy, Ra{\'u}l and {Arroyo-Figueroa}, Gustavo and Sucar, Luis Enrique and Sossa, Humberto},
  year = {2004},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {438--447},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-24694-7_45},
  abstract = {In a statistical machine translation system (SMTS), decoding is the process of finding the most likely translation based on a statistical model according to previously learned parameters. This paper proposes a new approach based on evolutionary hybrid algorithms to translate sentences in a specific technical context. The tests are carried out in the context of Spanish and then translated to English. The experimental results validate the performance of our method.},
  isbn = {978-3-540-24694-7},
  langid = {english},
  keywords = {Alignment Structure,irrelevant,Machine Translation,Target Sentence,Target Word,Translation Model},
  file = {/home/bestname/Zotero/storage/2KBMRMG4/Otto_Riff_2004_Towards an Efficient Evolutionary Decoding Algorithm for Statistical Machine.pdf}
}

@book{pearl_Causality_2009,
  title = {Causality},
  author = {Pearl, Judea},
  year = {2009},
  edition = {Second},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9780511803161},
  abstract = {Written by one of the preeminent researchers in the field, this book provides a comprehensive exposition of modern analysis of causation. It shows how causality has grown from a nebulous concept into a mathematical theory with significant applications in the fields of statistics, artificial intelligence, economics, philosophy, cognitive science, and the health and social sciences. Judea Pearl presents and unifies the probabilistic, manipulative, counterfactual, and structural approaches to causation and devises simple mathematical tools for studying the relationships between causal connections and statistical associations. Cited in more than 2,100 scientific publications, it continues to liberate scientists from the traditional molds of statistical thinking. In this revised edition, Judea Pearl elucidates thorny issues, answers readers' questions, and offers a panoramic view of recent advances in this field of research. Causality will be of interest to students and professionals in a wide variety of fields. Dr Judea Pearl has received the 2011 Rumelhart Prize for his leading research in Artificial Intelligence (AI) and systems from The Cognitive Science Society.},
  isbn = {978-0-521-89560-6},
  file = {/home/bestname/Zotero/storage/GDU22QP8/Pearl_2009_Causality.pdf;/home/bestname/Zotero/storage/RWP8PLXT/B0046844FAE10CBF274D4ACBDAEB5F5B.html}
}

@book{pearlCausalInferenceStatistics2016,
  title = {Causal Inference in Statistics: A Primer},
  shorttitle = {Causal Inference in Statistics},
  author = {Pearl, Judea and Glymour, Madelyn and Jewell, Nicholas P.},
  year = {2016},
  publisher = {{Wiley}},
  address = {{Chichester, West Sussex}},
  isbn = {978-1-119-18684-7},
  langid = {english},
  lccn = {QA276.A2 P43 2016},
  keywords = {Causation,Graphical Models,Mathematical statistics,Probabilities},
  annotation = {ZSCC: 0000942},
  file = {/home/bestname/Zotero/storage/YZDLFYU9/Pearl - Causal Inference in Statistics.pdf}
}

@inproceedings{qafari_CaseLevelCounterfactual_2021,
  ids = {qafari_CaseLevelCounterfactual_2021a},
  title = {Case {{Level Counterfactual Reasoning}} in {{Process Mining}}},
  booktitle = {Intelligent {{Information Systems}}},
  author = {Qafari, Mahnaz Sadat and {van der Aalst}, Wil M. P.},
  editor = {Nurcan, Selmin and Korthaus, Axel},
  year = {2021},
  series = {Lecture {{Notes}} in {{Business Information Processing}}},
  pages = {55--63},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-79108-7_7},
  abstract = {Process mining is widely used to diagnose processes and uncover performance and compliance problems. It is also possible to see relations between different behavioral aspects, e.g., cases that deviate more at the beginning of the process tend to get delayed in the later part of the process. However, correlations do not necessarily reveal causalities. Moreover, standard process mining diagnostics do not indicate how to improve the process. This is the reason we advocate the use of structural equation models and counterfactual reasoning. We use results from causal inference and adapt these to be able to reason over event logs and process interventions. We have implemented the approach as a ProM plug-in and have evaluated it on several data sets.},
  isbn = {978-3-030-79108-7},
  langid = {english},
  keywords = {Counterfactual statement,hot,Process mining,Structural equation model},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/G72NX9D2/Qafari_van der Aalst_2021_Case Level Counterfactual Reasoning in Process Mining.pdf}
}

@inproceedings{qian_XNLPLivingSurvey_2021,
  title = {{{XNLP}}: {{A Living Survey}} for {{XAI Research}} in {{Natural Language Processing}}},
  shorttitle = {{{XNLP}}},
  booktitle = {26th {{International Conference}} on {{Intelligent User Interfaces}}},
  author = {Qian, Kun and Danilevsky, Marina and Katsis, Yannis and Kawas, Ban and Oduor, Erick and Popa, Lucian and Li, Yunyao},
  year = {2021},
  month = apr,
  series = {{{IUI}} '21},
  pages = {78--80},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3397482.3450728},
  abstract = {We present XNLP: an interactive browser-based system embodying a living survey of recent state-of-the-art research in the field of Explainable AI (XAI) within the domain of Natural Language Processing (NLP). The system visually organizes and illustrates XAI-NLP publications and distills their content to allow users to gain insights, generate ideas, and explore the field. We hope that XNLP can become a leading demonstrative example of a living survey, balancing the depth and quality of a traditional well-constructed survey paper with the collaborative dynamism of a widely available interactive tool. XNLP can be accessed at: https://xainlp2020.github.io/xainlp.},
  isbn = {978-1-4503-8018-8},
  keywords = {Explainable AI,interactive survey,natural language processing,XAI},
  annotation = {ZSCC: 0000001},
  file = {/home/bestname/Zotero/storage/B2UGPC2Z/Qian et al_2021_XNLP.pdf}
}

@article{rabiner_IntroductionHiddenMarkov_1986,
  title = {An Introduction to Hidden {{Markov}} Models},
  author = {Rabiner, L. and Juang, B.},
  year = {1986},
  month = jan,
  journal = {IEEE ASSP Magazine},
  volume = {3},
  number = {1},
  pages = {4--16},
  issn = {1558-1284},
  doi = {10.1109/MASSP.1986.1165342},
  abstract = {The basic theory of Markov chains has been known to mathematicians and engineers for close to 80 years, but it is only in the past decade that it has been applied explicitly to problems in speech processing. One of the major reasons why speech models, based on Markov chains, have not been developed until recently was the lack of a method for optimizing the parameters of the Markov model to match observed signal patterns. Such a method was proposed in the late 1960's and was immediately applied to speech processing in several research institutions. Continued refinements in the theory and implementation of Markov modelling techniques have greatly enhanced the method, leading to a wide range of applications of these models. It is the purpose of this tutorial paper to give an introduction to the theory of Markov models, and to illustrate how they have been applied to problems in speech recognition.},
  keywords = {Fluctuations,Hidden Markov models,Linear systems,Mathematical model,Optimization methods,Pattern matching,Speech processing,Speech recognition,Time varying systems},
  annotation = {ZSCC: 0006244},
  file = {/home/bestname/Zotero/storage/6HACKMXF/1165342.html}
}

@misc{ramani_DeepCausalGenerative_2020,
  title = {Deep {{Causal Generative Modelling}} \textemdash{} {{A Brief Tutorial}}},
  author = {Ramani, Harish},
  year = {2020},
  month = sep,
  journal = {The Startup},
  abstract = {Integrating a causal model into a deep learning architecture.},
  langid = {english},
  keywords = {hot},
  annotation = {ZSCC: NoCitationData[s0]}
}

@inproceedings{robeer_GeneratingRealisticNatural_2021,
  ids = {robeer_GeneratingRealisticNatural_},
  title = {Generating {{Realistic Natural Language Counterfactuals}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2021},
  author = {Robeer, Marcel and Bex, Floris and Feelders, Ad},
  year = {2021},
  month = nov,
  pages = {3611--3625},
  publisher = {{Association for Computational Linguistics}},
  address = {{Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.findings-emnlp.306},
  abstract = {Counterfactuals are a valuable means for understanding decisions made by ML systems. However, the counterfactuals generated by the methods currently available for natural language text are either unrealistic or introduce imperceptible changes. We propose CounterfactualGAN: a method that combines a conditional GAN and the embeddings of a pretrained BERT encoder to model-agnostically generate realistic natural language text counterfactuals for explaining regression and classification tasks. Experimental results show that our method produces perceptibly distinguishable counterfactuals, while outperforming four baseline methods on fidelity and human judgments of naturalness, across multiple datasets and multiple predictive models.},
  langid = {english},
  keywords = {required,skimmed},
  annotation = {ZSCC: 0000001},
  file = {/home/bestname/Zotero/storage/A88SBILJ/Robeer et al. - Generating Realistic Natural Language Counterfactu.pdf;/home/bestname/Zotero/storage/R8ERX49Y/Robeer et al_2021_Generating Realistic Natural Language Counterfactuals.pdf}
}

@article{rodriguez_ApplicationDifferentEvolutionary_2008,
  title = {On the Application of Different Evolutionary Algorithms to the Alignment Problem in Statistical Machine Translation},
  author = {Rodr{\'i}guez, Luis and {Garc{\'i}a-Varea}, Ismael and G{\'a}mez, Jos{\'e} A.},
  year = {2008},
  month = jan,
  journal = {Neurocomputing},
  series = {Neural {{Networks}}: {{Algorithms}} and {{Applications}}},
  volume = {71},
  number = {4},
  pages = {755--765},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2007.10.006},
  abstract = {In statistical machine translation, an alignment defines a mapping between the words in the source and in the target sentence. Alignments are used, on the one hand, to train the statistical models and, on the other, during the decoding process to link the words in the source sentence to the words in the partial hypotheses generated. In both cases, the quality of the alignments is crucial for the success of the translation process. In this paper, we propose several evolutionary algorithms for computing alignments between two sentences in a parallel corpus. This algorithm has been tested on different tasks involving different pair of languages. Specifically, in the two shared tasks proposed in the HLT-NAACL 2003 and in the ACL 2005, the EDA-based algorithm outperforms the best participant systems. In addition, the experiments show that, because of the limitations of the well known statistical alignment models, new improvements in alignments quality could not be achieved by using improved search algorithms only.},
  langid = {english},
  keywords = {Estimation of distribution algorithms,Evolutionary algorithms,irrelevant,Statistical alignments,Statistical machine translation},
  file = {/home/bestname/Zotero/storage/8B8R7AEH/RodrÃ­guez et al_2008_On the application of different evolutionary algorithms to the alignment.pdf;/home/bestname/Zotero/storage/3KLAQERZ/S0925231207003529.html}
}

@article{sabbaghan_StatisticalMeasurementTrees_2020,
  title = {Statistical Measurement of Trees' Similarity},
  author = {Sabbaghan, Sahar and Chua, Cecil Eng Huang and Gardner, Lesley A.},
  year = {2020},
  month = jun,
  journal = {Quality \& Quantity},
  volume = {54},
  number = {3},
  pages = {781--806},
  issn = {1573-7845},
  doi = {10.1007/s11135-019-00957-8},
  abstract = {Diagnostic theories are fundamental to Information Systems practice and are represented in trees. One way of creating diagnostic trees is by employing independent experts to construct such trees and compare them. However, good measures of similarity to compare diagnostic trees have not been identified. This paper presents an analysis of the suitability of various measures of association to determine the similarity of two diagnostic trees using bootstrap simulations. We find that three measures of association, Goodman and Kruskal's Lambda, Cohen's Kappa, and Goodman and Kruskal's Gamma (J Am Stat Assoc 49(268):732\textendash 764, 1954) each behave differently depending on what is inconsistent between the two trees thus providing both measures for assessing alignment between two trees developed by independent experts as well as identifying the causes of the differences.},
  langid = {english},
  keywords = {Path,Tree},
  annotation = {ZSCC: 0000001},
  file = {/home/bestname/Zotero/storage/WRP9AZHT/Sabbaghan et al_2020_Statistical measurement of treesâ similarity.pdf}
}

@misc{salehi_VariationalInferenceNormalizing_2021,
  title = {Variational {{Inference}} with {{Normalizing Flows}} on {{MNIST}}},
  author = {Salehi, Mohammadreza},
  year = {2021},
  month = apr,
  journal = {Medium},
  abstract = {In this post, I will explain what normalizing flows are and how they can be used in variational inference and designing generative models\ldots},
  howpublished = {https://towardsdatascience.com/variational-inference-with-normalizing-flows-on-mnist-9258bbcf8810},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/55UK2WHD/variational-inference-with-normalizing-flows-on-mnist-9258bbcf8810.html}
}

@article{samoilescu_ModelagnosticScalableCounterfactual_2021,
  title = {Model-Agnostic and {{Scalable Counterfactual Explanations}} via {{Reinforcement Learning}}},
  author = {Samoilescu, Robert-Florian and Van Looveren, Arnaud and Klaise, Janis},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.02597 [cs, stat]},
  eprint = {2106.02597},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Counterfactual instances are a powerful tool to obtain valuable insights into automated decision processes, describing the necessary minimal changes in the input space to alter the prediction towards a desired target. Most previous approaches require a separate, computationally expensive optimization procedure per instance, making them impractical for both large amounts of data and high-dimensional data. Moreover, these methods are often restricted to certain subclasses of machine learning models (e.g. differentiable or tree-based models). In this work, we propose a deep reinforcement learning approach that transforms the optimization procedure into an end-to-end learnable process, allowing us to generate batches of counterfactual instances in a single forward pass. Our experiments on real-world data show that our method i) is model-agnostic (does not assume differentiability), relying only on feedback from model predictions; ii) allows for generating target-conditional counterfactual instances; iii) allows for flexible feature range constraints for numerical and categorical attributes, including the immutability of protected features (e.g. gender, race); iv) is easily extended to other data modalities such as images.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,hot,skimmed,Statistics - Machine Learning},
  annotation = {ZSCC: 0000000},
  file = {/home/bestname/Zotero/storage/2GNX2ZSY/Samoilescu et al_2021_Model-agnostic and Scalable Counterfactual Explanations via Reinforcement.pdf;/home/bestname/Zotero/storage/VT6KJXMP/2106.html}
}

@article{schat_DataRepresentativenessCriterion_2020,
  title = {The Data Representativeness Criterion: {{Predicting}} the Performance of Supervised Classification Based on Data Set Similarity},
  shorttitle = {The Data Representativeness Criterion},
  author = {Schat, Evelien and van de Schoot, Rens and Kouw, Wouter M. and Veen, Duco and Mendrik, Adri{\"e}nne M.},
  year = {2020},
  month = nov,
  journal = {PLOS ONE},
  volume = {15},
  number = {8},
  pages = {e0237009},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0237009},
  abstract = {In a broad range of fields it may be desirable to reuse a supervised classification algorithm and apply it to a new data set. However, generalization of such an algorithm and thus achieving a similar classification performance is only possible when the training data used to build the algorithm is similar to new unseen data one wishes to apply it to. It is often unknown in advance how an algorithm will perform on new unseen data, being a crucial reason for not deploying an algorithm at all. Therefore, tools are needed to measure the similarity of data sets. In this paper, we propose the Data Representativeness Criterion (DRC) to determine how representative a training data set is of a new unseen data set. We present a proof of principle, to see whether the DRC can quantify the similarity of data sets and whether the DRC relates to the performance of a supervised classification algorithm. We compared a number of magnetic resonance imaging (MRI) data sets, ranging from subtle to severe difference is acquisition parameters. Results indicate that, based on the similarity of data sets, the DRC is able to give an indication as to when the performance of a supervised classifier decreases. The strictness of the DRC can be set by the user, depending on what one considers to be an acceptable underperformance.},
  langid = {english},
  keywords = {Algorithms,Central nervous system,Computer vision,Data acquisition,Machine learning,Machine learning algorithms,Magnetic resonance imaging,Probability distribution},
  annotation = {ZSCC: 0000008},
  file = {/home/bestname/Zotero/storage/R2XMHJSA/Schat et al_2020_The data representativeness criterion.pdf;/home/bestname/Zotero/storage/F7H9A73U/article.html}
}

@article{shook_AssessmentUseStructural_2004,
  title = {An Assessment of the Use of Structural Equation Modeling in Strategic Management Research},
  author = {Shook, Christopher L. and Ketchen Jr., David J. and Hult, G. Tomas M. and Kacmar, K. Michele},
  year = {2004},
  journal = {Strategic Management Journal},
  volume = {25},
  number = {4},
  pages = {397--404},
  issn = {1097-0266},
  doi = {10.1002/smj.385},
  abstract = {Structural equation modeling (SEM) is a powerful, yet complex, analytical technique. The use of SEM to examine strategic management phenomena has increased dramatically in recent years, suggesting that a critical evaluation of the technique's implementation is needed. We compared the use of SEM in 92 strategic management studies published in nine prominent journals from 1984 to 2002 to guidelines culled from methodological research. We found that the use and reporting of SEM often have been less than ideal, indicating that authors may be drawing erroneous conclusions about relationships among variables. Given these results, we offer suggestions for researchers on how to better deploy SEM within future inquiry. Copyright \textcopyright{} 2004 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {data analysis,statistical methods,structural equation modeling},
  annotation = {ZSCC: 0001113  \_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/smj.385},
  file = {/home/bestname/Zotero/storage/UFXYDQKW/Shook et al_2004_An assessment of the use of structural equation modeling in strategic.pdf;/home/bestname/Zotero/storage/8JXQ58NB/smj.html}
}

@misc{shreyas_DeepEmbeddingCategorical_2019,
  title = {Deep {{Embedding}}'s for {{Categorical}} Variables ({{Cat2Vec}})},
  author = {Shreyas, Prajwal},
  year = {2019},
  month = sep,
  journal = {Medium},
  abstract = {In this blog I am going to take you through the steps involved in creating a embedding for categorical variables using a deep learning\ldots},
  howpublished = {https://towardsdatascience.com/deep-embeddings-for-categorical-variables-cat2vec-b05c8ab63ac0},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/PYNDCK4U/deep-embeddings-for-categorical-variables-cat2vec-b05c8ab63ac0.html}
}

@article{shrikumar_LearningImportantFeatures_2019,
  title = {Learning {{Important Features Through Propagating Activation Differences}}},
  author = {Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
  year = {2019},
  month = oct,
  journal = {arXiv:1704.02685 [cs]},
  eprint = {1704.02685},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The purported "black box" nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. Video tutorial: http://goo.gl/qKb7pL, ICML slides: bit.ly/deeplifticmlslides, ICML talk: https://vimeo.com/238275076, code: http://goo.gl/RM8jvH.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/8TX9M3N7/Shrikumar et al_2019_Learning Important Features Through Propagating Activation Differences.pdf;/home/bestname/Zotero/storage/HRMNAY8G/1704.html}
}

@misc{simonleglaive_DynamicalVariationalAutoencoders_2021,
  title = {Dynamical {{Variational Autoencoders}} (2/5)},
  author = {{Simon Leglaive}},
  year = {2021},
  month = jun,
  abstract = {Video 2/5: Dynamical VAEs Tutorial presented at IEEE ICASSP 2021 More resources available at https://dynamicalvae.github.io},
  annotation = {ZSCC: NoCitationData[s0]}
}

@misc{simonleglaive_DynamicalVariationalAutoencoders_2021a,
  title = {Dynamical {{Variational Autoencoders}} (3/5)},
  author = {{Simon Leglaive}},
  year = {2021},
  month = jun,
  abstract = {Video 3/5: Dynamical VAEs (continued) Tutorial presented at IEEE ICASSP 2021 More resources available at https://dynamicalvae.github.io}
}

@incollection{sloss_2019EvolutionaryAlgorithms_2020,
  title = {2019 {{Evolutionary Algorithms Review}}},
  booktitle = {Genetic {{Programming Theory}} and {{Practice XVII}}},
  author = {Sloss, Andrew N. and Gustafson, Steven},
  editor = {Banzhaf, Wolfgang and Goodman, Erik and Sheneman, Leigh and Trujillo, Leonardo and Worzel, Bill},
  year = {2020},
  series = {Genetic and {{Evolutionary Computation}}},
  pages = {307--344},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-39958-0_16},
  abstract = {Evolutionary algorithm research and applications began over 50 years ago. Like other artificial intelligence techniques, evolutionary algorithms will likely see increased use and development due to the increased availability of computation, more robust and available open source software libraries, and the increasing demand for artificial intelligence techniques. As these techniques become more adopted and capable, it is the right time to take a perspective of their ability to integrate into society and the human processes they intend to augment. In this review, we explore a new taxonomy of evolutionary algorithms and resulting classifications that look at five main areas: the ability to manage the control of the environment with limiters, the ability to explain and repeat the search process, the ability to understand input and output causality within a solution, the ability to manage algorithm bias due to data or user design, and lastly, the ability to add corrective measures. These areas are motivated by today's pressures on industry to conform to both societies concerns and new government regulatory rules. As many reviews of evolutionary algorithms exist, after motivating this new taxonomy, we briefly classify a broad range of algorithms and identify areas of future research.},
  isbn = {978-3-030-39958-0},
  langid = {english},
  keywords = {irrelevant,skimmed},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/PNS223J8/Sloss_Gustafson_2020_2019 Evolutionary Algorithms Review.pdf}
}

@article{sologub_MeasuringSimilarityTree_,
  title = {On {{Measuring}} of {{Similarity}} between {{Tree Nodes}}},
  author = {Sologub, Gleb B},
  pages = {9},
  abstract = {In this paper, a survey of similarity measures between vertices of a graph is presented. Distance-based and structural equivalence measures are described. It is demonstrated that most of them degenerate if applied directly to the tree nodes. Adjusted path-based similarity measure is proposed as well as a new method for representing tree nodes as binary vectors that is based on using of an ancestor matrix. It is shown that application of ordinary similarity measures to this representation gives desired non-trivial results.},
  langid = {english},
  keywords = {Path,Tree},
  annotation = {ZSCC: 0000006},
  file = {/home/bestname/Zotero/storage/RZF7B3WZ/Sologub - On Measuring of Similarity between Tree Nodes.pdf}
}

@incollection{starr_Counterfactuals_2021,
  ids = {starr_Counterfactuals_2021a},
  title = {Counterfactuals},
  booktitle = {The {{Stanford Encyclopedia}} of {{Philosophy}}},
  author = {Starr, William},
  editor = {Zalta, Edward N.},
  year = {2021},
  edition = {Summer 2021},
  publisher = {{Metaphysics Research Lab, Stanford University}},
  abstract = {Modal discourse concerns alternative ways things can be, e.g., whatmight be true, what isn't true but could have been, what shouldbe done. This entry focuses on counterfactualmodality which concerns what is not, but could or would havebeen. What if Martin Luther King had died when he was stabbed in 1958(Byrne 2005: 1)? What if the Americashad never been colonized? What if I were to put that box over here andthis one over there? These modes of thought and speech have been thesubject of extensive study in philosophy, linguistics, psychology,artificial intelligence, history, and many other allied fields. Thesediverse investigations are united by the fact that counterfactualmodality crops up at the center of foundational questions in thesefields., In philosophy, counterfactual modality has given rise to difficultsemantic, epistemological, and metaphysical questions:, These questions have attracted significant attention in recentdecades, revealing a wealth of puzzles and insights. While otherentries address the epistemic\textemdash the epistemology of modality\textemdash and metaphysical questions\textemdash possible worlds and actualism\textemdash this entry focuses on the semantic question. It will aim to refine thisquestion, explain its central role in certain philosophical debates,and outline the main semantic analyses of counterfactuals.,  Section 1 begins with a working definition of counterfactual conditionals (\textsection 1.1), and then surveys how counterfactuals feature in theories of agency,mental representation, and rationality (\textsection 1.2), and how they are used in metaphysical analysis and scientificexplanation (\textsection 1.3). Section 1.4 then details several ways in which the logic andtruth-conditions of counterfactuals are puzzling. This sets the stagefor the sections 2 and 3, which survey semantic analyses of counterfactuals that attempt toexplain this puzzling behavior.,  Section 2 focuses on two related analyses that were primarily developed tostudy the logic of counterfactuals: strict conditionalanalyses and similarity analyses. These analyses were not originallyconcerned with saying what the truth-conditions of particularcounterfactuals are. Attempts to extend them to that domain, however,have attracted intense criticism. Section 3 surveys more recent analyses that offer more explicit models of whencounterfactuals are true. These analyses include premise semantics (\textsection 3.1), conditional probability analyses (\textsection 3.2) and structural equations/causal models (\textsection 3.3). They are more closely connected to work on counterfactuals inpsychology, artificial intelligence, and the philosophy ofscience., Sections 2 and 3 of this entry employ some basic tools from set theory and logicalsemantics. But these sections also provide intuitive characterizationsalongside formal definitions, so familiarity with these tools is not apre-requisite. Readers interested in more familiarity with these toolswill find basic set theory, as well as Gamut (1991) and Sider(2010) useful.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/IIF3XSVT/counterfactuals.html;/home/bestname/Zotero/storage/JD8KY8HL/counterfactuals.html}
}

@article{strobelt_Seq2SeqVisVisualDebugging_2018,
  title = {{{Seq2Seq-Vis}}: {{A Visual Debugging Tool}} for {{Sequence-to-Sequence Models}}},
  shorttitle = {{{Seq2Seq-Vis}}},
  author = {Strobelt, Hendrik and Gehrmann, Sebastian and Behrisch, Michael and Perer, Adam and Pfister, Hanspeter and Rush, Alexander M.},
  year = {2018},
  month = oct,
  journal = {arXiv:1804.09299 [cs]},
  eprint = {1804.09299},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Neural Sequence-to-Sequence models have proven to be accurate and robust for many sequence prediction tasks, and have become the standard approach for automatic translation of text. The models work in a five stage blackbox process that involves encoding a source sequence to a vector space and then decoding out to a new target sequence. This process is now standard, but like many deep learning methods remains quite difficult to understand or debug. In this work, we present a visual analysis tool that allows interaction with a trained sequence-to-sequence model through each stage of the translation process. The aim is to identify which patterns have been learned and to detect model errors. We demonstrate the utility of our tool through several real-world large-scale sequence-to-sequence use cases.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/ZF3Y94CV/Strobelt et al_2018_Seq2Seq-Vis.pdf;/home/bestname/Zotero/storage/ZHN7NZ7R/1804.html}
}

@inproceedings{tax_PredictiveBusinessProcess_2017,
  title = {Predictive {{Business Process Monitoring}} with {{LSTM Neural Networks}}},
  booktitle = {Advanced {{Information Systems Engineering}}},
  author = {Tax, Niek and Verenich, Ilya and La Rosa, Marcello and Dumas, Marlon},
  editor = {Dubois, Eric and Pohl, Klaus},
  year = {2017},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {477--492},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-59536-8_30},
  abstract = {Predictive business process monitoring methods exploit logs of completed cases of a process in order to make predictions about running cases thereof. Existing methods in this space are tailor-made for specific prediction tasks. Moreover, their relative accuracy is highly sensitive to the dataset at hand, thus requiring users to engage in trial-and-error and tuning when applying them in a specific setting. This paper investigates Long Short-Term Memory (LSTM) neural networks as an approach to build consistently accurate models for a wide range of predictive process monitoring tasks. First, we show that LSTMs outperform existing techniques to predict the next event of a running case and its timestamp. Next, we show how to use models for predicting the next task in order to predict the full continuation of a running case. Finally, we apply the same approach to predict the remaining time, and show that this approach outperforms existing tailor-made methods.},
  isbn = {978-3-319-59536-8},
  langid = {english},
  keywords = {Activity Prediction,done,hot,Levenshtein Distance,Prediction Point,Recurrent Neural Network,Transition System State},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/WR2GFPG9/Tax et al_2017_Predictive Business Process Monitoring with LSTM Neural Networks.pdf}
}

@article{teinemaa_OutcomeOrientedPredictiveProcess_2019,
  ids = {teinemaa_OutcomeOrientedPredictiveProcess_2018a,teinemaa_OutcomeOrientedPredictiveProcess_2018b},
  title = {Outcome-{{Oriented Predictive Process Monitoring}}: {{Review}} and {{Benchmark}}},
  shorttitle = {Outcome-{{Oriented Predictive Process Monitoring}}},
  author = {Teinemaa, Irene and Dumas, Marlon and Rosa, Marcello La and Maggi, Fabrizio Maria},
  year = {2019},
  month = mar,
  journal = {ACM Transactions on Knowledge Discovery from Data},
  volume = {13},
  number = {2},
  eprint = {1707.06766},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {17:1--17:57},
  issn = {1556-4681},
  doi = {10.1145/3301300},
  abstract = {Predictive business process monitoring refers to the act of making predictions about the future state of ongoing cases of a business process, based on their incomplete execution traces and logs of historical (completed) traces. Motivated by the increasingly pervasive availability of fine-grained event data about business process executions, the problem of predictive process monitoring has received substantial attention in the past years. In particular, a considerable number of methods have been put forward to address the problem of outcome-oriented predictive process monitoring, which refers to classifying each ongoing case of a process according to a given set of possible categorical outcomes\textemdash e.g., Will the customer complain or not? Will an order be delivered, canceled, or withdrawn? Unfortunately, different authors have used different datasets, experimental settings, evaluation measures, and baselines to assess their proposals, resulting in poor comparability and an unclear picture of the relative merits and applicability of different methods. To address this gap, this article presents a systematic review and taxonomy of outcome-oriented predictive process monitoring methods, and a comparative experimental evaluation of eleven representative methods using a benchmark covering 24 predictive process monitoring tasks based on nine real-life event logs.},
  archiveprefix = {arXiv},
  keywords = {Business process,Computer Science - Artificial Intelligence,predictive monitoring,sequence classification},
  file = {/home/bestname/Zotero/storage/6856MSKP/Teinemaa et al_2019_Outcome-Oriented Predictive Process Monitoring.pdf;/home/bestname/Zotero/storage/IAAWT9X5/Teinemaa et al_2018_Outcome-Oriented Predictive Process Monitoring.pdf;/home/bestname/Zotero/storage/AT8F9AUQ/1707.html}
}

@article{tsirtsis_CounterfactualExplanationsSequential_2021,
  title = {Counterfactual {{Explanations}} in {{Sequential Decision Making Under Uncertainty}}},
  author = {Tsirtsis, Stratis and De, Abir and {Gomez-Rodriguez}, Manuel},
  year = {2021},
  month = jul,
  journal = {arXiv:2107.02776 [cs, stat]},
  eprint = {2107.02776},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Methods to find counterfactual explanations have predominantly focused on one step decision making processes. In this work, we initiate the development of methods to find counterfactual explanations for decision making processes in which multiple, dependent actions are taken sequentially over time. We start by formally characterizing a sequence of actions and states using finite horizon Markov decision processes and the Gumbel-Max structural causal model. Building upon this characterization, we formally state the problem of finding counterfactual explanations for sequential decision making processes. In our problem formulation, the counterfactual explanation specifies an alternative sequence of actions differing in at most k actions from the observed sequence that could have led the observed process realization to a better outcome. Then, we introduce a polynomial time algorithm based on dynamic programming to build a counterfactual policy that is guaranteed to always provide the optimal counterfactual explanation on every possible realization of the counterfactual environment dynamics. We validate our algorithm using both synthetic and real data from cognitive behavioral therapy and show that the counterfactual explanations our algorithm finds can provide valuable insights to enhance sequential decision making under uncertainty.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning,done,hot,Statistics - Machine Learning},
  annotation = {ZSCC: 0000000},
  file = {/home/bestname/Zotero/storage/UVJHCHIC/Tsirtsis et al_2021_Counterfactual Explanations in Sequential Decision Making Under Uncertainty.pdf;/home/bestname/Zotero/storage/NHKSHY45/2107.html}
}

@inproceedings{vanderaalst_ProcessMiningManifesto_2012,
  title = {Process {{Mining Manifesto}}},
  booktitle = {Business {{Process Management Workshops}}},
  author = {{van der Aalst}, Wil and Adriansyah, Arya and {de Medeiros}, Ana Karla Alves and Arcieri, Franco and Baier, Thomas and Blickle, Tobias and Bose, Jagadeesh Chandra and {van den Brand}, Peter and Brandtjen, Ronald and Buijs, Joos and Burattin, Andrea and Carmona, Josep and Castellanos, Malu and Claes, Jan and Cook, Jonathan and Costantini, Nicola and Curbera, Francisco and Damiani, Ernesto and {de Leoni}, Massimiliano and Delias, Pavlos and {van Dongen}, Boudewijn F. and Dumas, Marlon and Dustdar, Schahram and Fahland, Dirk and Ferreira, Diogo R. and Gaaloul, Walid and {van Geffen}, Frank and Goel, Sukriti and G{\"u}nther, Christian and Guzzo, Antonella and Harmon, Paul and {ter Hofstede}, Arthur and Hoogland, John and Ingvaldsen, Jon Espen and Kato, Koki and Kuhn, Rudolf and Kumar, Akhil and La Rosa, Marcello and Maggi, Fabrizio and Malerba, Donato and Mans, Ronny S. and Manuel, Alberto and McCreesh, Martin and Mello, Paola and Mendling, Jan and Montali, Marco and {Motahari-Nezhad}, Hamid R. and {zur Muehlen}, Michael and {Munoz-Gama}, Jorge and Pontieri, Luigi and Ribeiro, Joel and Rozinat, Anne and Seguel P{\'e}rez, Hugo and Seguel P{\'e}rez, Ricardo and Sep{\'u}lveda, Marcos and Sinur, Jim and Soffer, Pnina and Song, Minseok and Sperduti, Alessandro and Stilo, Giovanni and Stoel, Casper and Swenson, Keith and Talamo, Maurizio and Tan, Wei and Turner, Chris and Vanthienen, Jan and Varvaressos, George and Verbeek, Eric and Verdonk, Marc and Vigo, Roberto and Wang, Jianmin and Weber, Barbara and Weidlich, Matthias and Weijters, Ton and Wen, Lijie and Westergaard, Michael and Wynn, Moe},
  editor = {Daniel, Florian and Barkaoui, Kamel and Dustdar, Schahram},
  year = {2012},
  series = {Lecture {{Notes}} in {{Business Information Processing}}},
  pages = {169--194},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-28108-2_19},
  abstract = {Process mining techniques are able to extract knowledge from event logs commonly available in today's information systems. These techniques provide new means to discover, monitor, and improve processes in a variety of application domains. There are two main drivers for the growing interest in process mining. On the one hand, more and more events are being recorded, thus, providing detailed information about the history of processes. On the other hand, there is a need to improve and support business processes in competitive and rapidly changing environments. This manifesto is created by the IEEE Task Force on Process Mining and aims to promote the topic of process mining. Moreover, by defining a set of guiding principles and listing important challenges, this manifesto hopes to serve as a guide for software developers, scientists, consultants, business managers, and end-users. The goal is to increase the maturity of process mining as a new tool to improve the (re)design, control, and support of operational business processes.},
  isbn = {978-3-642-28108-2},
  langid = {english},
  keywords = {Business Intelligence,Business Process Management,Concept Drift,done,hot,Process Instance,Process Mining},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/HJ95L8HP/van der Aalst et al_2012_Process Mining Manifesto.pdf}
}

@article{vanderaalst_ProcessMiningOverview_2012,
  title = {Process {{Mining}}: {{Overview}} and {{Opportunities}}},
  shorttitle = {Process {{Mining}}},
  author = {{van der Aalst}, Wil},
  year = {2012},
  month = jul,
  journal = {ACM Transactions on Management Information Systems},
  volume = {3},
  number = {2},
  pages = {7:1--7:17},
  issn = {2158-656X},
  doi = {10.1145/2229156.2229157},
  abstract = {Over the last decade, process mining emerged as a new research field that focuses on the analysis of processes using event data. Classical data mining techniques such as classification, clustering, regression, association rule learning, and sequence/episode mining do not focus on business process models and are often only used to analyze a specific step in the overall process. Process mining focuses on end-to-end processes and is possible because of the growing availability of event data and new process discovery and conformance checking techniques. Process models are used for analysis (e.g., simulation and verification) and enactment by BPM/WFM systems. Previously, process models were typically made by hand without using event data. However, activities executed by people, machines, and software leave trails in so-called event logs. Process mining techniques use such logs to discover, analyze, and improve business processes. Recently, the Task Force on Process Mining released the Process Mining Manifesto. This manifesto is supported by 53 organizations and 77 process mining experts contributed to it. The active involvement of end-users, tool vendors, consultants, analysts, and researchers illustrates the growing significance of process mining as a bridge between data mining and business process modeling. The practical relevance of process mining and the interesting scientific challenges make process mining one of the ``hot'' topics in Business Process Management (BPM). This article introduces process mining as a new research field and summarizes the guiding principles and challenges described in the manifesto.},
  keywords = {business intelligence,business process management,data mining,done,hot,Process mining},
  annotation = {ZSCC: 0000351},
  file = {/home/bestname/Zotero/storage/323IRJ79/van der Aalst_2012_Process Mining.pdf}
}

@inproceedings{vanderaalst_ProcessMiningSimulation_2018,
  title = {Process Mining and Simulation: A Match Made in Heaven!},
  shorttitle = {Process Mining and Simulation},
  booktitle = {Proceedings of the 50th {{Computer Simulation Conference}}},
  author = {{van der Aalst}, Wil M. P.},
  year = {2018},
  month = jul,
  series = {{{SummerSim}} '18},
  pages = {1--12},
  publisher = {{Society for Computer Simulation International}},
  address = {{San Diego, CA, USA}},
  abstract = {Event data are collected everywhere: in logistics, manufacturing, finance, healthcare, e-learning, e-government, and many other domains. The events found in these domains typically refer to activities executed by resources at particular times and for particular cases. Process mining provides the means to discover the real processes, to detect deviations from normative processes, and to analyze bottlenecks and waste from such events. However, process mining tends to be backward-looking. Fortunately, simulation can be used to explore different design alternatives and to anticipate future performance problems. This keynote paper discusses the link between both types of analysis and elaborates on the challenges process discovery techniques are facing. Quality notions such as recall, precision, and generalization are discussed. Rather than introducing a specific process discovery or conformance checking algorithm, the paper provides a comprehensive set of conformance propositions. These conformance propositions serve two purposes: (1) introducing the essence of process mining by discussing the relation between event logs and process models, and (2) discussing possible requirements for the quantification of quality notions related to recall, precision, and generalization.},
  keywords = {conformance checking,done,hot,process discovery,process mining,simulation},
  annotation = {ZSCC: 0000034},
  file = {/home/bestname/Zotero/storage/A96465Y6/van der Aalst_2018_Process mining and simulation.pdf}
}

@article{vaswani_AttentionAllYou_2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  journal = {arXiv:1706.03762 [cs]},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/bestname/Zotero/storage/YY7ATS65/Vaswani et al_2017_Attention Is All You Need.pdf;/home/bestname/Zotero/storage/EA38I6IJ/1706.html}
}

@article{verboven_CombiningClinicalOperational_,
  title = {Combining the {{Clinical}} and {{Operational Perspective}} in {{Heterogeneous Treatment Effect Inference}} in {{Healthcare Processes}}},
  author = {Verboven, Sam and Martin, Niels},
  pages = {12},
  abstract = {Recent developments in causal machine learning open perspectives for new approaches that support decision-making in healthcare processes using causal models. In particular, Heterogeneous Treatment Effect (HTE) inference enables the estimation of causal treatment effects for individual cases, offering great potential in a process mining context. At the same time, HTE literature typically focuses on clinical outcome measures, disregarding process efficiency. This paper shows the potential of jointly considering the clinical and operational effects of treatments in the context of healthcare processes. Moreover, we present a simple pipeline that makes existing HTE machine learning techniques directly applicable to event logs. Besides these conceptual contributions, a proofof-concept application starting from the publicly available sepsis event log is outlined, forming the basis for a critical reflection regarding HTE estimation in a process mining context.},
  langid = {english},
  keywords = {hot,skimmed},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/W936WH4B/Verboven und Martin - Combining the Clinical and Operational Perspective.pdf}
}

@article{verma_CounterfactualExplanationsMachine_2020,
  title = {Counterfactual {{Explanations}} for {{Machine Learning}}: {{A Review}}},
  shorttitle = {Counterfactual {{Explanations}} for {{Machine Learning}}},
  author = {Verma, Sahil and Dickerson, John and Hines, Keegan},
  year = {2020},
  month = oct,
  journal = {arXiv:2010.10596 [cs, stat]},
  eprint = {2010.10596},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Machine learning plays a role in many deployed decision systems, often in ways that are difficult or impossible to understand by human stakeholders. Explaining, in a human-understandable way, the relationship between the input and output of machine learning models is essential to the development of trustworthy machine-learning-based systems. A burgeoning body of research seeks to define the goals and methods of explainability in machine learning. In this paper, we seek to review and categorize research on counterfactual explanations, a specific class of explanation that provides a link between what could have happened had input to a model been changed in a particular way. Modern approaches to counterfactual explainability in machine learning draw connections to the established legal doctrine in many countries, making them appealing to fielded systems in high-impact areas such as finance and healthcare. Thus, we design a rubric with desirable properties of counterfactual explanation algorithms and comprehensively evaluate all currently-proposed algorithms against that rubric. Our rubric provides easy comparison and comprehension of the advantages and disadvantages of different approaches and serves as an introduction to major research themes in this field. We also identify gaps and discuss promising research directions in the space of counterfactual explainability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,done,Statistics - Machine Learning},
  annotation = {ZSCC: 0000048},
  file = {/home/bestname/Zotero/storage/YYRRP69V/Verma et al_2020_Counterfactual Explanations for Machine Learning.pdf;/home/bestname/Zotero/storage/ARJRNWD2/2010.html}
}

@inproceedings{vikhar_EvolutionaryAlgorithmsCritical_2016,
  title = {Evolutionary Algorithms: {{A}} Critical Review and Its Future Prospects},
  shorttitle = {Evolutionary Algorithms},
  booktitle = {2016 {{International Conference}} on {{Global Trends}} in {{Signal Processing}}, {{Information Computing}} and {{Communication}} ({{ICGTSPICC}})},
  author = {Vikhar, Pradnya A.},
  year = {2016},
  month = dec,
  pages = {261--265},
  doi = {10.1109/ICGTSPICC.2016.7955308},
  abstract = {Evolutionary algorithm (EA) emerges as an important optimization and search technique in the last decade. EA is a subset of Evolutionary Computations (EC) and belongs to set of modern heuristics based search method. Due to flexible nature and robust behavior inherited from Evolutionary Computation, it becomes efficient means of problem solving method for widely used global optimization problems. It can be used successfully in many applications of high complexity. This paper presents a critical overview of Evolutionary algorithms and its generic procedure for implementation. It further discusses the various practical advantages using evolutionary algorithms over classical methods of optimization. It also includes unusual study of various invariants of EA like Genetic Programming (GP), Genetic Algorithm (GA), Evolutionary Programming (EP) and Evolution Strategies (ES). Extensions of EAs in the form of Memetic algorithms (MA) and distributed EA are also discussed. Further the paper focuses on various refinements done in area of EA to solve real life problems.},
  keywords = {Distributed EAs,done,Evolutionary Algorithm,Evolutionary computation,Evolutionary Computations,Genetic algorithms,Genetic programming,hot,Memetic Algorithms,Optimization,Programming,Sociology,Statistics},
  file = {/home/bestname/Zotero/storage/W8B9Y4MB/Vikhar_2016_Evolutionary algorithms.pdf;/home/bestname/Zotero/storage/9PIIN6N4/7955308.html}
}

@article{viner_ProcessMiningSoftware_2021,
  title = {A {{Process Mining Software Comparison}}},
  author = {Viner, Daniel and Stierle, Matthias and Matzner, Martin},
  year = {2021},
  month = feb,
  journal = {arXiv:2007.14038 [cs]},
  eprint = {2007.14038},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {www.processmining-software.com is a dedicated website for process mining software comparison and was developed to give practitioners and researchers an overview of commercial tools available on the market. Based on literature review and experimental tool testing, a set of criteria was developed in order to assess the tools' functional capabilities in an objective manner. With our publicly accessible website, we intend to increase the transparency of tool functionality. Being an academic endeavour, the non-commercial nature of the study ensures a less biased assessment as compared with reports from analyst firms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction,Computer Science - Software Engineering},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/JXYZZDFN/Viner et al_2021_A Process Mining Software Comparison.pdf;/home/bestname/Zotero/storage/6QHQVJZF/2007.html}
}

@article{wachter_CounterfactualExplanationsOpening_2017,
  title = {Counterfactual {{Explanations Without Opening}} the {{Black Box}}: {{Automated Decisions}} and the {{GDPR}}},
  shorttitle = {Counterfactual {{Explanations Without Opening}} the {{Black Box}}},
  author = {Wachter, Sandra and Mittelstadt, B. and Russell, Chris},
  year = {2018},
  journal = {Harvard Journal of Law \& Technology, 2018},
  doi = {10.2139/ssrn.3063289},
  abstract = {It is suggested data controllers should offer a particular type of explanation, unconditional counterfactual explanations, to support these three aims, which describe the smallest change to the world that can be made to obtain a desirable outcome, or to arrive at the closest possible world, without needing to explain the internal logic of the system. There has been much discussion of the right to explanation in the EU General Data Protection Regulation, and its existence, merits, and disadvantages. Implementing a right to explanation that opens the black box of algorithmic decision-making faces major legal and technical barriers. Explaining the functionality of complex algorithmic decision-making systems and their rationale in specific cases is a technically challenging problem. Some explanations may offer little meaningful information to data subjects, raising questions around their value. Explanations of automated decisions need not hinge on the general public understanding how algorithmic systems function. Even though such interpretability is of great importance and should be pursued, explanations can, in principle, be offered without opening the black box. Looking at explanations as a means to help a data subject act rather than merely understand, one could gauge the scope and content of explanations according to the specific goal or action they are intended to support. From the perspective of individuals affected by automated decision-making, we propose three aims for explanations: (1) to inform and help the individual understand why a particular decision was reached, (2) to provide grounds to contest the decision if the outcome is undesired, and (3) to understand what would need to change in order to receive a desired result in the future, based on the current decision-making model. We assess how each of these goals finds support in the GDPR. We suggest data controllers should offer a particular type of explanation, unconditional counterfactual explanations, to support these three aims. These counterfactual explanations describe the smallest change to the world that can be made to obtain a desirable outcome, or to arrive at the closest possible world, without needing to explain the internal logic of the system.},
  annotation = {ZSCC: 0001096},
  file = {/home/bestname/Zotero/storage/GVX7D2L7/Wachter et al_2017_Counterfactual Explanations Without Opening the Black Box.pdf}
}

@inproceedings{wang_CloserLookRobustness_2021,
  title = {A {{Closer Look}} into the {{Robustness}} of {{Neural Dependency Parsers Using Better Adversarial Examples}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL-IJCNLP}} 2021},
  author = {Wang, Yuxuan and Che, Wanxiang and Titov, Ivan and Cohen, Shay B. and Lei, Zhilin and Liu, Ting},
  year = {2021},
  month = aug,
  pages = {2344--2354},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.findings-acl.207},
  keywords = {Pertubations},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/X6S6ZXHY/Wang et al_2021_A Closer Look into the Robustness of Neural Dependency Parsers Using Better.pdf}
}

@incollection{wang_ControllableUnsupervisedText_2019,
  ids = {wang_ControllableUnsupervisedText_2019a},
  title = {Controllable Unsupervised Text Attribute Transfer via Editing Entangled Latent Representation},
  booktitle = {Proceedings of the 33rd {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Wang, Ke and Hua, Hang and Wan, Xiaojun},
  year = {2019},
  month = dec,
  number = {990},
  eprint = {1905.12926},
  eprinttype = {arxiv},
  pages = {11036--11046},
  publisher = {{Curran Associates Inc.}},
  address = {{Red Hook, NY, USA}},
  abstract = {Unsupervised text attribute transfer automatically transforms a text to alter a specific attribute (e.g. sentiment) without using any parallel data, while simultaneously preserving its attribute-independent content. The dominant approaches are trying to model the content-independent attribute separately, e.g., learning different attributes' representations or using multiple attribute-specific decoders. However, it may lead to inflexibility from the perspective of controlling the degree of transfer or transferring over multiple aspects at the same time. To address the above problems, we propose a more flexible unsupervised text attribute transfer framework which replaces the process of modeling attribute with minimal editing of latent representations based on an attribute classifier. Specifically, we first propose a Transformer-based autoencoder to learn an entangled latent representation for a discrete text, then we transform the attribute transfer task to an optimization problem and propose the Fast-Gradient-Iterative-Modification algorithm to edit the latent representation until conforming to the target attribute. Extensive experimental results demonstrate that our model achieves very competitive performance on three public data sets. Furthermore, we also show that our model can not only control the degree of transfer freely but also allow transferring over multiple aspects at the same time.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  annotation = {ZSCC: 0000044},
  file = {/home/bestname/Zotero/storage/9RCW9B9E/Wang et al_2019_Controllable unsupervised text attribute transfer via editing entangled latent.pdf;/home/bestname/Zotero/storage/7GQKC94P/1905.html;/home/bestname/Zotero/storage/XIPP5NHC/1905.html}
}

@inproceedings{wang_CounterfactualDataAugmentedSequential_2021,
  title = {Counterfactual {{Data-Augmented Sequential Recommendation}}},
  booktitle = {Proceedings of the 44th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Wang, Zhenlei and Zhang, Jingsen and Xu, Hongteng and Chen, Xu and Zhang, Yongfeng and Zhao, Wayne Xin and Wen, Ji-Rong},
  year = {2021},
  month = jul,
  series = {{{SIGIR}} '21},
  pages = {347--356},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3404835.3462855},
  abstract = {Sequential recommendation aims at predicting users' preferences based on their historical behaviors. However, this recommendation strategy may not perform well in practice due to the sparsity of the real-world data. In this paper, we propose a novel counterfactual data augmentation framework to mitigate the impact of the imperfect training data and empower sequential recommendation models. Our framework is composed of a sampler model and an anchor model. The sampler model aims to generate new user behavior sequences based on the observed ones, while the anchor model is leveraged to provide the final recommendation list, which is trained based on both observed and generated sequences. We design the sampler model to answer the key counterfactual question: "what would a user like to buy if her previously purchased items had been different?". Beyond heuristic intervention methods, we leverage two learning-based methods to implement the sampler model, and thus, improve the quality of the generated sequences when training the anchor model. Additionally, we analyze the influence of the generated sequences on the anchor model in theory and achieve a trade-off between the information and the noise introduced by the generated sequences. Experiments on nine real-world datasets demonstrate our framework's effectiveness and generality.},
  isbn = {978-1-4503-8037-9},
  keywords = {counterfactual data augmentation,recommendation system},
  annotation = {ZSCC: 0000001},
  file = {/home/bestname/Zotero/storage/NRZL6QT9/Wang et al_2021_Counterfactual Data-Augmented Sequential Recommendation.pdf}
}

@article{wang_DeconfoundedRecommenderCausal_2019,
  title = {The {{Deconfounded Recommender}}: {{A Causal Inference Approach}} to {{Recommendation}}},
  shorttitle = {The {{Deconfounded Recommender}}},
  author = {Wang, Yixin and Liang, Dawen and Charlin, Laurent and Blei, David M.},
  year = {2019},
  month = may,
  journal = {arXiv:1808.06581 [cs, stat]},
  eprint = {1808.06581},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The goal of recommendation is to show users items that they will like. Though usually framed as a prediction, the spirit of recommendation is to answer an interventional question---for each user and movie, what would the rating be if we "forced" the user to watch the movie? To this end, we develop a causal approach to recommendation, one where watching a movie is a "treatment" and a user's rating is an "outcome." The problem is there may be unobserved confounders, variables that affect both which movies the users watch and how they rate them; unobserved confounders impede causal predictions with observational data. To solve this problem, we develop the deconfounded recommender, a way to use classical recommendation models for causal recommendation. Following Wang \& Blei [23], the deconfounded recommender involves two probabilistic models. The first models which movies the users watch; it provides a substitute for the unobserved confounders. The second one models how each user rates each movie; it employs the substitute to help account for confounders. This two-stage approach removes bias due to confounding. It improves recommendation and enjoys stable performance against interventions on test sets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/bestname/Zotero/storage/XAQ2BUS6/Wang et al_2019_The Deconfounded Recommender.pdf;/home/bestname/Zotero/storage/5TF4XR8G/1808.html}
}

@article{wang_EfficientRecoveryMissing_2013,
  title = {Efficient Recovery of Missing Events},
  author = {Wang, Jianmin and Song, Shaoxu and Zhu, Xiaochen and Lin, Xuemin},
  year = {2013},
  month = aug,
  journal = {Proceedings of the VLDB Endowment},
  volume = {6},
  number = {10},
  pages = {841--852},
  issn = {2150-8097},
  doi = {10.14778/2536206.2536212},
  abstract = {For various entering and transmission issues raised by human or system, missing events often occur in event data, which record execution logs of business processes. Without recovering these missing events, applications such as provenance analysis or complex event processing built upon event data are not reliable. Following the minimum change discipline in improving data quality, it is also rational to find a recovery that minimally differs from the original data. Existing recovery approaches fall short of efficiency owing to enumerating and searching over all the possible sequences of events. In this paper, we study the efficient techniques for recovering missing events. According to our theoretical results, the recovery problem is proved to be NP-hard. Nevertheless, we are able to concisely represent the space of event sequences in a branching framework. Advanced indexing and pruning techniques are developed to further improve the recovery efficiency. Our proposed efficient techniques make it possible to find top-k recoveries. The experimental results demonstrate that our minimum recovery approach achieves high accuracy, and significantly outperforms the state-of-the-art technique for up to 5 orders of magnitudes improvement in time performance.},
  file = {/home/bestname/Zotero/storage/77JHRAT3/Wang et al_2013_Efficient recovery of missing events.pdf}
}

@article{wang_MeasurementTextSimilarity_2020,
  title = {Measurement of {{Text Similarity}}: {{A Survey}}},
  shorttitle = {Measurement of {{Text Similarity}}},
  author = {Wang, Jiapeng and Dong, Yihong},
  year = {2020},
  month = sep,
  journal = {Information},
  volume = {11},
  number = {9},
  pages = {421},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/info11090421},
  abstract = {Text similarity measurement is the basis of natural language processing tasks, which play an important role in information retrieval, automatic question answering, machine translation, dialogue systems, and document matching. This paper systematically combs the research status of similarity measurement, analyzes the advantages and disadvantages of current methods, develops a more comprehensive classification description system of text similarity measurement algorithms, and summarizes the future development direction. With the aim of providing reference for related research and application, the text similarity measurement method is described by two aspects: text distance and text representation. The text distance can be divided into length distance, distribution distance, and semantic distance; text representation is divided into string-based, corpus-based, single-semantic text, multi-semantic text, and graph-structure-based representation. Finally, the development of text similarity is also summarized in the discussion section.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {done,hot,text distance,text representation,text similarity measure},
  annotation = {ZSCC: 0000014},
  file = {/home/bestname/Zotero/storage/G34AXHI3/Wang_Dong_2020_Measurement of Text Similarity.pdf;/home/bestname/Zotero/storage/TKWW5Z44/htm.html}
}

@article{waterman_BiologicalSequenceMetrics_1976,
  title = {Some Biological Sequence Metrics},
  author = {Waterman, M. S and Smith, T. F and Beyer, W. A},
  year = {1976},
  month = jun,
  journal = {Advances in Mathematics},
  volume = {20},
  number = {3},
  pages = {367--387},
  issn = {0001-8708},
  doi = {10.1016/0001-8708(76)90202-4},
  abstract = {Some new metrics are introduced to measure the distance between biological sequences, such as amino acid sequences or nucleotide sequences. These metrics generalize a metric of Sellers, who considered only single deletions, mutations, and insertions. The present metrics allow, for example, multiple deletions and insertions and single mutations. They also allow computation of the distance among more than two sequences. Algorithms for computing the values of the metrics are given which also compute best alignments. The connection with the information theory approach of Reichert, Cohen, and Wong is discussed.},
  langid = {english},
  annotation = {ZSCC: 0000540},
  file = {/home/bestname/Zotero/storage/UB9C7BBS/Waterman et al_1976_Some biological sequence metrics.pdf}
}

@misc{wilde_DaffidwildeEdo_2022,
  title = {Daffidwilde/Edo},
  author = {Wilde, Henry},
  year = {2022},
  month = feb,
  abstract = {A library for generating artificial datasets through genetic evolution.},
  copyright = {MIT},
  keywords = {data-generation,evolutionary-algorithms,optimisation},
  annotation = {ZSCC: NoCitationData[s0]}
}

@article{yang_CausalVAEDisentangledRepresentation_2021,
  title = {{{CausalVAE}}: {{Disentangled Representation Learning}} via {{Neural Structural Causal Models}}},
  shorttitle = {{{CausalVAE}}},
  author = {Yang, Mengyue and Liu, Furui and Chen, Zhitang and Shen, Xinwei and Hao, Jianye and Wang, Jun},
  year = {2021},
  month = mar,
  journal = {arXiv:2004.08697 [cs, stat]},
  eprint = {2004.08697},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Learning disentanglement aims at finding a low dimensional representation which consists of multiple explanatory and generative factors of the observational data. The framework of variational autoencoder (VAE) is commonly used to disentangle independent factors from observations. However, in real scenarios, factors with semantics are not necessarily independent. Instead, there might be an underlying causal structure which renders these factors dependent. We thus propose a new VAE based framework named CausalVAE, which includes a Causal Layer to transform independent exogenous factors into causal endogenous ones that correspond to causally related concepts in data. We further analyze the model identifiabitily, showing that the proposed model learned from observations recovers the true one up to a certain degree. Experiments are conducted on various datasets, including synthetic and real word benchmark CelebA. Results show that the causal representations learned by CausalVAE are semantically interpretable, and their causal relationship as a Directed Acyclic Graph (DAG) is identified with good accuracy. Furthermore, we demonstrate that the proposed CausalVAE model is able to generate counterfactual data through "do-operation" to the causal factors.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,hot,skimmed,Statistics - Machine Learning,VAE},
  annotation = {ZSCC: 0000019},
  file = {/home/bestname/Zotero/storage/287JXWRK/Yang et al_2021_CausalVAE.pdf;/home/bestname/Zotero/storage/P4JGGQEK/2004.html}
}

@article{zhao_DeeperUnderstandingVariational_2017,
  title = {Towards {{Deeper Understanding}} of {{Variational Autoencoding Models}}},
  author = {Zhao, Shengjia and Song, Jiaming and Ermon, Stefano},
  year = {2017},
  month = feb,
  journal = {arXiv:1702.08658 [cs, stat]},
  eprint = {1702.08658},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We propose a new family of optimization criteria for variational auto-encoding models, generalizing the standard evidence lower bound. We provide conditions under which they recover the data distribution and learn latent features, and formally show that common issues such as blurry samples and uninformative latent features arise when these conditions are not met. Based on these new insights, we propose a new sequential VAE model that can generate sharp samples on the LSUN image dataset based on pixel-wise reconstruction loss, and propose an optimization criterion that encourages unsupervised learning of informative latent features.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,VAE},
  annotation = {ZSCC: 0000118},
  file = {/home/bestname/Zotero/storage/CB526M43/Zhao et al_2017_Towards Deeper Understanding of Variational Autoencoding Models.pdf;/home/bestname/Zotero/storage/Q8AYM9G9/1702.html}
}

@article{zhao_StringCorrectionUsing_2019,
  title = {String Correction Using the {{Damerau-Levenshtein}} Distance},
  author = {Zhao, Chunchun and Sahni, Sartaj},
  year = {2019},
  month = jun,
  journal = {BMC Bioinformatics},
  volume = {20},
  number = {11},
  pages = {277},
  issn = {1471-2105},
  doi = {10.1186/s12859-019-2819-0},
  abstract = {In the string correction problem, we are to transform one string into another using a set of prescribed edit operations. In string correction using the Damerau-Levenshtein (DL) distance, the permissible edit operations are: substitution, insertion, deletion and transposition. Several algorithms for string correction using the DL distance have been proposed. The fastest and most space efficient of these algorithms is due to Lowrance and Wagner. It computes the DL distance between strings of length m and n, respectively, in O(mn) time and O(mn) space. In this paper, we focus on the development of algorithms whose asymptotic space complexity is less and whose actual runtime and energy consumption are less than those of the algorithm of Lowrance and Wagner.},
  keywords = {Cache efficient,Damerau-Levenshtein distance,Edit distance,String correction},
  file = {/home/bestname/Zotero/storage/DGXF6HR4/Zhao_Sahni_2019_String correction using the Damerau-Levenshtein distance.pdf;/home/bestname/Zotero/storage/QCWLTY8B/s12859-019-2819-0.html}
}

@article{zhu_S3VAESelfSupervisedSequential_2020,
  title = {{{S3VAE}}: {{Self-Supervised Sequential VAE}} for {{Representation Disentanglement}} and {{Data Generation}}},
  shorttitle = {{{S3VAE}}},
  author = {Zhu, Yizhe and Min, Martin Renqiang and Kadav, Asim and Graf, Hans Peter},
  year = {2020},
  month = may,
  journal = {arXiv:2005.11437 [cs]},
  eprint = {2005.11437},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We propose a sequential variational autoencoder to learn disentangled representations of sequential data (e.g., videos and audios) under self-supervision. Specifically, we exploit the benefits of some readily accessible supervisory signals from input data itself or some off-the-shelf functional models and accordingly design auxiliary tasks for our model to utilize these signals. With the supervision of the signals, our model can easily disentangle the representation of an input sequence into static factors and dynamic factors (i.e., time-invariant and time-varying parts). Comprehensive experiments across videos and audios verify the effectiveness of our model on representation disentanglement and generation of sequential data, and demonstrate that, our model with self-supervision performs comparable to, if not better than, the fully-supervised model with ground truth labels, and outperforms state-of-the-art unsupervised models by a large margin.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,irrelevant,skimmed,VAE},
  annotation = {ZSCC: 0000023},
  file = {/home/bestname/Zotero/storage/YAPRZ7VV/Zhu et al_2020_S3VAE.pdf;/home/bestname/Zotero/storage/QUYWE8F6/2005.html}
}


